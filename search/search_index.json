{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Numaflow \u00b6 Welcome to Numaflow! A Kubernetes-native, serverless platform for running scalable and reliable event-driven applications. Numaflow decouples event sources and sinks from the processing logic, allowing each component to independently auto-scale based on demand. With out-of-the-box sources and sinks, and built-in observability, developers can focus on their processing logic without worrying about event consumption, writing boilerplate code, or operational complexities. Each step of the pipeline can be written in any programming language, offering unparalleled flexibility in using the best programming language for each step and ease of using the languages you are most familiar with. Use Cases \u00b6 Event driven applications: Process events as they happen, e.g., updating inventory and sending customer notifications in e-commerce. Real time analytics: Analyze data instantly, e.g., social media analytics, observability data processing. Inference on streaming data: Perform real-time predictions, e.g., anomaly detection. Workflows running in a streaming manner. Learn more in our User Guide . Key Features \u00b6 Kubernetes-native: If you know Kubernetes, you already know how to use Numaflow. Serverless: Focus on your code and let the system scale up and down based on demand. Language agnostic: Use your favorite programming language. Exactly-Once semantics: No input element is duplicated or lost even as pods are rescheduled or restarted. Auto-scaling with back-pressure: Each vertex automatically scales from zero to whatever is needed. Data Integrity Guarantees \u00b6 Minimally provide at-least-once semantics Provide exactly-once semantics for unbounded and near real-time data sources Preserving order is not required Roadmap \u00b6 Metadata propagation and Open Telemetry Tracing, Deprecate monitor container (1.7) Demo \u00b6 Getting Started \u00b6 For set-up information and running your first Numaflow pipeline, please see our getting started guide .","title":"Home"},{"location":"#numaflow","text":"Welcome to Numaflow! A Kubernetes-native, serverless platform for running scalable and reliable event-driven applications. Numaflow decouples event sources and sinks from the processing logic, allowing each component to independently auto-scale based on demand. With out-of-the-box sources and sinks, and built-in observability, developers can focus on their processing logic without worrying about event consumption, writing boilerplate code, or operational complexities. Each step of the pipeline can be written in any programming language, offering unparalleled flexibility in using the best programming language for each step and ease of using the languages you are most familiar with.","title":"Numaflow"},{"location":"#use-cases","text":"Event driven applications: Process events as they happen, e.g., updating inventory and sending customer notifications in e-commerce. Real time analytics: Analyze data instantly, e.g., social media analytics, observability data processing. Inference on streaming data: Perform real-time predictions, e.g., anomaly detection. Workflows running in a streaming manner. Learn more in our User Guide .","title":"Use Cases"},{"location":"#key-features","text":"Kubernetes-native: If you know Kubernetes, you already know how to use Numaflow. Serverless: Focus on your code and let the system scale up and down based on demand. Language agnostic: Use your favorite programming language. Exactly-Once semantics: No input element is duplicated or lost even as pods are rescheduled or restarted. Auto-scaling with back-pressure: Each vertex automatically scales from zero to whatever is needed.","title":"Key Features"},{"location":"#data-integrity-guarantees","text":"Minimally provide at-least-once semantics Provide exactly-once semantics for unbounded and near real-time data sources Preserving order is not required","title":"Data Integrity Guarantees"},{"location":"#roadmap","text":"Metadata propagation and Open Telemetry Tracing, Deprecate monitor container (1.7)","title":"Roadmap"},{"location":"#demo","text":"","title":"Demo"},{"location":"#getting-started","text":"For set-up information and running your first Numaflow pipeline, please see our getting started guide .","title":"Getting Started"},{"location":"APIs/","text":"Packages: numaflow.numaproj.io/v1alpha1 numaflow.numaproj.io/v1alpha1 Resource Types: AbstractPodTemplate ( Appears on: AbstractVertex , DaemonTemplate , JetStreamBufferService , JobTemplate , MonoVertexSpec , ServingSpec , SideInputsManagerTemplate , VertexTemplate ) AbstractPodTemplate provides a template for pod customization in vertices, daemon deployments and so on. Field Description metadata Metadata (Optional) Metadata sets the pods\u2019s metadata, i.e. annotations and labels nodeSelector map\\[string\\]string (Optional) NodeSelector is a selector which must be true for the pod to fit on a node. Selector which must match a node\u2019s labels for the pod to be scheduled on that node. More info: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/ tolerations \\[\\]Kubernetes core/v1.Toleration (Optional) If specified, the pod\u2019s tolerations. securityContext Kubernetes core/v1.PodSecurityContext (Optional) SecurityContext holds pod-level security attributes and common container settings. Optional: Defaults to empty. See type description for default values of each field. imagePullSecrets \\[\\]Kubernetes core/v1.LocalObjectReference (Optional) ImagePullSecrets is an optional list of references to secrets in the same namespace to use for pulling any of the images used by this PodSpec. If specified, these secrets will be passed to individual puller implementations for them to use. For example, in the case of docker, only DockerConfig type secrets are honored. More info: https://kubernetes.io/docs/concepts/containers/images#specifying-imagepullsecrets-on-a-pod priorityClassName string (Optional) If specified, indicates the pod\u2019s priority. \u201csystem-node-critical\u201d and \u201csystem-cluster-critical\u201d are two special keywords which indicate the highest priorities with the former being the highest priority. Any other name must be defined by creating a PriorityClass object with that name. If not specified, the pod priority will be default or zero if there is no default. More info: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/ priority int32 (Optional) The priority value. Various system components use this field to find the priority of the pod. When Priority Admission Controller is enabled, it prevents users from setting this field. The admission controller populates this field from PriorityClassName. The higher the value, the higher the priority. More info: https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/ affinity Kubernetes core/v1.Affinity (Optional) The pod\u2019s scheduling constraints More info: https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/ serviceAccountName string (Optional) ServiceAccountName applied to the pod runtimeClassName string (Optional) RuntimeClassName refers to a RuntimeClass object in the node.k8s.io group, which should be used to run this pod. If no RuntimeClass resource matches the named class, the pod will not be run. If unset or empty, the \u201clegacy\u201d RuntimeClass will be used, which is an implicit class with an empty definition that uses the default runtime handler. More info: https://git.k8s.io/enhancements/keps/sig-node/585-runtime-class automountServiceAccountToken bool (Optional) AutomountServiceAccountToken indicates whether a service account token should be automatically mounted. dnsPolicy Kubernetes core/v1.DNSPolicy (Optional) Set DNS policy for the pod. Defaults to \u201cClusterFirst\u201d. Valid values are \u2018ClusterFirstWithHostNet\u2019, \u2018ClusterFirst\u2019, \u2018Default\u2019 or \u2018None\u2019. DNS parameters given in DNSConfig will be merged with the policy selected with DNSPolicy. To have DNS options set along with hostNetwork, you have to specify DNS policy explicitly to \u2018ClusterFirstWithHostNet\u2019. dnsConfig Kubernetes core/v1.PodDNSConfig (Optional) Specifies the DNS parameters of a pod. Parameters specified here will be merged to the generated DNS configuration based on DNSPolicy. resourceClaims \\[\\]Kubernetes core/v1.PodResourceClaim (Optional) ResourceClaims defines which ResourceClaims must be allocated and reserved before the Pod is allowed to start. The resources will be made available to those containers which consume them by name. AbstractSink ( Appears on: Sink ) Field Description log Log (Optional) Log sink is used to write the data to the log. kafka KafkaSink (Optional) Kafka sink is used to write the data to the Kafka. blackhole Blackhole (Optional) Blackhole sink is used to write the data to the blackhole sink, which is a sink that discards all the data written to it. udsink UDSink (Optional) UDSink sink is used to write the data to the user-defined sink. serve ServeSink (Optional) Serve sink is used to return results when working with a ServingPipeline. sqs SqsSink (Optional) SQS sink is used to write the data to the AWS SQS. pulsar PulsarSink (Optional) Pulsar sink is used to write the data to the Apache Pulsar. AbstractVertex ( Appears on: PipelineSpec , VertexSpec ) Field Description name string source Source (Optional) sink Sink (Optional) udf UDF (Optional) containerTemplate ContainerTemplate (Optional) Container template for the main numa container. initContainerTemplate ContainerTemplate (Optional) Container template for all the vertex pod init containers spawned by numaflow, excluding the ones specified by the user. AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) volumes \\[\\]Kubernetes core/v1.Volume (Optional) limits VertexLimits (Optional) Limits define the limitations such as buffer read batch size for all the vertices of a pipeline, will override pipeline level settings scale Scale (Optional) Settings for autoscaling initContainers \\[\\]Kubernetes core/v1.Container (Optional) List of customized init containers belonging to the pod. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ sidecars \\[\\]Kubernetes core/v1.Container (Optional) List of customized sidecar containers belonging to the pod. partitions int32 (Optional) Number of partitions of the vertex owned buffers. It applies to udf and sink vertices only. sideInputs \\[\\]string (Optional) Names of the side inputs used in this vertex. sideInputsContainerTemplate ContainerTemplate (Optional) Container template for the side inputs watcher container. updateStrategy UpdateStrategy (Optional) The strategy to use to replace existing pods with new ones. AccumulatorWindow ( Appears on: Window ) AccumulatorWindow describes a special kind of SessionWindow (similar to Global Window) where output should always have monotonically increasing WM but it can be manipulated through event-time by reordering the messages. NOTE: Quite powerful, should not be abused; it can cause stalling of pipelines and leaks. Field Description timeout Kubernetes meta/v1.Duration Timeout is the duration of inactivity after which the state of the accumulator is removed. Authorization ( Appears on: HTTPSource , ServingSpec ) Field Description token Kubernetes core/v1.SecretKeySelector (Optional) A secret selector which contains bearer token To use this, the client needs to add \u201cAuthorization: Bearer \u201d in the header Backoff ( Appears on: RetryStrategy ) Backoff defines parameters used to systematically configure the retry strategy. Field Description interval Kubernetes meta/v1.Duration (Optional) Interval sets the initial retry duration, after a failure occurs. steps uint32 (Optional) Steps defines the maximum number of retry attempts factor float64 (Optional) Interval is multiplied by factor each iteration, if factor is not zero and the limits imposed by Steps and Cap have not been reached. cap Kubernetes meta/v1.Duration (Optional) A limit on revised values of the interval parameter. If a multiplication by the factor parameter would make the interval exceed the cap then the interval is set to the cap and the steps parameter is set to zero. jitter float64 (Optional) The sleep at each iteration is the interval plus an additional amount chosen uniformly at random from the interval between zero and jitter\\*interval . BasicAuth ( Appears on: NatsAuth ) BasicAuth represents the basic authentication approach which contains a user name and a password. Field Description user Kubernetes core/v1.SecretKeySelector (Optional) Secret for auth user password Kubernetes core/v1.SecretKeySelector (Optional) Secret for auth password Blackhole ( Appears on: AbstractSink ) Blackhole is a sink to emulate /dev/null BufferFullWritingStrategy ( string alias) ( Appears on: Edge ) BufferServiceConfig ( Appears on: GetServingPipelineResourceReq , InterStepBufferServiceStatus ) Field Description jetstream JetStreamConfig CombinedEdge ( Appears on: VertexSpec ) CombinedEdge is a combination of Edge and some other properties such as vertex type, partitions, limits. It\u2019s used to decorate the fromEdges and toEdges of the generated Vertex objects, so that in the vertex pod, it knows the properties of the connected vertices, for example, how many partitioned buffers I should write to, what is the write buffer length, etc. Field Description Edge Edge (Members of Edge are embedded into this type.) fromVertexType VertexType From vertex type. fromVertexPartitionCount int32 (Optional) The number of partitions of the from vertex, if not provided, the default value is set to \u201c1\u201d. fromVertexLimits VertexLimits (Optional) toVertexType VertexType To vertex type. toVertexPartitionCount int32 (Optional) The number of partitions of the to vertex, if not provided, the default value is set to \u201c1\u201d. toVertexLimits VertexLimits (Optional) Compression ( Appears on: InterStepBuffer ) Compression is the compression settings for the messages in the InterStepBuffer Field Description type CompressionType (Optional) Type is the type of compression to be used CompressionType ( string alias) ( Appears on: Compression ) CompressionType is a string enumeration type that enumerates all possible compression types. ConditionType ( string alias) ConditionType is a valid value of Condition.Type Container ( Appears on: ServingStore , SideInput , UDF , UDSink , UDSource , UDTransformer ) Container is used to define the container properties for user-defined functions, sinks, etc. Field Description image string (Optional) command \\[\\]string (Optional) args \\[\\]string (Optional) env \\[\\]Kubernetes core/v1.EnvVar (Optional) envFrom \\[\\]Kubernetes core/v1.EnvFromSource (Optional) volumeMounts \\[\\]Kubernetes core/v1.VolumeMount (Optional) resources Kubernetes core/v1.ResourceRequirements (Optional) securityContext Kubernetes core/v1.SecurityContext (Optional) imagePullPolicy Kubernetes core/v1.PullPolicy (Optional) readinessProbe Probe (Optional) livenessProbe Probe (Optional) ports \\[\\]Kubernetes core/v1.ContainerPort (Optional) ContainerTemplate ( Appears on: AbstractVertex , DaemonTemplate , JetStreamBufferService , JobTemplate , MonoVertexSpec , ServingSpec , SideInputsManagerTemplate , VertexTemplate ) ContainerTemplate defines customized spec for a container Field Description resources Kubernetes core/v1.ResourceRequirements (Optional) imagePullPolicy Kubernetes core/v1.PullPolicy (Optional) securityContext Kubernetes core/v1.SecurityContext (Optional) env \\[\\]Kubernetes core/v1.EnvVar (Optional) envFrom \\[\\]Kubernetes core/v1.EnvFromSource (Optional) readinessProbe Probe (Optional) livenessProbe Probe (Optional) DaemonTemplate ( Appears on: MonoVertexSpec , Templates ) Field Description AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) replicas int32 (Optional) Replicas is the number of desired replicas of the Deployment. This is a pointer to distinguish between explicit zero and unspecified. Defaults to 1. More info: https://kubernetes.io/docs/concepts/workloads/controllers/replicationcontroller#what-is-a-replicationcontroller containerTemplate ContainerTemplate (Optional) initContainerTemplate ContainerTemplate (Optional) Edge ( Appears on: CombinedEdge , PipelineSpec ) Field Description from string to string conditions ForwardConditions (Optional) Conditional forwarding, only allowed when \u201cFrom\u201d is a Source or UDF. onFull BufferFullWritingStrategy (Optional) OnFull specifies the behaviour for the write actions when the inter step buffer is full. There are currently two options, retryUntilSuccess and discardLatest. if not provided, the default value is set to \u201cretryUntilSuccess\u201d FixedWindow ( Appears on: Window ) FixedWindow describes a fixed window Field Description length Kubernetes meta/v1.Duration Length is the duration of the fixed window. streaming bool (Optional) Streaming should be set to true if the reduce udf is streaming. ForwardConditions ( Appears on: Edge ) Field Description tags TagConditions Tags used to specify tags for conditional forwarding GSSAPI ( Appears on: SASL ) GSSAPI represents a SASL GSSAPI config Field Description serviceName string realm string usernameSecret Kubernetes core/v1.SecretKeySelector UsernameSecret refers to the secret that contains the username authType KRB5AuthType valid inputs - KRB5_USER_AUTH, KRB5_KEYTAB_AUTH passwordSecret Kubernetes core/v1.SecretKeySelector (Optional) PasswordSecret refers to the secret that contains the password keytabSecret Kubernetes core/v1.SecretKeySelector (Optional) KeytabSecret refers to the secret that contains the keytab kerberosConfigSecret Kubernetes core/v1.SecretKeySelector (Optional) KerberosConfigSecret refers to the secret that contains the kerberos config GeneratorSource ( Appears on: Source ) Field Description rpu int64 (Optional) duration Kubernetes meta/v1.Duration (Optional) msgSize int32 (Optional) Size of each generated message keyCount int32 (Optional) KeyCount is the number of unique keys in the payload value uint64 (Optional) Value is an optional uint64 value to be written in to the payload jitter Kubernetes meta/v1.Duration (Optional) Jitter is the jitter for the message generation, used to simulate out of order messages for example if the jitter is 10s, then the message\u2019s event time will be delayed by a random time between 0 and 10s which will result in the message being out of order by 0 to 10s valueBlob string (Optional) ValueBlob is an optional string which is the base64 encoding of direct payload to send. This is useful for attaching a GeneratorSource to a true pipeline to test load behavior with true messages without requiring additional work to generate messages through the external source if present, the Value and MsgSize fields will be ignored. GetDaemonDeploymentReq Field Description ISBSvcType ISBSvcType Image string PullPolicy Kubernetes core/v1.PullPolicy Env \\[\\]Kubernetes core/v1.EnvVar DefaultResources Kubernetes core/v1.ResourceRequirements GetJetStreamServiceSpecReq Field Description Labels map\\[string\\]string ClusterPort int32 ClientPort int32 MonitorPort int32 MetricsPort int32 GetJetStreamStatefulSetSpecReq Field Description ServiceName string Labels map\\[string\\]string NatsImage string MetricsExporterImage string ConfigReloaderImage string ClusterPort int32 ClientPort int32 MonitorPort int32 MetricsPort int32 ServerAuthSecretName string ServerEncryptionSecretName string ConfigMapName string PvcNameIfNeeded string StartCommand string DefaultResources Kubernetes core/v1.ResourceRequirements GetMonoVertexDaemonDeploymentReq Field Description Image string PullPolicy Kubernetes core/v1.PullPolicy Env \\[\\]Kubernetes core/v1.EnvVar DefaultResources Kubernetes core/v1.ResourceRequirements GetMonoVertexPodSpecReq Field Description Image string PullPolicy Kubernetes core/v1.PullPolicy Env \\[\\]Kubernetes core/v1.EnvVar DefaultResources Kubernetes core/v1.ResourceRequirements GetServingPipelineResourceReq Field Description ISBSvcConfig BufferServiceConfig Image string PullPolicy Kubernetes core/v1.PullPolicy Env \\[\\]Kubernetes core/v1.EnvVar DefaultResources Kubernetes core/v1.ResourceRequirements GetSideInputDeploymentReq Field Description ISBSvcType ISBSvcType Image string PullPolicy Kubernetes core/v1.PullPolicy Env \\[\\]Kubernetes core/v1.EnvVar DefaultResources Kubernetes core/v1.ResourceRequirements GetVertexPodSpecReq Field Description ISBSvcType ISBSvcType Image string PullPolicy Kubernetes core/v1.PullPolicy Env \\[\\]Kubernetes core/v1.EnvVar SideInputsStoreName string ServingSourceStreamName string PipelineSpec PipelineSpec DefaultResources Kubernetes core/v1.ResourceRequirements GroupBy ( Appears on: UDF ) GroupBy indicates it is a reducer UDF Field Description window Window Window describes the windowing strategy. keyed bool (Optional) allowedLateness Kubernetes meta/v1.Duration (Optional) AllowedLateness allows late data to be included for the Reduce operation as long as the late data is not later than (Watermark - AllowedLateness). storage PBQStorage Storage is used to define the PBQ storage for a reduce vertex. HTTPSource ( Appears on: Source ) Field Description auth Authorization (Optional) service bool (Optional) Whether to create a ClusterIP Service ISBSvcPhase ( string alias) ( Appears on: InterStepBufferServiceStatus ) ISBSvcType ( string alias) ( Appears on: GetDaemonDeploymentReq , GetSideInputDeploymentReq , GetVertexPodSpecReq , InterStepBufferServiceStatus ) IdleSource ( Appears on: Watermark ) Field Description threshold Kubernetes meta/v1.Duration Threshold is the duration after which a source is marked as Idle due to lack of data. Ex: If watermark found to be idle after the Threshold duration then the watermark is progressed by IncrementBy . stepInterval Kubernetes meta/v1.Duration (Optional) StepInterval is the duration between the subsequent increment of the watermark as long the source remains Idle. The default value is 0s which means that once we detect idle source, we will be incrementing the watermark by IncrementBy for time we detect that we source is empty (in other words, this will be a very frequent update). incrementBy Kubernetes meta/v1.Duration IncrementBy is the duration to be added to the current watermark to progress the watermark when source is idling. InterStepBuffer ( Appears on: PipelineSpec , VertexSpec ) InterStepBuffer configuration specifically for the pipeline. Field Description compression Compression (Optional) Compression is the compression settings for the InterStepBufferService InterStepBufferService Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec InterStepBufferServiceSpec jetstream JetStreamBufferService status InterStepBufferServiceStatus (Optional) InterStepBufferServiceSpec ( Appears on: InterStepBufferService ) Field Description jetstream JetStreamBufferService InterStepBufferServiceStatus ( Appears on: InterStepBufferService ) Field Description Status Status (Members of Status are embedded into this type.) phase ISBSvcPhase message string config BufferServiceConfig type ISBSvcType observedGeneration int64 JetStreamBufferService ( Appears on: InterStepBufferServiceSpec ) Field Description version string JetStream version, such as \u201c2.7.1\u201d replicas int32 JetStream StatefulSet size containerTemplate ContainerTemplate (Optional) ContainerTemplate contains customized spec for NATS container reloaderContainerTemplate ContainerTemplate (Optional) ReloaderContainerTemplate contains customized spec for config reloader container metricsContainerTemplate ContainerTemplate (Optional) MetricsContainerTemplate contains customized spec for metrics container persistence PersistenceStrategy (Optional) AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) settings string (Optional) Nats/JetStream configuration, if not specified, global settings in numaflow-controller-config will be used. See https://docs.nats.io/running-a-nats-service/configuration#limits and https://docs.nats.io/running-a-nats-service/configuration#jetstream . For limits, only \u201cmax_payload\u201d is supported for configuration, defaults to 1048576 (1MB), not recommended to use values over 8388608 (8MB) but max_payload can be set up to 67108864 (64MB). For jetstream, only \u201cmax_memory_store\u201d and \u201cmax_file_store\u201d are supported for configuration, do not set \u201cstore_dir\u201d as it has been hardcoded. startArgs \\[\\]string (Optional) Optional arguments to start nats-server. For example, \u201c-D\u201d to enable debugging output, \u201c-DV\u201d to enable debugging and tracing. Check https://docs.nats.io/ for all the available arguments. bufferConfig string (Optional) Optional configuration for the streams, consumers and buckets to be created in this JetStream service, if specified, it will be merged with the default configuration in numaflow-controller-config. It accepts a YAML format configuration, it may include 4 sections, \u201cstream\u201d, \u201cconsumer\u201d, \u201cotBucket\u201d and \u201cprocBucket\u201d. Available fields under \u201cstream\u201d include \u201cretention\u201d (e.g. interest, limits, workerQueue), \u201cmaxMsgs\u201d, \u201cmaxAge\u201d (e.g. 72h), \u201creplicas\u201d (1, 3, 5), \u201cduplicates\u201d (e.g. 5m). Available fields under \u201cconsumer\u201d include \u201cackWait\u201d (e.g. 60s) Available fields under \u201cotBucket\u201d include \u201cmaxValueSize\u201d, \u201chistory\u201d, \u201cttl\u201d (e.g. 72h), \u201cmaxBytes\u201d, \u201creplicas\u201d (1, 3, 5). Available fields under \u201cprocBucket\u201d include \u201cmaxValueSize\u201d, \u201chistory\u201d, \u201cttl\u201d (e.g. 72h), \u201cmaxBytes\u201d, \u201creplicas\u201d (1, 3, 5). encryption bool (Optional) Whether encrypt the data at rest, defaults to false Enabling encryption might impact the performance, see https://docs.nats.io/running-a-nats-service/nats_admin/jetstream_admin/encryption_at_rest for the detail Toggling the value will impact encrypting/decrypting existing messages. tls bool (Optional) Whether enable TLS, defaults to false Enabling TLS might impact the performance JetStreamConfig ( Appears on: BufferServiceConfig ) Field Description url string JetStream (NATS) URL auth NatsAuth streamConfig string (Optional) tlsEnabled bool TLS enabled or not JetStreamSource ( Appears on: Source ) Field Description url string URL to connect to NATS cluster, multiple urls could be separated by comma. stream string Stream represents the name of the stream. consumer string (Optional) Consumer represents the name of the consumer of the stream If not specified, a consumer with name numaflow-pipeline_name-vertex_name-stream_name will be created. If a consumer name is specified, a consumer with that name will be created if it doesn\u2019t exist on the stream. deliver_policy string (Optional) The point in the stream from which to receive messages. https://docs.nats.io/nats-concepts/jetstream/consumers#deliverpolicy Valid options are: \u201call\u201d, \u201cnew\u201d, \u201clast\u201d, \u201clast_per_subject\u201d, \u201cby_start_sequence 42\u201d, \u201cby_start_time 1753428483000\u201d. The second value to \u201cby_start_time\u201d is unix epoch time in milliseconds. filter_subjects \\[\\]string (Optional) A set of subjects that overlap with the subjects bound to the stream to filter delivery to subscribers. https://docs.nats.io/nats-concepts/jetstream/consumers#filtesubjects tls TLS (Optional) TLS configuration for the nats client. auth NatsAuth (Optional) Auth information JobTemplate ( Appears on: Templates ) Field Description AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) containerTemplate ContainerTemplate (Optional) ttlSecondsAfterFinished int32 (Optional) ttlSecondsAfterFinished limits the lifetime of a Job that has finished execution (either Complete or Failed). If this field is set, ttlSecondsAfterFinished after the Job finishes, it is eligible to be automatically deleted. When the Job is being deleted, its lifecycle guarantees (e.g. finalizers) will be honored. If this field is unset, the Job won\u2019t be automatically deleted. If this field is set to zero, the Job becomes eligible to be deleted immediately after it finishes. Numaflow defaults to 30 backoffLimit int32 (Optional) Specifies the number of retries before marking this job failed. More info: https://kubernetes.io/docs/concepts/workloads/controllers/job/#pod-backoff-failure-policy Numaflow defaults to 20 KRB5AuthType ( string alias) ( Appears on: GSSAPI ) KRB5AuthType describes the kerberos auth type KafkaSink ( Appears on: AbstractSink ) Field Description brokers \\[\\]string topic string setKey bool (Optional) SetKey sets the Kafka key to the keys passed in the Message. When the key is null (default), the record is sent randomly to one of the available partitions of the topic. If a key exists, Kafka hashes the key, and the result is used to map the message to a specific partition. This ensures that messages with the same key end up in the same partition. tls TLS (Optional) TLS user to configure TLS connection for kafka broker TLS.enable=true default for TLS. config string (Optional) sasl SASL (Optional) SASL user to configure SASL connection for kafka broker SASL.enable=true default for SASL. KafkaSource ( Appears on: Source ) Field Description brokers \\[\\]string topic string consumerGroup string tls TLS (Optional) TLS user to configure TLS connection for kafka broker TLS.enable=true default for TLS. config string (Optional) sasl SASL (Optional) SASL user to configure SASL connection for kafka broker SASL.enable=true default for SASL. kafkaVersion string Lifecycle ( Appears on: PipelineSpec ) Field Description deletionGracePeriodSeconds int64 (Optional) DeletionGracePeriodSeconds used to delete pipeline gracefully desiredPhase PipelinePhase (Optional) DesiredPhase used to bring the pipeline from current phase to desired phase pauseGracePeriodSeconds int64 (Optional) PauseGracePeriodSeconds used to pause pipeline gracefully deleteGracePeriodSeconds int64 (Optional) DeleteGracePeriodSeconds used to delete pipeline gracefully Deprecated: Use DeletionGracePeriodSeconds instead Log ( Appears on: AbstractSink ) LogicOperator ( string alias) ( Appears on: TagConditions ) Metadata ( Appears on: AbstractPodTemplate ) Field Description annotations map\\[string\\]string labels map\\[string\\]string MonoVertex Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec MonoVertexSpec replicas int32 (Optional) source Source sink Sink AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) containerTemplate ContainerTemplate (Optional) Container template for the main numa container. volumes \\[\\]Kubernetes core/v1.Volume (Optional) limits MonoVertexLimits (Optional) Limits define the limitations such as read batch size for the mono vertex. scale Scale (Optional) Settings for autoscaling initContainers \\[\\]Kubernetes core/v1.Container (Optional) List of customized init containers belonging to the pod. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ sidecars \\[\\]Kubernetes core/v1.Container (Optional) List of customized sidecar containers belonging to the pod. daemonTemplate DaemonTemplate (Optional) Template for the daemon service deployment. updateStrategy UpdateStrategy (Optional) The strategy to use to replace existing pods with new ones. lifecycle MonoVertexLifecycle (Optional) Lifecycle defines the Lifecycle properties of a MonoVertex status MonoVertexStatus (Optional) MonoVertexLifecycle ( Appears on: MonoVertexSpec ) Field Description desiredPhase MonoVertexPhase (Optional) DesiredPhase used to bring the MonoVertex from current phase to desired phase MonoVertexLimits ( Appears on: MonoVertexSpec ) Field Description readBatchSize uint64 (Optional) Read batch size from the source. readTimeout Kubernetes meta/v1.Duration (Optional) ReadTimeout is the read timeout duration from the source. rateLimit RateLimit (Optional) RateLimit for MonoVertex defines how many messages can be read from Source. This is computed by number of read calls per second multiplied by the readBatchSize . This is how RateLimit is calculated for MonoVertex and for Source vertices. MonoVertexPhase ( string alias) ( Appears on: MonoVertexLifecycle , MonoVertexStatus ) MonoVertexSpec ( Appears on: MonoVertex ) Field Description replicas int32 (Optional) source Source sink Sink AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) containerTemplate ContainerTemplate (Optional) Container template for the main numa container. volumes \\[\\]Kubernetes core/v1.Volume (Optional) limits MonoVertexLimits (Optional) Limits define the limitations such as read batch size for the mono vertex. scale Scale (Optional) Settings for autoscaling initContainers \\[\\]Kubernetes core/v1.Container (Optional) List of customized init containers belonging to the pod. More info: https://kubernetes.io/docs/concepts/workloads/pods/init-containers/ sidecars \\[\\]Kubernetes core/v1.Container (Optional) List of customized sidecar containers belonging to the pod. daemonTemplate DaemonTemplate (Optional) Template for the daemon service deployment. updateStrategy UpdateStrategy (Optional) The strategy to use to replace existing pods with new ones. lifecycle MonoVertexLifecycle (Optional) Lifecycle defines the Lifecycle properties of a MonoVertex MonoVertexStatus ( Appears on: MonoVertex ) Field Description Status Status (Members of Status are embedded into this type.) phase MonoVertexPhase (Optional) replicas uint32 (Optional) Total number of non-terminated pods targeted by this MonoVertex (their labels match the selector). desiredReplicas uint32 (Optional) The number of desired replicas. selector string (Optional) reason string (Optional) message string (Optional) lastUpdated Kubernetes meta/v1.Time (Optional) lastScaledAt Kubernetes meta/v1.Time (Optional) Time of last scaling operation. observedGeneration int64 (Optional) The generation observed by the MonoVertex controller. readyReplicas uint32 (Optional) The number of pods targeted by this MonoVertex with a Ready Condition. updatedReplicas uint32 The number of Pods created by the controller from the MonoVertex version indicated by updateHash. updatedReadyReplicas uint32 The number of ready Pods created by the controller from the MonoVertex version indicated by updateHash. currentHash string If not empty, indicates the current version of the MonoVertex used to generate Pods. updateHash string If not empty, indicates the updated version of the MonoVertex used to generate Pods. NatsAuth ( Appears on: JetStreamConfig , JetStreamSource , NatsSource ) NatsAuth defines how to authenticate the nats access Field Description basic BasicAuth (Optional) Basic auth which contains a username and a password token Kubernetes core/v1.SecretKeySelector (Optional) Token auth nkey Kubernetes core/v1.SecretKeySelector (Optional) NKey auth NatsSource ( Appears on: Source ) Field Description url string URL to connect to NATS cluster, multiple urls could be separated by comma. subject string Subject holds the name of the subject onto which messages are published. queue string Queue is used for queue subscription. tls TLS (Optional) TLS configuration for the nats client. auth NatsAuth (Optional) Auth information NoStore ( Appears on: PBQStorage ) NoStore means there will be no persistence storage and there will be data loss during pod restarts. Use this option only if you do not care about correctness (e.g., approx statistics pipeline like sampling rate, etc.). OnFailureRetryStrategy ( string alias) ( Appears on: RetryStrategy ) PBQStorage ( Appears on: GroupBy ) PBQStorage defines the persistence configuration for a vertex. Field Description persistentVolumeClaim PersistenceStrategy (Optional) emptyDir Kubernetes core/v1.EmptyDirVolumeSource (Optional) no_store NoStore (Optional) PersistenceStrategy ( Appears on: JetStreamBufferService , PBQStorage ) PersistenceStrategy defines the strategy of persistence Field Description storageClassName string (Optional) Name of the StorageClass required by the claim. More info: https://kubernetes.io/docs/concepts/storage/persistent-volumes#class-1 accessMode Kubernetes core/v1.PersistentVolumeAccessMode (Optional) Available access modes such as ReadWriteOncePod, ReadWriteOnce, ReadWriteMany https://kubernetes.io/docs/concepts/storage/persistent-volumes/#access-modes volumeSize k8s.io/apimachinery/pkg/api/resource.Quantity Volume size, e.g. 50Gi Pipeline Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec PipelineSpec interStepBufferServiceName string (Optional) InterStepBufferServiceName is the name of the InterStepBufferService to be used by the pipeline vertices \\[\\]AbstractVertex edges \\[\\]Edge Edges define the relationships between vertices lifecycle Lifecycle (Optional) Lifecycle define the Lifecycle properties limits PipelineLimits (Optional) Limits define the limitations such as buffer read batch size for all the vertices of a pipeline, they could be overridden by each vertex\u2019s settings watermark Watermark (Optional) Watermark enables watermark progression across the entire pipeline. templates Templates (Optional) Templates are used to customize additional kubernetes resources required for the Pipeline sideInputs \\[\\]SideInput (Optional) SideInputs defines the Side Inputs of a pipeline. interStepBuffer InterStepBuffer (Optional) InterStepBuffer configuration specific to this pipeline. status PipelineStatus (Optional) PipelineLimits ( Appears on: PipelineSpec ) Field Description readBatchSize uint64 (Optional) Read batch size for all the vertices in the pipeline, can be overridden by the vertex\u2019s limit settings. bufferMaxLength uint64 (Optional) BufferMaxLength is used to define the max length of a buffer. Only applies to UDF and Source vertices as only they do buffer write. It can be overridden by the settings in vertex limits. bufferUsageLimit uint32 (Optional) BufferUsageLimit is used to define the percentage of the buffer usage limit, a valid value should be less than 100, for example, 85. Only applies to UDF and Source vertices as only they do buffer write. It will be overridden by the settings in vertex limits. readTimeout Kubernetes meta/v1.Duration (Optional) Read timeout for all the vertices in the pipeline, can be overridden by the vertex\u2019s limit settings rateLimit RateLimit (Optional) RateLimit is used to define the rate limit for all the vertices in the pipeline, it could be overridden by the vertex\u2019s limit settings. For source vertices, it will be set to rate divided by readBatchSize because for source vertices, the rate limit is defined by how many times the Read is called per second Reduce does not support RateLimit. PipelinePhase ( string alias) ( Appears on: Lifecycle , PipelineStatus ) PipelineResumeStrategy ( string alias) PipelineSpec ( Appears on: GetVertexPodSpecReq , Pipeline , ServingPipelineSpec ) Field Description interStepBufferServiceName string (Optional) InterStepBufferServiceName is the name of the InterStepBufferService to be used by the pipeline vertices \\[\\]AbstractVertex edges \\[\\]Edge Edges define the relationships between vertices lifecycle Lifecycle (Optional) Lifecycle define the Lifecycle properties limits PipelineLimits (Optional) Limits define the limitations such as buffer read batch size for all the vertices of a pipeline, they could be overridden by each vertex\u2019s settings watermark Watermark (Optional) Watermark enables watermark progression across the entire pipeline. templates Templates (Optional) Templates are used to customize additional kubernetes resources required for the Pipeline sideInputs \\[\\]SideInput (Optional) SideInputs defines the Side Inputs of a pipeline. interStepBuffer InterStepBuffer (Optional) InterStepBuffer configuration specific to this pipeline. PipelineStatus ( Appears on: Pipeline ) Field Description Status Status (Members of Status are embedded into this type.) phase PipelinePhase (Optional) message string (Optional) lastUpdated Kubernetes meta/v1.Time (Optional) vertexCount uint32 (Optional) sourceCount uint32 (Optional) sinkCount uint32 (Optional) udfCount uint32 (Optional) mapUDFCount uint32 (Optional) reduceUDFCount uint32 (Optional) observedGeneration int64 (Optional) The generation observed by the Pipeline controller. drainedOnPause bool (Optional) Field to indicate if a pipeline drain successfully occurred, only meaningful when the pipeline is paused. True means it has been successfully drained. Ports ( Appears on: ServingSpec ) Field Description https int32 (Optional) http int32 (Optional) Probe ( Appears on: Container , ContainerTemplate ) Probe is used to customize the configuration for Readiness and Liveness probes. Field Description initialDelaySeconds int32 (Optional) Number of seconds after the container has started before liveness probes are initiated. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes timeoutSeconds int32 (Optional) Number of seconds after which the probe times out. More info: https://kubernetes.io/docs/concepts/workloads/pods/pod-lifecycle#container-probes periodSeconds int32 (Optional) How often (in seconds) to perform the probe. successThreshold int32 (Optional) Minimum consecutive successes for the probe to be considered successful after having failed. Defaults to 1. Must be 1 for liveness and startup. Minimum value is 1. failureThreshold int32 (Optional) Minimum consecutive failures for the probe to be considered failed after having succeeded. Defaults to 3. Minimum value is 1. PulsarAuth ( Appears on: PulsarSink , PulsarSource ) PulsarAuth defines how to authenticate with Pulsar Field Description token Kubernetes core/v1.SecretKeySelector (Optional) JWT Token auth basicAuth PulsarBasicAuth (Optional) Authentication using HTTP basic https://pulsar.apache.org/docs/4.0.x/security-basic-auth/ PulsarBasicAuth ( Appears on: PulsarAuth ) Field Description username Kubernetes core/v1.SecretKeySelector (Optional) password Kubernetes core/v1.SecretKeySelector (Optional) PulsarSink ( Appears on: AbstractSink ) Field Description serverAddr string topic string producerName string auth PulsarAuth (Optional) Auth information PulsarSource ( Appears on: Source ) Field Description serverAddr string topic string consumerName string subscriptionName string maxUnack uint32 Maximum number of messages that are in not yet acked state. Once this limit is crossed, futher read requests will return empty list. auth PulsarAuth (Optional) Auth information RateLimit ( Appears on: MonoVertexLimits , PipelineLimits , VertexLimits ) Field Description max uint64 Max is the maximum TPS that this vertex can process give a distributed Store is configured. Otherwise, it will be the maximum TPS for a single replica. min uint64 Minimum TPS allowed during initial bootup. This value will be distributed across all the replicas if a distributed Store is configured. Otherwise, it will be the minimum TPS for a single replica. rampUpDuration Kubernetes meta/v1.Duration RampUpDuration is the duration to reach the maximum TPS from the minimum TPS. The min unit of ramp up is 1 in 1 second. store RateLimiterStore (Optional) Store is used to define the Distributed Store for the rate limiting. We also support in-memory store if no store is configured. This means that every replica will have its own rate limit and the actual TPS will be the sum of all the replicas. RateLimiterInMemoryStore ( Appears on: RateLimiterStore ) RateLimiterRedisStore ( Appears on: RateLimiterStore ) Field Description mode string Choose how to connect to Redis. - Single: use a single URL (redis://\u2026 or rediss://\u2026) - Sentinel: discover the node via Redis Sentinel url string (Optional) SINGLE MODE: Full connection URL, e.g. redis://host: 6379 \u2044 0 or rediss://host:port/0 Mutually exclusive with .sentinel sentinel RedisSentinelConfig (Optional) SENTINEL MODE: Settings to reach Sentinel and the selected Redis node Mutually exclusive with .url db int32 (Optional) COMMON: Optional DB index (default 0) RateLimiterStore ( Appears on: RateLimit ) Field Description redisStore RateLimiterRedisStore (Optional) RedisStore is used to define the redis store for the rate limit. inMemoryStore RateLimiterInMemoryStore (Optional) InMemoryStore is used to define the in-memory store for the rate limit. RedisAuth ( Appears on: RedisSentinelConfig ) Field Description username Kubernetes core/v1.SecretKeySelector (Optional) For Redis 6+ ACLs. If Username omitted, password-only is also supported. password Kubernetes core/v1.SecretKeySelector (Optional) RedisSentinelConfig ( Appears on: RateLimiterRedisStore ) Field Description masterName string Required Sentinel \u201cservice name\u201d (aka master name) from sentinel.conf endpoints \\[\\]string At least one Sentinel endpoint; 2\u20133 recommended. Use host:port pairs. Example: \\[\u201csentinel-0.redis.svc:26379\u201d, \u201csentinel-1.redis.svc:26379\u201d\\] sentinelAuth RedisAuth (Optional) Auth to talk to the Sentinel daemons (control-plane). Optional. redisAuth RedisAuth (Optional) Auth to talk to the Redis data nodes (data-plane). Optional. sentinelTLS TLS (Optional) TLS for Sentinel connections (if your Sentinels expose TLS). redisTLS TLS (Optional) TLS for Redis data nodes (redis). Often enabled even if Sentinel is plaintext. RetryStrategy ( Appears on: Sink ) The RetryStrategy struct defines the configuration for handling operation retries in case of failures. It incorporates an Exponential BackOff strategy to control retry timing and specifies the actions to take upon failure. Field Description backoff Backoff (Optional) BackOff specifies the parameters for the exponential backoff strategy, controlling how delays between retries should increase. onFailure OnFailureRetryStrategy (Optional) OnFailure specifies the action to take when the specified retry strategy fails. The possible values are: 1. \u201cretry\u201d: start another round of retrying the operation, 2. \u201cfallback\u201d: re-route the operation to a fallback sink and 3. \u201cdrop\u201d: drop the operation and perform no further action. The default action is to retry. RollingUpdateStrategy ( Appears on: UpdateStrategy ) RollingUpdateStrategy is used to communicate parameter for RollingUpdateStrategyType. Field Description maxUnavailable k8s.io/apimachinery/pkg/util/intstr.IntOrString (Optional) The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5) or a percentage of desired pods (ex: 10%). Absolute number is calculated from percentage by rounding down. Defaults to 25%. Example: when this is set to 30%, the old pods can be scaled down to 70% of desired pods immediately when the rolling update starts. Once new pods are ready, old pods can be scaled down further, followed by scaling up the new pods, ensuring that the total number of pods available at all times during the update is at least 70% of desired pods. SASL ( Appears on: KafkaSink , KafkaSource ) Field Description mechanism SASLType SASL mechanism to use gssapi GSSAPI (Optional) GSSAPI contains the kerberos config plain SASLPlain (Optional) SASLPlain contains the sasl plain config scramsha256 SASLPlain (Optional) SASLSCRAMSHA256 contains the sasl plain config scramsha512 SASLPlain (Optional) SASLSCRAMSHA512 contains the sasl plain config oauth SASLOAuth (Optional) OAuth contains the oauth config SASLOAuth ( Appears on: SASL ) Field Description clientID Kubernetes core/v1.SecretKeySelector ClientID refers to the secret that contains the client id clientSecret Kubernetes core/v1.SecretKeySelector ClientSecret refers to the secret that contains the client secret tokenEndpoint string TokenEndpoint refers to the token endpoint SASLPlain ( Appears on: SASL ) Field Description userSecret Kubernetes core/v1.SecretKeySelector UserSecret refers to the secret that contains the user passwordSecret Kubernetes core/v1.SecretKeySelector (Optional) PasswordSecret refers to the secret that contains the password handshake bool SASLType ( string alias) ( Appears on: SASL ) SASLType describes the SASL type Scale ( Appears on: AbstractVertex , MonoVertexSpec ) Scale defines the parameters for autoscaling. Field Description disabled bool (Optional) Whether to disable autoscaling. Set to \u201ctrue\u201d when using Kubernetes HPA or any other 3rd party autoscaling strategies. min int32 (Optional) Minimum replicas. max int32 (Optional) Maximum replicas. lookbackSeconds uint32 (Optional) Lookback seconds to calculate the average pending messages and processing rate. zeroReplicaSleepSeconds uint32 (Optional) After scaling down the source vertex to 0, sleep how many seconds before scaling the source vertex back up to peek. targetProcessingSeconds uint32 (Optional) TargetProcessingSeconds is used to tune the aggressiveness of autoscaling for source vertices, it measures how fast you want the vertex to process all the pending messages. Typically increasing the value, which leads to lower processing rate, thus less replicas. It\u2019s only effective for source vertices. targetBufferAvailability uint32 (Optional) TargetBufferAvailability is used to define the target percentage of the buffer availability. A valid and meaningful value should be less than the BufferUsageLimit defined in the Edge spec (or Pipeline spec), for example, 50. It only applies to UDF and Sink vertices because only they have buffers to read. replicasPerScale uint32 (Optional) DeprecatedReplicasPerScale defines the number of maximum replicas that can be changed in a single scale up or down operation. The is use to prevent from too aggressive scaling operations Deprecated: Use ReplicasPerScaleUp and ReplicasPerScaleDown instead scaleUpCooldownSeconds uint32 (Optional) ScaleUpCooldownSeconds defines the cooldown seconds after a scaling operation, before a follow-up scaling up. It defaults to the CooldownSeconds if not set. scaleDownCooldownSeconds uint32 (Optional) ScaleDownCooldownSeconds defines the cooldown seconds after a scaling operation, before a follow-up scaling down. It defaults to the CooldownSeconds if not set. replicasPerScaleUp uint32 (Optional) ReplicasPerScaleUp defines the number of maximum replicas that can be changed in a single scaled up operation. The is use to prevent from too aggressive scaling up operations replicasPerScaleDown uint32 (Optional) ReplicasPerScaleDown defines the number of maximum replicas that can be changed in a single scaled down operation. The is use to prevent from too aggressive scaling down operations ServeSink ( Appears on: AbstractSink ) ServingPipeline Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec ServingPipelineSpec serving ServingSpec pipeline PipelineSpec status ServingPipelineStatus (Optional) ServingPipelinePhase ( string alias) ( Appears on: ServingPipelineStatus ) ServingPipelineSpec ( Appears on: ServingPipeline ) Field Description serving ServingSpec pipeline PipelineSpec ServingPipelineStatus ( Appears on: ServingPipeline ) Field Description Status Status (Members of Status are embedded into this type.) phase ServingPipelinePhase (Optional) message string (Optional) lastUpdated Kubernetes meta/v1.Time (Optional) observedGeneration int64 (Optional) The generation observed by the ServingPipeline controller. ServingSource ( Appears on: Source ) ServingSource is the source vertex for ServingPipeline and should be used only with ServingPipeline. ServingSpec ( Appears on: ServingPipelineSpec ) Field Description auth Authorization (Optional) service bool (Optional) Whether to create a ClusterIP Service ports Ports (Optional) Ports to listen on, default we will use 8443 for HTTPS. To start http server the http port should be explicitly set. msgIDHeaderKey string The header key from which the message id will be extracted requestTimeoutSeconds uint32 (Optional) Request timeout in seconds. Default value is 120 seconds. store ServingStore (Optional) containerTemplate ContainerTemplate (Optional) Container template for the serving container. replicas int32 (Optional) Initial replicas of the serving server deployment. AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) ServingStore ( Appears on: ServingSpec ) ServingStore defines information of a Serving Store used in a pipeline Field Description container Container SessionWindow ( Appears on: Window ) SessionWindow describes a session window Field Description timeout Kubernetes meta/v1.Duration Timeout is the duration of inactivity after which a session window closes. SideInput ( Appears on: PipelineSpec ) SideInput defines information of a Side Input Field Description name string container Container volumes \\[\\]Kubernetes core/v1.Volume (Optional) trigger SideInputTrigger SideInputTrigger ( Appears on: SideInput ) Field Description schedule string The schedule to trigger the retrievement of the side input data. It supports cron format, for example, \u201c0 30 \\* \\* \\* \\*\u201d. Or interval based format, such as \u201c@hourly\u201d, \u201c@every 1h30m\u201d, etc. timezone string (Optional) SideInputsManagerTemplate ( Appears on: Templates ) Field Description AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) containerTemplate ContainerTemplate (Optional) Template for the side inputs manager numa container initContainerTemplate ContainerTemplate (Optional) Template for the side inputs manager init container Sink ( Appears on: AbstractVertex , MonoVertexSpec ) Field Description AbstractSink AbstractSink (Members of AbstractSink are embedded into this type.) fallback AbstractSink (Optional) Fallback sink can be imagined as DLQ for primary Sink. The writes to Fallback sink will only be initiated if the ud-sink response field sets it. retryStrategy RetryStrategy (Optional) RetryStrategy struct encapsulates the settings for retrying operations in the event of failures. SlidingWindow ( Appears on: Window ) SlidingWindow describes a sliding window Field Description length Kubernetes meta/v1.Duration Length is the duration of the sliding window. slide Kubernetes meta/v1.Duration Slide is the slide parameter that controls the frequency at which the sliding window is created. streaming bool (Optional) Streaming should be set to true if the reduce udf is streaming. Source ( Appears on: AbstractVertex , MonoVertexSpec ) Field Description generator GeneratorSource (Optional) kafka KafkaSource (Optional) http HTTPSource (Optional) nats NatsSource (Optional) transformer UDTransformer (Optional) udsource UDSource (Optional) jetstream JetStreamSource (Optional) serving ServingSource (Optional) pulsar PulsarSource (Optional) sqs SqsSource (Optional) SqsSink ( Appears on: AbstractSink ) Field Description awsRegion string AWSRegion is the AWS Region where the SQS queue is located queueName string QueueName is the name of the SQS queue queueOwnerAWSAccountID string QueueOwnerAWSAccountID is the queue owner aws account id SqsSource ( Appears on: Source ) SqsSource represents the configuration of an AWS SQS source Field Description awsRegion string AWSRegion is the AWS Region where the SQS queue is located queueName string QueueName is the name of the SQS queue queueOwnerAWSAccountID string QueueOwnerAWSAccountID is the queue owner aws account id visibilityTimeout int32 (Optional) VisibilityTimeout is the duration (in seconds) that the received messages are hidden from subsequent retrieve requests after being retrieved by a ReceiveMessage request. Valid values: 0-43200 (12 hours) maxNumberOfMessages int32 (Optional) MaxNumberOfMessages is the maximum number of messages to return in a single poll. Valid values: 1-10 Defaults to 1 waitTimeSeconds int32 (Optional) WaitTimeSeconds is the duration (in seconds) for which the call waits for a message to arrive in the queue before returning. If a message is available, the call returns sooner than WaitTimeSeconds. Valid values: 0-20 Defaults to 0 (short polling) endpointUrl string (Optional) EndpointURL is the custom endpoint URL for the AWS SQS API. This is useful for testing with localstack or when using VPC endpoints. attributeNames \\[\\]string (Optional) AttributeNames is a list of attributes that need to be returned along with each message. Valid values: All \\| Policy \\| VisibilityTimeout \\| MaximumMessageSize \\| MessageRetentionPeriod \\| ApproximateNumberOfMessages \\| ApproximateNumberOfMessagesNotVisible \\| CreatedTimestamp \\| LastModifiedTimestamp \\| QueueArn \\| ApproximateNumberOfMessagesDelayed \\| DelaySeconds \\| ReceiveMessageWaitTimeSeconds \\| RedrivePolicy \\| FifoQueue \\| ContentBasedDeduplication \\| KmsMasterKeyId \\| KmsDataKeyReusePeriodSeconds \\| DeduplicationScope \\| FifoThroughputLimit \\| RedriveAllowPolicy \\| SqsManagedSseEnabled messageAttributeNames \\[\\]string (Optional) MessageAttributeNames is a list of message attributes that need to be returned along with each message. Status ( Appears on: InterStepBufferServiceStatus , MonoVertexStatus , PipelineStatus , ServingPipelineStatus , VertexStatus ) Status is a common structure which can be used for Status field. Field Description conditions \\[\\]Kubernetes meta/v1.Condition (Optional) Conditions are the latest available observations of a resource\u2019s current state. TLS ( Appears on: JetStreamSource , KafkaSink , KafkaSource , NatsSource , RedisSentinelConfig ) Field Description insecureSkipVerify bool (Optional) caCertSecret Kubernetes core/v1.SecretKeySelector (Optional) CACertSecret refers to the secret that contains the CA cert certSecret Kubernetes core/v1.SecretKeySelector (Optional) CertSecret refers to the secret that contains the cert keySecret Kubernetes core/v1.SecretKeySelector (Optional) KeySecret refers to the secret that contains the key TagConditions ( Appears on: ForwardConditions ) Field Description operator LogicOperator (Optional) Operator specifies the type of operation that should be used for conditional forwarding value could be \u201cand\u201d, \u201cor\u201d, \u201cnot\u201d values \\[\\]string Values tag values for conditional forwarding Templates ( Appears on: PipelineSpec ) Field Description daemon DaemonTemplate (Optional) DaemonTemplate is used to customize the Daemon Deployment. job JobTemplate (Optional) JobTemplate is used to customize Jobs. sideInputsManager SideInputsManagerTemplate (Optional) SideInputsManagerTemplate is used to customize the Side Inputs Manager. vertex VertexTemplate (Optional) VertexTemplate is used to customize the vertices of the pipeline. UDF ( Appears on: AbstractVertex ) Field Description container Container (Optional) groupBy GroupBy (Optional) UDSink ( Appears on: AbstractSink ) Field Description container Container UDSource ( Appears on: Source ) Field Description container Container UDTransformer ( Appears on: Source ) Field Description container Container (Optional) UpdateStrategy ( Appears on: AbstractVertex , MonoVertexSpec ) UpdateStrategy indicates the strategy that the controller will use to perform updates for Vertex or MonoVertex. Field Description type UpdateStrategyType (Optional) Type indicates the type of the StatefulSetUpdateStrategy. Default is RollingUpdate. rollingUpdate RollingUpdateStrategy (Optional) RollingUpdate is used to communicate parameters when Type is RollingUpdateStrategy. UpdateStrategyType ( string alias) ( Appears on: UpdateStrategy ) UpdateStrategyType is a string enumeration type that enumerates all possible update strategies. Vertex ( Appears on: VertexInstance ) Field Description metadata Kubernetes meta/v1.ObjectMeta Refer to the Kubernetes API documentation for the fields of the metadata field. spec VertexSpec AbstractVertex AbstractVertex (Members of AbstractVertex are embedded into this type.) pipelineName string interStepBufferServiceName string (Optional) replicas int32 (Optional) fromEdges \\[\\]CombinedEdge (Optional) toEdges \\[\\]CombinedEdge (Optional) watermark Watermark (Optional) Watermark indicates watermark progression in the vertex, it\u2019s populated from the pipeline watermark settings. lifecycle VertexLifecycle (Optional) Lifecycle defines the Lifecycle properties of a vertex interStepBuffer InterStepBuffer (Optional) InterStepBuffer configuration specific to this pipeline. status VertexStatus (Optional) VertexInstance VertexInstance is a wrapper of a vertex instance, which contains the vertex spec and the instance information such as hostname and replica index. Field Description vertex Vertex hostname string replica int32 VertexLifecycle ( Appears on: VertexSpec ) Field Description desiredPhase VertexPhase (Optional) DesiredPhase used to bring the vertex from current phase to desired phase VertexLimits ( Appears on: AbstractVertex , CombinedEdge ) Field Description readBatchSize uint64 (Optional) Read batch size from the source or buffer. It overrides the settings from pipeline limits. readTimeout Kubernetes meta/v1.Duration (Optional) Read timeout duration from the source or buffer It overrides the settings from pipeline limits. bufferMaxLength uint64 (Optional) BufferMaxLength is used to define the max length of a buffer. It overrides the settings from pipeline limits. bufferUsageLimit uint32 (Optional) BufferUsageLimit is used to define the percentage of the buffer usage limit, a valid value should be less than 100, for example, 85. It overrides the settings from pipeline limits. rateLimit RateLimit (Optional) RateLimit is used to define the rate limit for the vertex, it overrides the settings from pipeline limits. For Source vertices, the rate limit is defined by how many times the Read is called per second multiplied by the readBatchSize . Pipeline level rate limit is not applied to Source vertices. VertexPhase ( string alias) ( Appears on: VertexLifecycle , VertexStatus ) VertexSpec ( Appears on: Vertex ) Field Description AbstractVertex AbstractVertex (Members of AbstractVertex are embedded into this type.) pipelineName string interStepBufferServiceName string (Optional) replicas int32 (Optional) fromEdges \\[\\]CombinedEdge (Optional) toEdges \\[\\]CombinedEdge (Optional) watermark Watermark (Optional) Watermark indicates watermark progression in the vertex, it\u2019s populated from the pipeline watermark settings. lifecycle VertexLifecycle (Optional) Lifecycle defines the Lifecycle properties of a vertex interStepBuffer InterStepBuffer (Optional) InterStepBuffer configuration specific to this pipeline. VertexStatus ( Appears on: Vertex ) Field Description Status Status (Members of Status are embedded into this type.) phase VertexPhase (Optional) replicas uint32 (Optional) Total number of non-terminated pods targeted by this Vertex (their labels match the selector). desiredReplicas uint32 (Optional) The number of desired replicas. selector string (Optional) reason string (Optional) message string (Optional) lastScaledAt Kubernetes meta/v1.Time (Optional) Time of last scaling operation. observedGeneration int64 (Optional) The generation observed by the Vertex controller. readyReplicas uint32 (Optional) The number of pods targeted by this Vertex with a Ready Condition. updatedReplicas uint32 The number of Pods created by the controller from the Vertex version indicated by updateHash. updatedReadyReplicas uint32 The number of ready Pods created by the controller from the Vertex version indicated by updateHash. currentHash string If not empty, indicates the current version of the Vertex used to generate Pods. updateHash string If not empty, indicates the updated version of the Vertex used to generate Pods. VertexTemplate ( Appears on: Templates ) Field Description AbstractPodTemplate AbstractPodTemplate (Members of AbstractPodTemplate are embedded into this type.) (Optional) containerTemplate ContainerTemplate (Optional) Template for the vertex numa container initContainerTemplate ContainerTemplate (Optional) Template for the vertex init container VertexType ( string alias) ( Appears on: CombinedEdge ) Watermark ( Appears on: PipelineSpec , VertexSpec ) Field Description disabled bool (Optional) Disabled toggles the watermark propagation, defaults to false. maxDelay Kubernetes meta/v1.Duration (Optional) Maximum delay allowed for watermark calculation, defaults to \u201c0s\u201d, which means no delay. idleSource IdleSource (Optional) IdleSource defines the idle watermark properties, it could be configured in case source is idling. Window ( Appears on: GroupBy ) Window describes windowing strategy Field Description fixed FixedWindow (Optional) sliding SlidingWindow (Optional) session SessionWindow (Optional) accumulator AccumulatorWindow (Optional) Generated with gen-crd-api-reference-docs .","title":"APIs"},{"location":"quick-start/","text":"Quick Start \u00b6 This guide will walk you through the following steps: Installing Numaflow Creating and running a simple Numaflow Pipeline Creating and running an advanced Numaflow Pipeline Creating and running a Numaflow MonoVertex Before You Begin: Prerequisites \u00b6 To get started with Numaflow, ensure you have the following tools and setups ready: Container Runtime \u00b6 You need a container runtime to run container images. Choose one of the following options: Docker Desktop Podman Local Kubernetes Cluster \u00b6 Set up a local Kubernetes cluster using one of these tools: Docker Desktop Kubernetes k3d kind minikube Kubernetes CLI ( kubectl ) \u00b6 Install kubectl to manage your Kubernetes cluster. Follow the official guide for installation instructions. If you're unfamiliar with kubectl , refer to the kubectl Quick Reference Page for a list of commonly used commands. Once these prerequisites are in place, you're ready to proceed with installing and using Numaflow. Installing Numaflow \u00b6 After completing the prerequisites, follow these steps to install Numaflow and set up the Inter-Step Buffer Service , which facilitates communication between pipeline vertices: Create a namespace for Numaflow \u00b6 kubectl create ns numaflow-system Install Numaflow components \u00b6 kubectl apply -n numaflow-system -f https://raw.githubusercontent.com/numaproj/numaflow/main/config/install.yaml Deploy the JetStream-based Inter-Step Buffer Service \u00b6 kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/0-isbsvc-jetstream.yaml Once these steps are complete, Numaflow will be ready for use. Creating a Simple Pipeline \u00b6 In this section, we will create a simple pipeline that includes a source vertex to generate messages, a processing vertex that echoes the messages, and a sink vertex to log the messages. Deploy the Simple Pipeline \u00b6 Run the following command to deploy the simple pipeline: kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/1-simple-pipeline.yaml Verify the Pipeline Deployment \u00b6 To view the list of pipelines you have created, use the following command: kubectl get pipeline # or \"pl\" as a short name You should see an output similar to this, where AGE indicates the time elapsed since the pipeline was created: NAME PHASE VERTICES AGE MESSAGE simple-pipeline Running 3 47s Next, inspect the status of the pipeline by checking the pods. Note that the pod names in your environment may differ from the example below: # Wait for pods to be ready kubectl get pods NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3 /3 Running 0 4m9s isbsvc-default-js-1 3 /3 Running 0 4m9s isbsvc-default-js-2 3 /3 Running 0 4m9s simple-pipeline-cat-0-xjqbe 3 /3 Running 0 99s simple-pipeline-daemon-784d5cfd97-vpsmk 1 /1 Running 0 99s simple-pipeline-in-0-vvhu1 2 /2 Running 0 100s simple-pipeline-out-0-y1z8e 2 /2 Running 0 99s View Logs for the Output Vertex \u00b6 To monitor the logs for the output vertex, run the following command. Replace xxxxx with the appropriate pod name from the output above: kubectl logs -f simple-pipeline-out-0-xxxxx You should see logs similar to the following: 2025 /05/09 11 :23:38 ( out ) Payload - { \"Data\" : { \"value\" :1746789818182898304 } , \"Createdts\" :1746789818182898304 } Keys - [ key-0-0 ] EventTime - 1746789818182 Headers - ID - cat-1526-0-0 2025 /05/09 11 :23:38 ( out ) Payload - { \"Data\" : { \"value\" :1746789818182898304 } , \"Createdts\" :1746789818182898304 } Keys - [ key-0-0 ] EventTime - 1746789818182 Headers - ID - cat-1529-0-0 2025 /05/09 11 :23:38 ( out ) Payload - { \"Data\" : { \"value\" :1746789818182898304 } , \"Createdts\" :1746789818182898304 } Keys - [ key-0-0 ] EventTime - 1746789818182 Headers - ID - cat-1528-0-0 Access the Numaflow UI \u00b6 Numaflow includes a built-in user interface for monitoring pipelines. If your local Kubernetes cluster does not include a metrics server by default (e.g., Kind), install it using the following commands: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl patch -n kube-system deployment metrics-server --type = json -p '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/args/-\",\"value\":\"--kubelet-insecure-tls\"}]' To access the UI, port-forward the Numaflow server: kubectl -n numaflow-system port-forward deployment/numaflow-server 8443 :8443 Visit https://localhost:8443/ to view the UI. Below is the UI for the simple pipeline: Cluster View \u00b6 Default Namespace View \u00b6 Simple Pipeline View \u00b6 Note : For more details about the UI features and built-in debugging tools, check out the UI section . Deleting Pipeline \u00b6 To delete the simple pipeline, run the following command: kubectl delete -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/1-simple-pipeline.yaml Creating an Advanced Pipeline \u00b6 Now we will walk you through creating an advanced pipeline. In our example, this is called the even-odd pipeline, illustrated by the following diagram: There are five vertices in this example of an advanced pipeline. An HTTP source vertex which serves an HTTP endpoint to receive numbers as source data, a UDF vertex to tag the ingested numbers with the key even or odd , three Log sinks, one to print the even numbers, one to print the odd numbers, and the other one to print both the even and odd numbers. Deploying the even-odd Pipeline \u00b6 To deploy the even-odd pipeline, run the following command: kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/2-even-odd-pipeline.yaml You can check the list of pipelines you have created so far using: kubectl get pipeline # or \"pl\" as a short name NAME PHASE VERTICES AGE MESSAGE even-odd Running 5 51s Otherwise, proceed to inspect the status of the pipeline by checking the pods: # Wait for pods to be ready kubectl get pods NAME READY STATUS RESTARTS AGE even-odd-daemon-75cdcd5f4c-nmrrp 1 /1 Running 0 95s even-odd-even-or-odd-0-i72hw 3 /3 Running 0 95s even-odd-even-sink-0-gnhou 2 /2 Running 0 95s even-odd-in-0-tvfef 2 /2 Running 0 95s even-odd-number-sink-0-3s3nc 2 /2 Running 0 95s even-odd-odd-sink-0-zktib 2 /2 Running 0 95s isbsvc-default-js-0 3 /3 Running 0 15m isbsvc-default-js-1 3 /3 Running 0 15m isbsvc-default-js-2 3 /3 Running 0 15m Sending Data to the Pipeline \u00b6 Port-forward the HTTP endpoint of the source vertex and send data using curl . Replace xxxxx with the appropriate pod name: kubectl port-forward even-odd-in-0-xxxxx 8444 :8443 # Send data to the HTTP endpoint curl -kq -X POST -d \"101\" https://localhost:8444/vertices/in curl -kq -X POST -d \"102\" https://localhost:8444/vertices/in curl -kq -X POST -d \"103\" https://localhost:8444/vertices/in curl -kq -X POST -d \"104\" https://localhost:8444/vertices/in Viewing Logs for the even and odd Vertices \u00b6 Monitor the logs for the even and odd vertices to verify the pipeline's functionality. Replace xxxxx with the appropriate pod names: # Logs for the even vertex kubectl logs -f even-odd-even-sink-0-xxxxx 2025 /05/09 13 :34:26 ( even-sink ) Payload - 104 Keys - [ even ] EventTime - 1746797665477 Headers - Content-Length: 3 , Content-Type: application/x-www-form-urlencoded, User-Agent: curl/8.7.1, Accept: */*, ID - even-or-odd-1-0-0 2025 /05/09 13 :34:26 ( even-sink ) Payload - 102 Keys - [ even ] EventTime - 1746797665430 Headers - Content-Type: application/x-www-form-urlencoded, User-Agent: curl/8.7.1, Accept: */*, Content-Length: 3 , ID - even-or-odd-2-0-0 # Logs for the odd vertex kubectl logs -f even-odd-odd-sink-0-xxxxx 2025 /05/09 13 :34:26 ( odd-sink ) Payload - 101 Keys - [ odd ] EventTime - 1746797665407 Headers - Content-Length: 3 , Content-Type: application/x-www-form-urlencoded, User-Agent: curl/8.7.1, Accept: */*, ID - even-or-odd-4-0-0 2025 /05/09 13 :34:26 ( odd-sink ) Payload - 103 Keys - [ odd ] EventTime - 1746797665452 Headers - Content-Length: 3 , Content-Type: application/x-www-form-urlencoded, User-Agent: curl/8.7.1, Accept: */*, ID - even-or-odd-3-0-0 Accessing the Numaflow UI \u00b6 To visualize the pipeline, access the Numaflow UI at https://localhost:8443/ after port forwarding. Below is the UI for the even-odd pipeline: Deleting the Pipeline \u00b6 To delete the even-odd pipeline, run: kubectl delete -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/2-even-odd-pipeline.yaml Additional Notes \u00b6 The source code for the even-odd user-defined function is available here . You can also replace the Log Sink with other sinks, such as Kafka , to forward data to Kafka topics. Creating a MonoVertex \u00b6 This section demonstrates how to create and deploy a simple MonoVertex that includes a User-Defined Source (UDSource) , a Transformer , and a User-Defined Sink (UDSink) . The sink in this example is a log sink. Deploying the MonoVertex \u00b6 To deploy the MonoVertex, run the following command: kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/21-simple-mono-vertex.yaml Verifying the Deployment \u00b6 Check the list of deployed MonoVertices using: kubectl get monovertex # or \"mvtx\" as a short name You should see an output similar to this: NAME PHASE DESIRED CURRENT READY AGE REASON MESSAGE simple-mono-vertex Running 1 1 1 38s Inspect the status of the MonoVertex by listing the pods. Replace xxxxx with the appropriate pod name from your environment: # Wait for pods to be ready kubectl get pods NAME READY STATUS RESTARTS AGE simple-mono-vertex-mv-0-w7fmq 5 /5 Running 0 2m30s simple-mono-vertex-mv-daemon-55bff65db5-mk4g2 1 /1 Running 0 2m30s Viewing Pod Details \u00b6 To ensure all containers (monitor, udsource, transformer, udsink, and numa) are running, describe the MonoVertex pod: kubectl describe pod simple-mono-vertex-mv-0-xxxxx Monitoring Logs for the Sink Container \u00b6 To view logs from the udsink container, use the following command. Replace xxxxx with the appropriate pod name: kubectl logs -f simple-mono-vertex-mv-0-xxxxx -c udsink You should see output similar to: 107705990 107705991 107705992 107705993 107705994 107705995 107705996 107705997 107705998 107705999 Accessing the Numaflow UI \u00b6 To visualize the MonoVertex, access the Numaflow UI by port-forwarding the Numaflow server: kubectl -n numaflow-system port-forward deployment/numaflow-server 8443 :8443 Visit https://localhost:8443/ to view the UI. Below is an example of the UI for the MonoVertex: For more details about the UI features and debugging tools, refer to the UI section.(TODO: add UI section link) Deleting the MonoVertex \u00b6 To delete the MonoVertex, run: kubectl delete -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/21-simple-mono-vertex.yaml Additional Notes \u00b6 The source code for the user-defined functions used in this MonoVertex is available here: UDSource Transformer UDSink A pipeline with reduce (aggregation) \u00b6 To set up an example pipeline with the Reduce UDF , see Reduce Examples . What's Next \u00b6 Try more examples in the examples directory. After exploring how Numaflow pipelines run, you can check what data Sources and Sinks Numaflow supports out of the box, or learn how to write User-defined Functions . Numaflow can also be paired with Numalogic, a collection of ML models and algorithms for real-time data analytics and AIOps including anomaly detection. Visit the Numalogic homepage for more information.","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"This guide will walk you through the following steps: Installing Numaflow Creating and running a simple Numaflow Pipeline Creating and running an advanced Numaflow Pipeline Creating and running a Numaflow MonoVertex","title":"Quick Start"},{"location":"quick-start/#before-you-begin-prerequisites","text":"To get started with Numaflow, ensure you have the following tools and setups ready:","title":"Before You Begin: Prerequisites"},{"location":"quick-start/#container-runtime","text":"You need a container runtime to run container images. Choose one of the following options: Docker Desktop Podman","title":"Container Runtime"},{"location":"quick-start/#local-kubernetes-cluster","text":"Set up a local Kubernetes cluster using one of these tools: Docker Desktop Kubernetes k3d kind minikube","title":"Local Kubernetes Cluster"},{"location":"quick-start/#kubernetes-cli-kubectl","text":"Install kubectl to manage your Kubernetes cluster. Follow the official guide for installation instructions. If you're unfamiliar with kubectl , refer to the kubectl Quick Reference Page for a list of commonly used commands. Once these prerequisites are in place, you're ready to proceed with installing and using Numaflow.","title":"Kubernetes CLI (kubectl)"},{"location":"quick-start/#installing-numaflow","text":"After completing the prerequisites, follow these steps to install Numaflow and set up the Inter-Step Buffer Service , which facilitates communication between pipeline vertices:","title":"Installing Numaflow"},{"location":"quick-start/#create-a-namespace-for-numaflow","text":"kubectl create ns numaflow-system","title":"Create a namespace for Numaflow"},{"location":"quick-start/#install-numaflow-components","text":"kubectl apply -n numaflow-system -f https://raw.githubusercontent.com/numaproj/numaflow/main/config/install.yaml","title":"Install Numaflow components"},{"location":"quick-start/#deploy-the-jetstream-based-inter-step-buffer-service","text":"kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/0-isbsvc-jetstream.yaml Once these steps are complete, Numaflow will be ready for use.","title":"Deploy the JetStream-based Inter-Step Buffer Service"},{"location":"quick-start/#creating-a-simple-pipeline","text":"In this section, we will create a simple pipeline that includes a source vertex to generate messages, a processing vertex that echoes the messages, and a sink vertex to log the messages.","title":"Creating a Simple Pipeline"},{"location":"quick-start/#deploy-the-simple-pipeline","text":"Run the following command to deploy the simple pipeline: kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/1-simple-pipeline.yaml","title":"Deploy the Simple Pipeline"},{"location":"quick-start/#verify-the-pipeline-deployment","text":"To view the list of pipelines you have created, use the following command: kubectl get pipeline # or \"pl\" as a short name You should see an output similar to this, where AGE indicates the time elapsed since the pipeline was created: NAME PHASE VERTICES AGE MESSAGE simple-pipeline Running 3 47s Next, inspect the status of the pipeline by checking the pods. Note that the pod names in your environment may differ from the example below: # Wait for pods to be ready kubectl get pods NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3 /3 Running 0 4m9s isbsvc-default-js-1 3 /3 Running 0 4m9s isbsvc-default-js-2 3 /3 Running 0 4m9s simple-pipeline-cat-0-xjqbe 3 /3 Running 0 99s simple-pipeline-daemon-784d5cfd97-vpsmk 1 /1 Running 0 99s simple-pipeline-in-0-vvhu1 2 /2 Running 0 100s simple-pipeline-out-0-y1z8e 2 /2 Running 0 99s","title":"Verify the Pipeline Deployment"},{"location":"quick-start/#view-logs-for-the-output-vertex","text":"To monitor the logs for the output vertex, run the following command. Replace xxxxx with the appropriate pod name from the output above: kubectl logs -f simple-pipeline-out-0-xxxxx You should see logs similar to the following: 2025 /05/09 11 :23:38 ( out ) Payload - { \"Data\" : { \"value\" :1746789818182898304 } , \"Createdts\" :1746789818182898304 } Keys - [ key-0-0 ] EventTime - 1746789818182 Headers - ID - cat-1526-0-0 2025 /05/09 11 :23:38 ( out ) Payload - { \"Data\" : { \"value\" :1746789818182898304 } , \"Createdts\" :1746789818182898304 } Keys - [ key-0-0 ] EventTime - 1746789818182 Headers - ID - cat-1529-0-0 2025 /05/09 11 :23:38 ( out ) Payload - { \"Data\" : { \"value\" :1746789818182898304 } , \"Createdts\" :1746789818182898304 } Keys - [ key-0-0 ] EventTime - 1746789818182 Headers - ID - cat-1528-0-0","title":"View Logs for the Output Vertex"},{"location":"quick-start/#access-the-numaflow-ui","text":"Numaflow includes a built-in user interface for monitoring pipelines. If your local Kubernetes cluster does not include a metrics server by default (e.g., Kind), install it using the following commands: kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl patch -n kube-system deployment metrics-server --type = json -p '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/args/-\",\"value\":\"--kubelet-insecure-tls\"}]' To access the UI, port-forward the Numaflow server: kubectl -n numaflow-system port-forward deployment/numaflow-server 8443 :8443 Visit https://localhost:8443/ to view the UI. Below is the UI for the simple pipeline:","title":"Access the Numaflow UI"},{"location":"quick-start/#cluster-view","text":"","title":"Cluster View"},{"location":"quick-start/#default-namespace-view","text":"","title":"Default Namespace View"},{"location":"quick-start/#simple-pipeline-view","text":"Note : For more details about the UI features and built-in debugging tools, check out the UI section .","title":"Simple Pipeline View"},{"location":"quick-start/#deleting-pipeline","text":"To delete the simple pipeline, run the following command: kubectl delete -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/1-simple-pipeline.yaml","title":"Deleting Pipeline"},{"location":"quick-start/#creating-an-advanced-pipeline","text":"Now we will walk you through creating an advanced pipeline. In our example, this is called the even-odd pipeline, illustrated by the following diagram: There are five vertices in this example of an advanced pipeline. An HTTP source vertex which serves an HTTP endpoint to receive numbers as source data, a UDF vertex to tag the ingested numbers with the key even or odd , three Log sinks, one to print the even numbers, one to print the odd numbers, and the other one to print both the even and odd numbers.","title":"Creating an Advanced Pipeline"},{"location":"quick-start/#deploying-the-even-odd-pipeline","text":"To deploy the even-odd pipeline, run the following command: kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/2-even-odd-pipeline.yaml You can check the list of pipelines you have created so far using: kubectl get pipeline # or \"pl\" as a short name NAME PHASE VERTICES AGE MESSAGE even-odd Running 5 51s Otherwise, proceed to inspect the status of the pipeline by checking the pods: # Wait for pods to be ready kubectl get pods NAME READY STATUS RESTARTS AGE even-odd-daemon-75cdcd5f4c-nmrrp 1 /1 Running 0 95s even-odd-even-or-odd-0-i72hw 3 /3 Running 0 95s even-odd-even-sink-0-gnhou 2 /2 Running 0 95s even-odd-in-0-tvfef 2 /2 Running 0 95s even-odd-number-sink-0-3s3nc 2 /2 Running 0 95s even-odd-odd-sink-0-zktib 2 /2 Running 0 95s isbsvc-default-js-0 3 /3 Running 0 15m isbsvc-default-js-1 3 /3 Running 0 15m isbsvc-default-js-2 3 /3 Running 0 15m","title":"Deploying the even-odd Pipeline"},{"location":"quick-start/#sending-data-to-the-pipeline","text":"Port-forward the HTTP endpoint of the source vertex and send data using curl . Replace xxxxx with the appropriate pod name: kubectl port-forward even-odd-in-0-xxxxx 8444 :8443 # Send data to the HTTP endpoint curl -kq -X POST -d \"101\" https://localhost:8444/vertices/in curl -kq -X POST -d \"102\" https://localhost:8444/vertices/in curl -kq -X POST -d \"103\" https://localhost:8444/vertices/in curl -kq -X POST -d \"104\" https://localhost:8444/vertices/in","title":"Sending Data to the Pipeline"},{"location":"quick-start/#viewing-logs-for-the-even-and-odd-vertices","text":"Monitor the logs for the even and odd vertices to verify the pipeline's functionality. Replace xxxxx with the appropriate pod names: # Logs for the even vertex kubectl logs -f even-odd-even-sink-0-xxxxx 2025 /05/09 13 :34:26 ( even-sink ) Payload - 104 Keys - [ even ] EventTime - 1746797665477 Headers - Content-Length: 3 , Content-Type: application/x-www-form-urlencoded, User-Agent: curl/8.7.1, Accept: */*, ID - even-or-odd-1-0-0 2025 /05/09 13 :34:26 ( even-sink ) Payload - 102 Keys - [ even ] EventTime - 1746797665430 Headers - Content-Type: application/x-www-form-urlencoded, User-Agent: curl/8.7.1, Accept: */*, Content-Length: 3 , ID - even-or-odd-2-0-0 # Logs for the odd vertex kubectl logs -f even-odd-odd-sink-0-xxxxx 2025 /05/09 13 :34:26 ( odd-sink ) Payload - 101 Keys - [ odd ] EventTime - 1746797665407 Headers - Content-Length: 3 , Content-Type: application/x-www-form-urlencoded, User-Agent: curl/8.7.1, Accept: */*, ID - even-or-odd-4-0-0 2025 /05/09 13 :34:26 ( odd-sink ) Payload - 103 Keys - [ odd ] EventTime - 1746797665452 Headers - Content-Length: 3 , Content-Type: application/x-www-form-urlencoded, User-Agent: curl/8.7.1, Accept: */*, ID - even-or-odd-3-0-0","title":"Viewing Logs for the even and odd Vertices"},{"location":"quick-start/#accessing-the-numaflow-ui","text":"To visualize the pipeline, access the Numaflow UI at https://localhost:8443/ after port forwarding. Below is the UI for the even-odd pipeline:","title":"Accessing the Numaflow UI"},{"location":"quick-start/#deleting-the-pipeline","text":"To delete the even-odd pipeline, run: kubectl delete -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/2-even-odd-pipeline.yaml","title":"Deleting the Pipeline"},{"location":"quick-start/#additional-notes","text":"The source code for the even-odd user-defined function is available here . You can also replace the Log Sink with other sinks, such as Kafka , to forward data to Kafka topics.","title":"Additional Notes"},{"location":"quick-start/#creating-a-monovertex","text":"This section demonstrates how to create and deploy a simple MonoVertex that includes a User-Defined Source (UDSource) , a Transformer , and a User-Defined Sink (UDSink) . The sink in this example is a log sink.","title":"Creating a MonoVertex"},{"location":"quick-start/#deploying-the-monovertex","text":"To deploy the MonoVertex, run the following command: kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/21-simple-mono-vertex.yaml","title":"Deploying the MonoVertex"},{"location":"quick-start/#verifying-the-deployment","text":"Check the list of deployed MonoVertices using: kubectl get monovertex # or \"mvtx\" as a short name You should see an output similar to this: NAME PHASE DESIRED CURRENT READY AGE REASON MESSAGE simple-mono-vertex Running 1 1 1 38s Inspect the status of the MonoVertex by listing the pods. Replace xxxxx with the appropriate pod name from your environment: # Wait for pods to be ready kubectl get pods NAME READY STATUS RESTARTS AGE simple-mono-vertex-mv-0-w7fmq 5 /5 Running 0 2m30s simple-mono-vertex-mv-daemon-55bff65db5-mk4g2 1 /1 Running 0 2m30s","title":"Verifying the Deployment"},{"location":"quick-start/#viewing-pod-details","text":"To ensure all containers (monitor, udsource, transformer, udsink, and numa) are running, describe the MonoVertex pod: kubectl describe pod simple-mono-vertex-mv-0-xxxxx","title":"Viewing Pod Details"},{"location":"quick-start/#monitoring-logs-for-the-sink-container","text":"To view logs from the udsink container, use the following command. Replace xxxxx with the appropriate pod name: kubectl logs -f simple-mono-vertex-mv-0-xxxxx -c udsink You should see output similar to: 107705990 107705991 107705992 107705993 107705994 107705995 107705996 107705997 107705998 107705999","title":"Monitoring Logs for the Sink Container"},{"location":"quick-start/#accessing-the-numaflow-ui_1","text":"To visualize the MonoVertex, access the Numaflow UI by port-forwarding the Numaflow server: kubectl -n numaflow-system port-forward deployment/numaflow-server 8443 :8443 Visit https://localhost:8443/ to view the UI. Below is an example of the UI for the MonoVertex: For more details about the UI features and debugging tools, refer to the UI section.(TODO: add UI section link)","title":"Accessing the Numaflow UI"},{"location":"quick-start/#deleting-the-monovertex","text":"To delete the MonoVertex, run: kubectl delete -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/21-simple-mono-vertex.yaml","title":"Deleting the MonoVertex"},{"location":"quick-start/#additional-notes_1","text":"The source code for the user-defined functions used in this MonoVertex is available here: UDSource Transformer UDSink","title":"Additional Notes"},{"location":"quick-start/#a-pipeline-with-reduce-aggregation","text":"To set up an example pipeline with the Reduce UDF , see Reduce Examples .","title":"A pipeline with reduce (aggregation)"},{"location":"quick-start/#whats-next","text":"Try more examples in the examples directory. After exploring how Numaflow pipelines run, you can check what data Sources and Sinks Numaflow supports out of the box, or learn how to write User-defined Functions . Numaflow can also be paired with Numalogic, a collection of ML models and algorithms for real-time data analytics and AIOps including anomaly detection. Visit the Numalogic homepage for more information.","title":"What's Next"},{"location":"core-concepts/inter-step-buffer-service/","text":"Inter-Step Buffer Service \u00b6 Inter-Step Buffer Service is the service to provide Inter-Step Buffers . An Inter-Step Buffer Service is described by a Custom Resource . It is required to be existing in a namespace before Pipeline objects are created. A sample InterStepBufferService with JetStream implementation looks like below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : version : latest # Do NOT use \"latest\" but a specific version in your real deployment InterStepBufferService is a namespaced object. It can be used by all the Pipelines in the same namespace. By default, Pipeline objects look for an InterStepBufferService named default , so a common practice is to create an InterStepBufferService with the name default . If you give the InterStepBufferService a name other than default , then you need to give the same name in the Pipeline spec. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : # Optional, if not specified, defaults to \"default\" interStepBufferServiceName : different-name To query Inter-Step Buffer Service objects with kubectl : kubectl get isbsvc JetStream \u00b6 JetStream is one of the supported Inter-Step Buffer Service implementations. A keyword jetstream under spec means a JetStream cluster will be created in the namespace. For Production Setup , please make sure you configure replicas , persistence , anti-affinity , and PDB . Version \u00b6 Property spec.jetstream.version is required for a JetStream InterStepBufferService . Supported versions can be found from the ConfigMap numaflow-controller-config in the control plane namespace. Note The version latest in the ConfigMap should only be used for testing purpose. It's recommended that you always use a fixed version in your real workload. Replicas \u00b6 An optional property spec.jetstream.replicas (defaults to 3) can be specified, which gives the total number of nodes. Persistence \u00b6 Following example shows a JetStream InterStepBufferService with persistence. apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : version : latest # Do NOT use \"latest\" but a specific version in your real deployment persistence : storageClassName : standard # Optional, will use K8s cluster default storage class if not specified accessMode : ReadWriteOncePod # Optional, defaults to ReadWriteOncePod volumeSize : 10Gi # Optional, defaults to 20Gi Anti-Affinity \u00b6 Anti-affinity is used to spread the ISB pods across different nodes. Example Anti-Affinity \u00b6 apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : version : latest affinity : podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - podAffinityTerm : labelSelector : matchLabels : app.kubernetes.io/component : isbsvc numaflow.numaproj.io/isbsvc-name : default topologyKey : topology.kubernetes.io/zone weight : 100 PDB \u00b6 PDB (Pod Disruption Budget) is essential for running ISB in production to ensure availability. Example PDB Configuration \u00b6 apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : default spec : maxUnavailable : 1 selector : matchLabels : app.kubernetes.io/component : isbsvc numaflow.numaproj.io/isbsvc-name : default JetStream Settings \u00b6 There are 2 places to configure JetStream settings: ConfigMap numaflow-controller-config in the control plane namespace. This is the default configuration for all the JetStream InterStepBufferService created in the Kubernetes cluster. Property spec.jetstream.settings in an InterStepBufferService object. This optional property can be used to override the default configuration defined in the ConfigMap numaflow-controller-config . A sample JetStream configuration: # https://docs.nats.io/running-a-nats-service/configuration#limits # Only \"max_payload\" is supported for configuration in this section. # Max payload size in bytes, defaults to 1 MB. It is not recommended to use values over 8MB but max_payload can be set up to 64MB. max_payload: 1048576 # # https://docs.nats.io/running-a-nats-service/configuration#jetstream # Only configure \"max_memory_store\" or \"max_file_store\" in this section, do not set \"store_dir\" as it has been hardcoded. # # e.g. 1G. -1 means no limit, up to 75% of available memory. This only take effect for streams created using memory storage. max_memory_store: -1 # e.g. 20G. -1 means no limit, Up to 1TB if available max_file_store: 1TB Buffer Configuration \u00b6 For the Inter-Step Buffers created in JetStream ISB Service, there are 2 places to configure the default properties. ConfigMap numaflow-controller-config in the control plane namespace. This is the place to configure the default properties for the streams and consumers created in all the Jet Stream ISB Services in the Kubernetes cluster. Field spec.jetstream.bufferConfig in an InterStepBufferService object. This optional field can be used to customize the stream and consumer properties of that particular InterStepBufferService , and the configuration will be merged into the default one from the ConfigMap numaflow-controller-config . For example, if you only want to change maxMsgs for created streams, then you only need to give stream.maxMsgs in the field, all the rest config will still go with the default values in the control plane ConfigMap. Both these 2 places expect a YAML format configuration like below: bufferConfig : | # The properties of the buffers (streams) to be created in this JetStream service stream: # 0: Limits, 1: Interest, 2: WorkQueue retention: 1 maxMsgs: 30000 maxAge: 168h maxBytes: -1 # 0: File, 1: Memory storage: 0 replicas: 3 duplicates: 60s # The consumer properties for the created streams consumer: ackWait: 60s maxAckPending: 20000 Note Changing the buffer configuration either in the control plane ConfigMap or in the InterStepBufferService object does NOT make any change to the buffers (streams) already existing. TLS \u00b6 TLS is optional to configure through spec.jetstream.tls: true . Enabling TLS will use a self signed CERT to encrypt the connection from Vertex Pods to JetStream service. By default TLS is not enabled. Encryption At Rest \u00b6 Encryption at rest can be enabled by setting spec.jetstream.encryption: true . Be aware this will impact the performance a bit, see the detail at official doc . Once a JetStream ISB Service is created, toggling the encryption field will cause problem for the exiting messages, so if you want to change the value, please delete and recreate the ISB Service, and you also need to restart all the Vertex Pods to pick up the new credentials. Other Configuration \u00b6 Check here for the full spec of spec.jetstream .","title":"Inter-Step Buffer Service"},{"location":"core-concepts/inter-step-buffer-service/#inter-step-buffer-service","text":"Inter-Step Buffer Service is the service to provide Inter-Step Buffers . An Inter-Step Buffer Service is described by a Custom Resource . It is required to be existing in a namespace before Pipeline objects are created. A sample InterStepBufferService with JetStream implementation looks like below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : version : latest # Do NOT use \"latest\" but a specific version in your real deployment InterStepBufferService is a namespaced object. It can be used by all the Pipelines in the same namespace. By default, Pipeline objects look for an InterStepBufferService named default , so a common practice is to create an InterStepBufferService with the name default . If you give the InterStepBufferService a name other than default , then you need to give the same name in the Pipeline spec. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : # Optional, if not specified, defaults to \"default\" interStepBufferServiceName : different-name To query Inter-Step Buffer Service objects with kubectl : kubectl get isbsvc","title":"Inter-Step Buffer Service"},{"location":"core-concepts/inter-step-buffer-service/#jetstream","text":"JetStream is one of the supported Inter-Step Buffer Service implementations. A keyword jetstream under spec means a JetStream cluster will be created in the namespace. For Production Setup , please make sure you configure replicas , persistence , anti-affinity , and PDB .","title":"JetStream"},{"location":"core-concepts/inter-step-buffer-service/#version","text":"Property spec.jetstream.version is required for a JetStream InterStepBufferService . Supported versions can be found from the ConfigMap numaflow-controller-config in the control plane namespace. Note The version latest in the ConfigMap should only be used for testing purpose. It's recommended that you always use a fixed version in your real workload.","title":"Version"},{"location":"core-concepts/inter-step-buffer-service/#replicas","text":"An optional property spec.jetstream.replicas (defaults to 3) can be specified, which gives the total number of nodes.","title":"Replicas"},{"location":"core-concepts/inter-step-buffer-service/#persistence","text":"Following example shows a JetStream InterStepBufferService with persistence. apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : version : latest # Do NOT use \"latest\" but a specific version in your real deployment persistence : storageClassName : standard # Optional, will use K8s cluster default storage class if not specified accessMode : ReadWriteOncePod # Optional, defaults to ReadWriteOncePod volumeSize : 10Gi # Optional, defaults to 20Gi","title":"Persistence"},{"location":"core-concepts/inter-step-buffer-service/#anti-affinity","text":"Anti-affinity is used to spread the ISB pods across different nodes.","title":"Anti-Affinity"},{"location":"core-concepts/inter-step-buffer-service/#example-anti-affinity","text":"apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : version : latest affinity : podAntiAffinity : preferredDuringSchedulingIgnoredDuringExecution : - podAffinityTerm : labelSelector : matchLabels : app.kubernetes.io/component : isbsvc numaflow.numaproj.io/isbsvc-name : default topologyKey : topology.kubernetes.io/zone weight : 100","title":"Example Anti-Affinity"},{"location":"core-concepts/inter-step-buffer-service/#pdb","text":"PDB (Pod Disruption Budget) is essential for running ISB in production to ensure availability.","title":"PDB"},{"location":"core-concepts/inter-step-buffer-service/#example-pdb-configuration","text":"apiVersion : policy/v1 kind : PodDisruptionBudget metadata : name : default spec : maxUnavailable : 1 selector : matchLabels : app.kubernetes.io/component : isbsvc numaflow.numaproj.io/isbsvc-name : default","title":"Example PDB Configuration"},{"location":"core-concepts/inter-step-buffer-service/#jetstream-settings","text":"There are 2 places to configure JetStream settings: ConfigMap numaflow-controller-config in the control plane namespace. This is the default configuration for all the JetStream InterStepBufferService created in the Kubernetes cluster. Property spec.jetstream.settings in an InterStepBufferService object. This optional property can be used to override the default configuration defined in the ConfigMap numaflow-controller-config . A sample JetStream configuration: # https://docs.nats.io/running-a-nats-service/configuration#limits # Only \"max_payload\" is supported for configuration in this section. # Max payload size in bytes, defaults to 1 MB. It is not recommended to use values over 8MB but max_payload can be set up to 64MB. max_payload: 1048576 # # https://docs.nats.io/running-a-nats-service/configuration#jetstream # Only configure \"max_memory_store\" or \"max_file_store\" in this section, do not set \"store_dir\" as it has been hardcoded. # # e.g. 1G. -1 means no limit, up to 75% of available memory. This only take effect for streams created using memory storage. max_memory_store: -1 # e.g. 20G. -1 means no limit, Up to 1TB if available max_file_store: 1TB","title":"JetStream Settings"},{"location":"core-concepts/inter-step-buffer-service/#buffer-configuration","text":"For the Inter-Step Buffers created in JetStream ISB Service, there are 2 places to configure the default properties. ConfigMap numaflow-controller-config in the control plane namespace. This is the place to configure the default properties for the streams and consumers created in all the Jet Stream ISB Services in the Kubernetes cluster. Field spec.jetstream.bufferConfig in an InterStepBufferService object. This optional field can be used to customize the stream and consumer properties of that particular InterStepBufferService , and the configuration will be merged into the default one from the ConfigMap numaflow-controller-config . For example, if you only want to change maxMsgs for created streams, then you only need to give stream.maxMsgs in the field, all the rest config will still go with the default values in the control plane ConfigMap. Both these 2 places expect a YAML format configuration like below: bufferConfig : | # The properties of the buffers (streams) to be created in this JetStream service stream: # 0: Limits, 1: Interest, 2: WorkQueue retention: 1 maxMsgs: 30000 maxAge: 168h maxBytes: -1 # 0: File, 1: Memory storage: 0 replicas: 3 duplicates: 60s # The consumer properties for the created streams consumer: ackWait: 60s maxAckPending: 20000 Note Changing the buffer configuration either in the control plane ConfigMap or in the InterStepBufferService object does NOT make any change to the buffers (streams) already existing.","title":"Buffer Configuration"},{"location":"core-concepts/inter-step-buffer-service/#tls","text":"TLS is optional to configure through spec.jetstream.tls: true . Enabling TLS will use a self signed CERT to encrypt the connection from Vertex Pods to JetStream service. By default TLS is not enabled.","title":"TLS"},{"location":"core-concepts/inter-step-buffer-service/#encryption-at-rest","text":"Encryption at rest can be enabled by setting spec.jetstream.encryption: true . Be aware this will impact the performance a bit, see the detail at official doc . Once a JetStream ISB Service is created, toggling the encryption field will cause problem for the exiting messages, so if you want to change the value, please delete and recreate the ISB Service, and you also need to restart all the Vertex Pods to pick up the new credentials.","title":"Encryption At Rest"},{"location":"core-concepts/inter-step-buffer-service/#other-configuration","text":"Check here for the full spec of spec.jetstream .","title":"Other Configuration"},{"location":"core-concepts/inter-step-buffer/","text":"Inter-Step Buffer \u00b6 A Pipeline contains multiple vertices that ingest data from sources, process data, and forward processed data to sinks. Vertices are not connected directly, but through Inter-Step Buffers. Inter-Step Buffer can be implemented by a variety of data buffering technologies. Those technologies should support: Durability Offsets Transactions for Exactly-Once forwarding Concurrent reading Ability to explicitly acknowledge each data or offset Claim pending messages (read but not acknowledge) Ability to trim data (buffer size control) Fast (high throughput low latency) Ability to query buffer information Supported Inter-Step Buffer implementations: Nats JetStream","title":"Inter-Step Buffer"},{"location":"core-concepts/inter-step-buffer/#inter-step-buffer","text":"A Pipeline contains multiple vertices that ingest data from sources, process data, and forward processed data to sinks. Vertices are not connected directly, but through Inter-Step Buffers. Inter-Step Buffer can be implemented by a variety of data buffering technologies. Those technologies should support: Durability Offsets Transactions for Exactly-Once forwarding Concurrent reading Ability to explicitly acknowledge each data or offset Claim pending messages (read but not acknowledge) Ability to trim data (buffer size control) Fast (high throughput low latency) Ability to query buffer information Supported Inter-Step Buffer implementations: Nats JetStream","title":"Inter-Step Buffer"},{"location":"core-concepts/message-headers/","text":"Message Headers \u00b6 Message Headers are metadata key-value pairs attached to each message (also called a datum) as it flows through the Numaflow pipeline. They provide contextual information about the message\u2014such as event timestamps, routing keys, or custom tags\u2014without modifying the message's main content. Message headers are immutable and cannot be manipulated through the SDKs. Use Cases \u00b6 Propagating Source Metadata Carry metadata from external systems (like Kafka or HTTP) into the pipeline by copying their headers into message headers. Tracing and Debugging Attach unique IDs or trace information to each message, making it easier to follow a message's journey through the pipeline for troubleshooting. Conditional Routing and Processing Use header values (such as priority, type, or custom flags) to decide how messages are routed or processed at different steps. Auditing and Compliance Store audit information (e.g., who triggered the message, when it was created) in headers for logging and compliance tracking. Custom Enrichment in UDFs UDFs can read message headers to enrich processing logic, but they cannot modify or add new headers. Feature Flags and Experimentation Pass feature flags or experiment IDs in headers to enable A/B testing or canary deployments within your data processing logic. Application References \u00b6 Kafka Source Example In pkg/sources/kafka/reader.go , when a message is read from Kafka, all Kafka headers are copied into the Numaflow message headers. This allows downstream components to access all original Kafka metadata. HTTP Source Example In pkg/sources/http/http.go , HTTP headers from incoming requests are added to the message headers. This means any custom or standard HTTP header is available throughout the pipeline. Accessing Headers in UDFs \u26a0\ufe0f Note: Headers are read-only in UDFs. You can access them, but not modify or add new headers. Go SDK The datum object provides a map[string]string of all headers: headers := datum . Headers () fmt . Println ( headers [ \"X-My-Header\" ]) Python SDK The datum object exposes headers as a dictionary: headers = datum . headers print ( headers . get ( \"X-My-Header\" )) Under the hood, header information is preserved and passed through the dataplane, making it accessible across all SDKs. How Message Headers Work in Numaflow \u00b6 Creation at the Source Headers are created at the beginning of the pipeline. For example, Kafka headers or HTTP request headers are copied into the Numaflow message when the source ingests it. Propagation Through the Pipeline As the datum flows through the pipeline\u2014from source to UDF to sink\u2014the headers are preserved and passed through each vertex and buffer. Reading in UDFs Developers can access headers using the SDKs to make decisions such as filtering, routing, or branching logic. Headers are read-only and cannot be modified or added in UDFs. Delivery to the Sink When the datum reaches the sink, the headers are still attached. Sinks can use this data for routing, logging, or integration with external systems. This end-to-end propagation ensures metadata remains intact and useful across the full lifecycle of a message. Example Pipeline with Headers (HTTP Source) \u00b6 Here's a minimal example of a pipeline where HTTP headers are passed from an HTTP source to a UDF, then to a log sink: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : http-header-demo spec : vertices : - name : in source : http : {} - name : process udf : container : # A simple UDF that logs the message and its headers image : quay.io/numaproj/numaflow-go/map-log:v1.2.1 - name : out sink : log : {} edges : - from : in to : process - from : process to : out You can send a request to the HTTP source with custom headers using curl . First, port-forward the pod (replace ${pod-name} with your actual pod name): kubectl port-forward pod/ ${ pod -name } 8443 :8443 Then, send the request: curl -k -H \"X-My-Header: test-value\" -d \"hello\" https://localhost:8443/vertices/in In your UDF, X-My-Header will be available as a message header.","title":"Message Headers"},{"location":"core-concepts/message-headers/#message-headers","text":"Message Headers are metadata key-value pairs attached to each message (also called a datum) as it flows through the Numaflow pipeline. They provide contextual information about the message\u2014such as event timestamps, routing keys, or custom tags\u2014without modifying the message's main content. Message headers are immutable and cannot be manipulated through the SDKs.","title":"Message Headers"},{"location":"core-concepts/message-headers/#use-cases","text":"Propagating Source Metadata Carry metadata from external systems (like Kafka or HTTP) into the pipeline by copying their headers into message headers. Tracing and Debugging Attach unique IDs or trace information to each message, making it easier to follow a message's journey through the pipeline for troubleshooting. Conditional Routing and Processing Use header values (such as priority, type, or custom flags) to decide how messages are routed or processed at different steps. Auditing and Compliance Store audit information (e.g., who triggered the message, when it was created) in headers for logging and compliance tracking. Custom Enrichment in UDFs UDFs can read message headers to enrich processing logic, but they cannot modify or add new headers. Feature Flags and Experimentation Pass feature flags or experiment IDs in headers to enable A/B testing or canary deployments within your data processing logic.","title":"Use Cases"},{"location":"core-concepts/message-headers/#application-references","text":"Kafka Source Example In pkg/sources/kafka/reader.go , when a message is read from Kafka, all Kafka headers are copied into the Numaflow message headers. This allows downstream components to access all original Kafka metadata. HTTP Source Example In pkg/sources/http/http.go , HTTP headers from incoming requests are added to the message headers. This means any custom or standard HTTP header is available throughout the pipeline. Accessing Headers in UDFs \u26a0\ufe0f Note: Headers are read-only in UDFs. You can access them, but not modify or add new headers. Go SDK The datum object provides a map[string]string of all headers: headers := datum . Headers () fmt . Println ( headers [ \"X-My-Header\" ]) Python SDK The datum object exposes headers as a dictionary: headers = datum . headers print ( headers . get ( \"X-My-Header\" )) Under the hood, header information is preserved and passed through the dataplane, making it accessible across all SDKs.","title":"Application References"},{"location":"core-concepts/message-headers/#how-message-headers-work-in-numaflow","text":"Creation at the Source Headers are created at the beginning of the pipeline. For example, Kafka headers or HTTP request headers are copied into the Numaflow message when the source ingests it. Propagation Through the Pipeline As the datum flows through the pipeline\u2014from source to UDF to sink\u2014the headers are preserved and passed through each vertex and buffer. Reading in UDFs Developers can access headers using the SDKs to make decisions such as filtering, routing, or branching logic. Headers are read-only and cannot be modified or added in UDFs. Delivery to the Sink When the datum reaches the sink, the headers are still attached. Sinks can use this data for routing, logging, or integration with external systems. This end-to-end propagation ensures metadata remains intact and useful across the full lifecycle of a message.","title":"How Message Headers Work in Numaflow"},{"location":"core-concepts/message-headers/#example-pipeline-with-headers-http-source","text":"Here's a minimal example of a pipeline where HTTP headers are passed from an HTTP source to a UDF, then to a log sink: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : http-header-demo spec : vertices : - name : in source : http : {} - name : process udf : container : # A simple UDF that logs the message and its headers image : quay.io/numaproj/numaflow-go/map-log:v1.2.1 - name : out sink : log : {} edges : - from : in to : process - from : process to : out You can send a request to the HTTP source with custom headers using curl . First, port-forward the pod (replace ${pod-name} with your actual pod name): kubectl port-forward pod/ ${ pod -name } 8443 :8443 Then, send the request: curl -k -H \"X-My-Header: test-value\" -d \"hello\" https://localhost:8443/vertices/in In your UDF, X-My-Header will be available as a message header.","title":"Example Pipeline with Headers (HTTP Source)"},{"location":"core-concepts/monovertex/","text":"MonoVertex \u00b6 MonoVertex is a simplified version of the Pipeline . The major design idea behind MonoVertex is to simplify data-processing in those cases where independently autoscaling vertices are not required. This means there will only be a single vertex called the MonoVertex, which is capable of running the Source , the Sink (with Fallback Sink if need be), and optionally the Transformer together. There is no concept of Edges in MonoVertex since there is only one Vertex . The MonoVertex runs the same containers run by the Pipeline , this means the users can switch between MonoVertex and Pipeline by just changing the spec. MonoVertex\u2019s autoscaling is similar to the Source vertex of a Pipeline , where the vertex scales out if the pending at the Source is increasing, and scales down when the pending at the Source drops. The major benefits of MonoVertex are as follows: Very high TPS can be supported because there are no ISBs (RAFT consensus) in between. Extremely low latency because there is no network hops between the Vertices (only one Vertex). Low Operational overhead, no need to maintain ISBs. MonoVertex - Condensed Pipeline with just one Vertex Use cases of MonoVertex \u00b6 There are a few scenarios where MonoVertex is the best fit, which can be best described as use-cases where \u201cthe user is reading from the Source, doing simple transformations, and writing to a Sink\u201d. You don't need to scale these Source, Sink, or Map vertices by themselves. If the Sink can't keep up, you just need to add more pods. This brings a new set of Source, Transformer, and a Sink (optionally with a backup sink). The same goes for if the Source cannot keep up (pending is increasing), we just need to add more pods. A Brief History \u00b6 MonoVertex was developed because we found that, there are a decent amount of applications that read from Sources like Kafka, Pulsar, etc., and write to a User-Defined Sink (DBs, etc). Often, the transformation itself was not required because the User-Defined Sink can do the transformation before persisting the data in to the Sink. The extra hop through the ISB was not helping but just making things slower by adding more delays. Throughput was also getting limited to about 50K unless larger ISBs were used. When not to use MonoVertex \u00b6 The rule of thumb is, if you are just reading from the source and writing to a sink (with some transformation), then you might be able to get away with MonoVertex. For all other use cases, use the full Pipeline Semantics. Below are a few examples where you cannot use MonoVertex. Map is not supported, only Transformer can be used. If you need Map support, please let us know by creating an issue, If you are using the Reduce feature, then the full Pipeline semantics is required. This is because of the need for shuffling of data. There are cases where you want to autoscale intermediate process nodes, especially in the case of ML workloads. If there is a need for custom placement of vertices (e.g., GPU nodes for Inference), then the complete pipeline spec is required. If you want to use the Join feature, then MonoVertex will not work. Anatomy of MonoVertex \u00b6 MonoVertex supports the same Sources, Sinks, and Transformers which are used in the Pipeline spec. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : simple-mono-vertex spec : source : # same as the Pipeline Source # ... transformer : # transformer is an optional container to do any transformation to the incoming data before passing to the sink # same as the Pipeline Transformer # ... sink : # same as the Pipeline Sink # ... Please keep in mind that, moving from a MonoVertex to Pipeline does not require code change, only spec change. Example \u00b6 A simple example using user-defined source, transformer, and sink.","title":"MonoVertex"},{"location":"core-concepts/monovertex/#monovertex","text":"MonoVertex is a simplified version of the Pipeline . The major design idea behind MonoVertex is to simplify data-processing in those cases where independently autoscaling vertices are not required. This means there will only be a single vertex called the MonoVertex, which is capable of running the Source , the Sink (with Fallback Sink if need be), and optionally the Transformer together. There is no concept of Edges in MonoVertex since there is only one Vertex . The MonoVertex runs the same containers run by the Pipeline , this means the users can switch between MonoVertex and Pipeline by just changing the spec. MonoVertex\u2019s autoscaling is similar to the Source vertex of a Pipeline , where the vertex scales out if the pending at the Source is increasing, and scales down when the pending at the Source drops. The major benefits of MonoVertex are as follows: Very high TPS can be supported because there are no ISBs (RAFT consensus) in between. Extremely low latency because there is no network hops between the Vertices (only one Vertex). Low Operational overhead, no need to maintain ISBs. MonoVertex - Condensed Pipeline with just one Vertex","title":"MonoVertex"},{"location":"core-concepts/monovertex/#use-cases-of-monovertex","text":"There are a few scenarios where MonoVertex is the best fit, which can be best described as use-cases where \u201cthe user is reading from the Source, doing simple transformations, and writing to a Sink\u201d. You don't need to scale these Source, Sink, or Map vertices by themselves. If the Sink can't keep up, you just need to add more pods. This brings a new set of Source, Transformer, and a Sink (optionally with a backup sink). The same goes for if the Source cannot keep up (pending is increasing), we just need to add more pods.","title":"Use cases of MonoVertex"},{"location":"core-concepts/monovertex/#a-brief-history","text":"MonoVertex was developed because we found that, there are a decent amount of applications that read from Sources like Kafka, Pulsar, etc., and write to a User-Defined Sink (DBs, etc). Often, the transformation itself was not required because the User-Defined Sink can do the transformation before persisting the data in to the Sink. The extra hop through the ISB was not helping but just making things slower by adding more delays. Throughput was also getting limited to about 50K unless larger ISBs were used.","title":"A Brief History"},{"location":"core-concepts/monovertex/#when-not-to-use-monovertex","text":"The rule of thumb is, if you are just reading from the source and writing to a sink (with some transformation), then you might be able to get away with MonoVertex. For all other use cases, use the full Pipeline Semantics. Below are a few examples where you cannot use MonoVertex. Map is not supported, only Transformer can be used. If you need Map support, please let us know by creating an issue, If you are using the Reduce feature, then the full Pipeline semantics is required. This is because of the need for shuffling of data. There are cases where you want to autoscale intermediate process nodes, especially in the case of ML workloads. If there is a need for custom placement of vertices (e.g., GPU nodes for Inference), then the complete pipeline spec is required. If you want to use the Join feature, then MonoVertex will not work.","title":"When not to use MonoVertex"},{"location":"core-concepts/monovertex/#anatomy-of-monovertex","text":"MonoVertex supports the same Sources, Sinks, and Transformers which are used in the Pipeline spec. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : simple-mono-vertex spec : source : # same as the Pipeline Source # ... transformer : # transformer is an optional container to do any transformation to the incoming data before passing to the sink # same as the Pipeline Transformer # ... sink : # same as the Pipeline Sink # ... Please keep in mind that, moving from a MonoVertex to Pipeline does not require code change, only spec change.","title":"Anatomy of MonoVertex"},{"location":"core-concepts/monovertex/#example","text":"A simple example using user-defined source, transformer, and sink.","title":"Example"},{"location":"core-concepts/overview/","text":"Numaflow Core Concepts Overview \u00b6 Numaflow is a Kubernetes-native platform for event processing at scale, purpose-built for creating event-driven applications, real-time stream processing pipelines, and serving systems. It provides a set of modular and composable building blocks to help developers process events and serve efficiently in cloud-native environments. The following sections introduce the core concepts of Numaflow. MonoVertex Pipeline Serving MonoVertex \u00b6 MonoVertex is a lightweight way to develop event-driven applications in Numaflow. Each MonoVertex contains source, transformer, and sink within a single unit, making it ideal for simple event processing tasks. The entire unit scales as one, making it a lightweight option for handling simple event processing patterns. Use Cases \u00b6 Stateless event transformation Filtering, mapping, or enriching event data Microservice-style event handling (e.g., call external APIs per event) Ingesting from or writing to external systems within a single unit Pipeline \u00b6 Pipeline is designed for developing real-time stream processing pipelines. It allows you to connect multiple vertices \u2014 each representing a processing step \u2014 to handle data. Pipelines can include multiple sources and sinks, enabling integration with diverse systems at both the input and output stages. Each step in a pipeline can perform operations like transformation, routing, reduction, or aggregation, and is independently scalable. This makes Pipelines ideal for building robust, modular stream processing applications. Use Cases \u00b6 Aggregation and windowing (e.g., sum, count, reduce over time windows) Routing events based on payload or metadata Multi-step data processing and enrichment Scalable data pipelines for analytics and monitoring Serving \u00b6 Serving enables request/response interaction with streaming systems, allowing external clients to send data and receive processed results in real time through interfaces like REST or Server-Sent Events (SSE). Use Cases \u00b6 Machine learning inference (e.g., send features, receive predictions) Event-driven APIs backed by stream processing logic Asynchronous request/response workflows over SSE or HTTP","title":"Overview"},{"location":"core-concepts/overview/#numaflow-core-concepts-overview","text":"Numaflow is a Kubernetes-native platform for event processing at scale, purpose-built for creating event-driven applications, real-time stream processing pipelines, and serving systems. It provides a set of modular and composable building blocks to help developers process events and serve efficiently in cloud-native environments. The following sections introduce the core concepts of Numaflow. MonoVertex Pipeline Serving","title":"Numaflow Core Concepts Overview"},{"location":"core-concepts/overview/#monovertex","text":"MonoVertex is a lightweight way to develop event-driven applications in Numaflow. Each MonoVertex contains source, transformer, and sink within a single unit, making it ideal for simple event processing tasks. The entire unit scales as one, making it a lightweight option for handling simple event processing patterns.","title":"MonoVertex"},{"location":"core-concepts/overview/#use-cases","text":"Stateless event transformation Filtering, mapping, or enriching event data Microservice-style event handling (e.g., call external APIs per event) Ingesting from or writing to external systems within a single unit","title":"Use Cases"},{"location":"core-concepts/overview/#pipeline","text":"Pipeline is designed for developing real-time stream processing pipelines. It allows you to connect multiple vertices \u2014 each representing a processing step \u2014 to handle data. Pipelines can include multiple sources and sinks, enabling integration with diverse systems at both the input and output stages. Each step in a pipeline can perform operations like transformation, routing, reduction, or aggregation, and is independently scalable. This makes Pipelines ideal for building robust, modular stream processing applications.","title":"Pipeline"},{"location":"core-concepts/overview/#use-cases_1","text":"Aggregation and windowing (e.g., sum, count, reduce over time windows) Routing events based on payload or metadata Multi-step data processing and enrichment Scalable data pipelines for analytics and monitoring","title":"Use Cases"},{"location":"core-concepts/overview/#serving","text":"Serving enables request/response interaction with streaming systems, allowing external clients to send data and receive processed results in real time through interfaces like REST or Server-Sent Events (SSE).","title":"Serving"},{"location":"core-concepts/overview/#use-cases_2","text":"Machine learning inference (e.g., send features, receive predictions) Event-driven APIs backed by stream processing logic Asynchronous request/response workflows over SSE or HTTP","title":"Use Cases"},{"location":"core-concepts/pipeline/","text":"Pipeline \u00b6 The Pipeline represents a data processing job (a simpler version of this is called MonoVertex ). The most important concept in Numaflow, it defines: A list of vertices , which define the data processing tasks; A list of edges , which are used to describe the relationship between the vertices. Note an edge may go from a vertex to multiple vertices, and an edge may also go from multiple vertices to a vertex. This many-to-one relationship is possible via Join and Cycles The Pipeline is abstracted as a Kubernetes Custom Resource . A Pipeline spec looks like below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : in source : generator : rpu : 5 duration : 1s - name : cat udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always - name : out sink : log : {} edges : - from : in to : cat - from : cat to : out To query Pipeline objects with kubectl : kubectl get pipeline # or \"pl\" as a short name","title":"Pipeline"},{"location":"core-concepts/pipeline/#pipeline","text":"The Pipeline represents a data processing job (a simpler version of this is called MonoVertex ). The most important concept in Numaflow, it defines: A list of vertices , which define the data processing tasks; A list of edges , which are used to describe the relationship between the vertices. Note an edge may go from a vertex to multiple vertices, and an edge may also go from multiple vertices to a vertex. This many-to-one relationship is possible via Join and Cycles The Pipeline is abstracted as a Kubernetes Custom Resource . A Pipeline spec looks like below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : in source : generator : rpu : 5 duration : 1s - name : cat udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always - name : out sink : log : {} edges : - from : in to : cat - from : cat to : out To query Pipeline objects with kubectl : kubectl get pipeline # or \"pl\" as a short name","title":"Pipeline"},{"location":"core-concepts/serving/","text":"ServingPipeline \u00b6 The ServingPipeline is a specialized Numaflow resource designed to expose a standard Numaflow data processing Pipeline as an interactive HTTP service. The major design idea behind ServingPipeline is to bridge traditional request/response API patterns with the power of Numaflow's stream processing capabilities, allowing external clients to directly inject data, trigger processing, and retrieve results via familiar REST or Server-Sent Events (SSE) mechanisms. A ServingPipeline consists of two main parts: An HTTP Serving Layer: Defined in spec.serving , this configures the external interface, including service exposure, request identification (via headers), timeouts, authentication, and result storage options. A Standard Numaflow Pipeline: Defined in spec.pipeline , this contains the familiar vertices and edges for data processing. However, it must use a specific serving source as its entry point and conclude with a User Defined Sink (UDSink) capable of handling serving responses (using ResponseServe ). This structure enables features like synchronous ( /sync ), asynchronous ( /async ), and streaming ( /sse ) API endpoints, along with request tracking via unique IDs. Unlike a standard Pipeline focused purely on stream processing, ServingPipeline adds this interactive HTTP request/response lifecycle management layer on top. The major benefits of ServingPipeline are as follows: API Exposure: Easily expose complex stream processing logic or ML models within Numaflow pipelines via standard RESTful APIs. Interactive Workflows: Supports common application patterns requiring synchronous request/response or traceable asynchronous tasks initiated via HTTP. Flexible Interaction: Offers synchronous, asynchronous, and streaming (Server-Sent Events) options for clients to receive results based on their needs. Traceability: Built-in request ID mechanism allows tracking individual requests and retrieving their specific results or status. Use Cases of ServingPipeline \u00b6 ServingPipeline is ideal for scenarios where external systems or users need to interact directly with a Numaflow pipeline via HTTP: ML Model Serving: Deploying a machine learning model within a Numaflow pipeline (e.g., for pre/post-processing) and exposing it as a real-time inference API endpoint. Interactive Data Services: Building services for data validation, enrichment, or transformation where clients submit data via an API call and receive the processed result. API Gateway Pattern: Using ServingPipeline as a front-end to trigger event-driven backends processed by Numaflow, potentially returning a final status or result synchronously or asynchronously. Traceable Asynchronous Jobs: Kicking off complex, multi-step processing within Numaflow via an API call and allowing the client to poll for status or results later using a unique request ID. Anatomy of ServingPipeline \u00b6 A ServingPipeline resource defines both the serving layer configuration ( spec.serving ) and the underlying processing pipeline ( spec.pipeline ). apiVersion : numaflow.numaproj.io/v1alpha1 kind : ServingPipeline metadata : name : serving-pipeline-custom-store # Example name spec : # Configures the HTTP serving aspects serving : ... serving configuration options ... # Defines the underlying Numaflow pipeline for processing pipeline : ... pipeline ... Configuration ( spec.serving ) \u00b6 This section configures the HTTP interface: service (boolean): If true , automatically creates a Kubernetes Service. msgIDHeaderKey (string): The HTTP header key for the unique Request ID. requestTimeoutSeconds (integer): Timeout for /sync and /sse requests (default: 120 ). auth : Optional configuration for token-based authentication using a Kubernetes Secret . store : Optional configuration for a custom result storage backend. If omitted, internal storage (e.g., JetStream) is used. See Custom Results Store below. Custom Results Store ( spec.serving.store ) \u00b6 You can provide a custom storage backend by specifying a container image in spec.serving.store.container . This container must implement a specific gRPC interface (defined in the Numaflow SDKs) for storing ( Put ) and retrieving ( Get ) results associated with request IDs. This allows using preferred databases or caches. Refer to the Numaflow SDK documentation for your language for the exact interface. The Go interface requires methods like: type ServingStorer interface { Put ( ctx context . Context , put PutDatum ) Get ( ctx context . Context , get GetDatum ) StoredResult } // PutDatum, GetDatum, StoredResult provide necessary details A Golang example can be found here . User-Defined Sink Implementation for Serving \u00b6 The User Defined Sink (UDSink) in a ServingPipeline 's pipeline ( spec.pipeline.vertices[].sink.udsink ) must signal the final response payload for the original HTTP request. Use the ResponseServe(requestID, resultBytes) function (or SDK equivalent) in your sink code. Example (Go SDK) \u00b6 type serveSink struct {} func ( l * serveSink ) Sink ( ctx context . Context , datumStreamCh <- chan sinksdk . Datum ) sinksdk . Responses { result := sinksdk . ResponsesBuilder () for d := range datumStreamCh { id := d . ID () // Original Request ID val := d . Value () // Final payload from pipeline // Use ResponseServe to mark 'val' as the result for 'id' result = result . Append ( sinksdk . ResponseServe ( id , val )) } return result } A complete example can be found here . Using ResponseServe ensures the result is correctly stored and available via the API endpoints. API Endpoints \u00b6 The ServingPipeline exposes the following endpoints: POST /v1/process/sync : Submit data, wait for result in response body. POST /v1/process/async : Submit data, return immediately; fetch result later. GET /v1/process/fetch?id=<request_id> : Retrieve status/result(s) for a request ID. GET /v1/process/sse?id=<request_id> : Stream results using Server-Sent Events. GET /v1/process/message?id=<request_id> : Get message path info. Interaction Example \u00b6 Submit data synchronously: curl -k -XPOST \\ --header 'X-Numaflow-Id: job-456' \\ --header 'content-type: application/json' \\ --header 'Authorization: Bearer <your-token-if-auth-enabled>' \\ --data '{\"value\":123}' \\ --url https://<serving-pipeline-service-address>:8443/v1/process/sync Fetch results later: curl -k \\ --header 'Authorization: Bearer <your-token-if-auth-enabled>' \\ --url https://<serving-pipeline-service-address>:8443/v1/process/fetch?id = job-456 Stream results using SSE: curl -k -N \\ --header 'X-Numaflow-Id: stream-789' \\ --header 'Authorization: Bearer <your-token-if-auth-enabled>' \\ --url https://<serving-pipeline-service-address>:8443/v1/process/sse?id = stream-789 # Client receives events as results become available To query ServingPipeline objects with kubectl : kubectl get servingpipeline","title":"ServingPipeline"},{"location":"core-concepts/serving/#servingpipeline","text":"The ServingPipeline is a specialized Numaflow resource designed to expose a standard Numaflow data processing Pipeline as an interactive HTTP service. The major design idea behind ServingPipeline is to bridge traditional request/response API patterns with the power of Numaflow's stream processing capabilities, allowing external clients to directly inject data, trigger processing, and retrieve results via familiar REST or Server-Sent Events (SSE) mechanisms. A ServingPipeline consists of two main parts: An HTTP Serving Layer: Defined in spec.serving , this configures the external interface, including service exposure, request identification (via headers), timeouts, authentication, and result storage options. A Standard Numaflow Pipeline: Defined in spec.pipeline , this contains the familiar vertices and edges for data processing. However, it must use a specific serving source as its entry point and conclude with a User Defined Sink (UDSink) capable of handling serving responses (using ResponseServe ). This structure enables features like synchronous ( /sync ), asynchronous ( /async ), and streaming ( /sse ) API endpoints, along with request tracking via unique IDs. Unlike a standard Pipeline focused purely on stream processing, ServingPipeline adds this interactive HTTP request/response lifecycle management layer on top. The major benefits of ServingPipeline are as follows: API Exposure: Easily expose complex stream processing logic or ML models within Numaflow pipelines via standard RESTful APIs. Interactive Workflows: Supports common application patterns requiring synchronous request/response or traceable asynchronous tasks initiated via HTTP. Flexible Interaction: Offers synchronous, asynchronous, and streaming (Server-Sent Events) options for clients to receive results based on their needs. Traceability: Built-in request ID mechanism allows tracking individual requests and retrieving their specific results or status.","title":"ServingPipeline"},{"location":"core-concepts/serving/#use-cases-of-servingpipeline","text":"ServingPipeline is ideal for scenarios where external systems or users need to interact directly with a Numaflow pipeline via HTTP: ML Model Serving: Deploying a machine learning model within a Numaflow pipeline (e.g., for pre/post-processing) and exposing it as a real-time inference API endpoint. Interactive Data Services: Building services for data validation, enrichment, or transformation where clients submit data via an API call and receive the processed result. API Gateway Pattern: Using ServingPipeline as a front-end to trigger event-driven backends processed by Numaflow, potentially returning a final status or result synchronously or asynchronously. Traceable Asynchronous Jobs: Kicking off complex, multi-step processing within Numaflow via an API call and allowing the client to poll for status or results later using a unique request ID.","title":"Use Cases of ServingPipeline"},{"location":"core-concepts/serving/#anatomy-of-servingpipeline","text":"A ServingPipeline resource defines both the serving layer configuration ( spec.serving ) and the underlying processing pipeline ( spec.pipeline ). apiVersion : numaflow.numaproj.io/v1alpha1 kind : ServingPipeline metadata : name : serving-pipeline-custom-store # Example name spec : # Configures the HTTP serving aspects serving : ... serving configuration options ... # Defines the underlying Numaflow pipeline for processing pipeline : ... pipeline ...","title":"Anatomy of ServingPipeline"},{"location":"core-concepts/serving/#configuration-specserving","text":"This section configures the HTTP interface: service (boolean): If true , automatically creates a Kubernetes Service. msgIDHeaderKey (string): The HTTP header key for the unique Request ID. requestTimeoutSeconds (integer): Timeout for /sync and /sse requests (default: 120 ). auth : Optional configuration for token-based authentication using a Kubernetes Secret . store : Optional configuration for a custom result storage backend. If omitted, internal storage (e.g., JetStream) is used. See Custom Results Store below.","title":"Configuration (spec.serving)"},{"location":"core-concepts/serving/#custom-results-store-specservingstore","text":"You can provide a custom storage backend by specifying a container image in spec.serving.store.container . This container must implement a specific gRPC interface (defined in the Numaflow SDKs) for storing ( Put ) and retrieving ( Get ) results associated with request IDs. This allows using preferred databases or caches. Refer to the Numaflow SDK documentation for your language for the exact interface. The Go interface requires methods like: type ServingStorer interface { Put ( ctx context . Context , put PutDatum ) Get ( ctx context . Context , get GetDatum ) StoredResult } // PutDatum, GetDatum, StoredResult provide necessary details A Golang example can be found here .","title":"Custom Results Store (spec.serving.store)"},{"location":"core-concepts/serving/#user-defined-sink-implementation-for-serving","text":"The User Defined Sink (UDSink) in a ServingPipeline 's pipeline ( spec.pipeline.vertices[].sink.udsink ) must signal the final response payload for the original HTTP request. Use the ResponseServe(requestID, resultBytes) function (or SDK equivalent) in your sink code.","title":"User-Defined Sink Implementation for Serving"},{"location":"core-concepts/serving/#example-go-sdk","text":"type serveSink struct {} func ( l * serveSink ) Sink ( ctx context . Context , datumStreamCh <- chan sinksdk . Datum ) sinksdk . Responses { result := sinksdk . ResponsesBuilder () for d := range datumStreamCh { id := d . ID () // Original Request ID val := d . Value () // Final payload from pipeline // Use ResponseServe to mark 'val' as the result for 'id' result = result . Append ( sinksdk . ResponseServe ( id , val )) } return result } A complete example can be found here . Using ResponseServe ensures the result is correctly stored and available via the API endpoints.","title":"Example (Go SDK)"},{"location":"core-concepts/serving/#api-endpoints","text":"The ServingPipeline exposes the following endpoints: POST /v1/process/sync : Submit data, wait for result in response body. POST /v1/process/async : Submit data, return immediately; fetch result later. GET /v1/process/fetch?id=<request_id> : Retrieve status/result(s) for a request ID. GET /v1/process/sse?id=<request_id> : Stream results using Server-Sent Events. GET /v1/process/message?id=<request_id> : Get message path info.","title":"API Endpoints"},{"location":"core-concepts/serving/#interaction-example","text":"Submit data synchronously: curl -k -XPOST \\ --header 'X-Numaflow-Id: job-456' \\ --header 'content-type: application/json' \\ --header 'Authorization: Bearer <your-token-if-auth-enabled>' \\ --data '{\"value\":123}' \\ --url https://<serving-pipeline-service-address>:8443/v1/process/sync Fetch results later: curl -k \\ --header 'Authorization: Bearer <your-token-if-auth-enabled>' \\ --url https://<serving-pipeline-service-address>:8443/v1/process/fetch?id = job-456 Stream results using SSE: curl -k -N \\ --header 'X-Numaflow-Id: stream-789' \\ --header 'Authorization: Bearer <your-token-if-auth-enabled>' \\ --url https://<serving-pipeline-service-address>:8443/v1/process/sse?id = stream-789 # Client receives events as results become available To query ServingPipeline objects with kubectl : kubectl get servingpipeline","title":"Interaction Example"},{"location":"core-concepts/vertex/","text":"Vertex \u00b6 The Vertex is a key component of Numaflow Pipeline where the data processing happens. Vertex is defined as a list in the pipeline spec, each representing a data processing task. There are 3 types of Vertex in Numaflow today: Source - To ingest data from sources. Sink - To forward processed data to sinks. UDF - User-defined Function, which is used to define data processing logic. We have defined a Kubernetes Custom Resource for Vertex . A Pipeline containing multiple vertices will automatically generate multiple Vertex objects by the controller. As a user, you should NOT create a Vertex object directly. In a Pipeline , the vertices are not connected directly, but through Inter-Step Buffers . To query Vertex objects with kubectl : kubectl get vertex # or \"vtx\" as a short name","title":"Vertex"},{"location":"core-concepts/vertex/#vertex","text":"The Vertex is a key component of Numaflow Pipeline where the data processing happens. Vertex is defined as a list in the pipeline spec, each representing a data processing task. There are 3 types of Vertex in Numaflow today: Source - To ingest data from sources. Sink - To forward processed data to sinks. UDF - User-defined Function, which is used to define data processing logic. We have defined a Kubernetes Custom Resource for Vertex . A Pipeline containing multiple vertices will automatically generate multiple Vertex objects by the controller. As a user, you should NOT create a Vertex object directly. In a Pipeline , the vertices are not connected directly, but through Inter-Step Buffers . To query Vertex objects with kubectl : kubectl get vertex # or \"vtx\" as a short name","title":"Vertex"},{"location":"core-concepts/watermarks/","text":"Watermarks \u00b6 When processing an unbounded data stream, Numaflow has to materialize the results of the processing done on the data. The materialization of the output depends on the notion of time, e.g., the total number of logins served per minute. Without the idea of time inbuilt into the platform, we will not be able to determine the passage of time, which is necessary for grouping elements together to materialize the result. Watermarks is that notion of time that will help us group unbounded data into discrete chunks. Numaflow supports watermarks out-of-the-box. Source vertices generate watermarks based on the event time, and propagate to downstream vertices. Watermark is defined as \u201ca monotonically increasing timestamp of the oldest work/event not yet completed\u201d . In other words, if the watermark has advanced past some timestamp T, we are guaranteed by its monotonic property that no more processing will occur for on-time events at or before T. Configuration \u00b6 Disable Watermark \u00b6 Watermarks can be disabled with by setting disabled: true . Idle Detection \u00b6 Watermark is assigned at the source; this means that the watermark will only progress if there is data coming into the source. There are many cases where the source might not be getting data, causing the source to idle (e.g., data is periodic, say once an hour). When the source is idling the reduce vertices won't emit results because the watermark is not moving. To detect source idling and propagate watermark, we can use the idle detection feature. The idle source watermark progressor will make sure that the watermark cannot progress beyond time.now() - maxDelay ( maxDelay is defined below). To enable this, we provide the following setting: Threshold \u00b6 Threshold is the duration after which a source is marked as Idle due to a lack of data flowing into the source. StepInterval \u00b6 StepInterval is the duration between the subsequent increment of the watermark as long the source remains Idle. The default value is 0s, which means that once we detect an idle source, we will increment the watermark by IncrementBy for the time we detect that our source is empty (in other words, this will be a very frequent update). Default Value: 0s IncrementBy \u00b6 IncrementBy is the duration to be added to the current watermark to progress the watermark when the source is idling. Example \u00b6 The below example will consider the source as idle after there is no data at the source for 5s. After 5s, every other 2s an idle watermark will be emitted which increments the watermark by 3s. watermark : idleSource : threshold : 5s # The pipeline will be considered idle if the source has not emitted any data for given threshold value. incrementBy : 3s # If source is found to be idle then increment the watermark by given incrementBy value. stepInterval : 2s # If source is idling then publish the watermark only when step interval has passed. maxDelay \u00b6 Watermark assignments happen at the source. Sources could be out of order, so sometimes we want to extend the window (default is 0s ) to wait before we start marking data as late-data. You can give more time for the system to wait for late data with maxDelay so that the late data within the specified time duration will be considered as data on-time. This means the watermark propagation will be delayed by maxDelay . Example \u00b6 apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline spec : watermark : disabled : false # Optional, defaults to false. maxDelay : 60s # Optional, defaults to \"0s\". Watermark API \u00b6 When processing data in user-defined functions , you can get the current watermark through an API. Watermark API is supported in all our client SDKs. Example Golang \u00b6 // Go func mapFn ( context context . Context , keys [] string , d mapper . Datum ) mapper . Messages { _ = d . EventTime () // Event time _ = d . Watermark () // Watermark ... ... }","title":"Watermarks"},{"location":"core-concepts/watermarks/#watermarks","text":"When processing an unbounded data stream, Numaflow has to materialize the results of the processing done on the data. The materialization of the output depends on the notion of time, e.g., the total number of logins served per minute. Without the idea of time inbuilt into the platform, we will not be able to determine the passage of time, which is necessary for grouping elements together to materialize the result. Watermarks is that notion of time that will help us group unbounded data into discrete chunks. Numaflow supports watermarks out-of-the-box. Source vertices generate watermarks based on the event time, and propagate to downstream vertices. Watermark is defined as \u201ca monotonically increasing timestamp of the oldest work/event not yet completed\u201d . In other words, if the watermark has advanced past some timestamp T, we are guaranteed by its monotonic property that no more processing will occur for on-time events at or before T.","title":"Watermarks"},{"location":"core-concepts/watermarks/#configuration","text":"","title":"Configuration"},{"location":"core-concepts/watermarks/#disable-watermark","text":"Watermarks can be disabled with by setting disabled: true .","title":"Disable Watermark"},{"location":"core-concepts/watermarks/#idle-detection","text":"Watermark is assigned at the source; this means that the watermark will only progress if there is data coming into the source. There are many cases where the source might not be getting data, causing the source to idle (e.g., data is periodic, say once an hour). When the source is idling the reduce vertices won't emit results because the watermark is not moving. To detect source idling and propagate watermark, we can use the idle detection feature. The idle source watermark progressor will make sure that the watermark cannot progress beyond time.now() - maxDelay ( maxDelay is defined below). To enable this, we provide the following setting:","title":"Idle Detection"},{"location":"core-concepts/watermarks/#threshold","text":"Threshold is the duration after which a source is marked as Idle due to a lack of data flowing into the source.","title":"Threshold"},{"location":"core-concepts/watermarks/#stepinterval","text":"StepInterval is the duration between the subsequent increment of the watermark as long the source remains Idle. The default value is 0s, which means that once we detect an idle source, we will increment the watermark by IncrementBy for the time we detect that our source is empty (in other words, this will be a very frequent update). Default Value: 0s","title":"StepInterval"},{"location":"core-concepts/watermarks/#incrementby","text":"IncrementBy is the duration to be added to the current watermark to progress the watermark when the source is idling.","title":"IncrementBy"},{"location":"core-concepts/watermarks/#example","text":"The below example will consider the source as idle after there is no data at the source for 5s. After 5s, every other 2s an idle watermark will be emitted which increments the watermark by 3s. watermark : idleSource : threshold : 5s # The pipeline will be considered idle if the source has not emitted any data for given threshold value. incrementBy : 3s # If source is found to be idle then increment the watermark by given incrementBy value. stepInterval : 2s # If source is idling then publish the watermark only when step interval has passed.","title":"Example"},{"location":"core-concepts/watermarks/#maxdelay","text":"Watermark assignments happen at the source. Sources could be out of order, so sometimes we want to extend the window (default is 0s ) to wait before we start marking data as late-data. You can give more time for the system to wait for late data with maxDelay so that the late data within the specified time duration will be considered as data on-time. This means the watermark propagation will be delayed by maxDelay .","title":"maxDelay"},{"location":"core-concepts/watermarks/#example_1","text":"apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline spec : watermark : disabled : false # Optional, defaults to false. maxDelay : 60s # Optional, defaults to \"0s\".","title":"Example"},{"location":"core-concepts/watermarks/#watermark-api","text":"When processing data in user-defined functions , you can get the current watermark through an API. Watermark API is supported in all our client SDKs.","title":"Watermark API"},{"location":"core-concepts/watermarks/#example-golang","text":"// Go func mapFn ( context context . Context , keys [] string , d mapper . Datum ) mapper . Messages { _ = d . EventTime () // Event time _ = d . Watermark () // Watermark ... ... }","title":"Example Golang"},{"location":"development/debugging/","text":"How To Debug \u00b6 To enable debug logs in a Vertex Pod, set environment variable NUMAFLOW_DEBUG to true for the Vertex. For example: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : in source : generator : rpu : 100 duration : 1s - name : p1 udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always containerTemplate : env : - name : NUMAFLOW_DEBUG value : !!str \"true\" - name : out sink : log : {} edges : - from : in to : p1 - from : p1 to : out To enable debug logs in the daemon pod, set environment variable NUMAFLOW_DEBUG to true for the daemon pod. For example: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : templates : daemon : containerTemplate : env : - name : NUMAFLOW_DEBUG value : !!str \"true\" Profiling \u00b6 If your pipeline is running with NUMAFLOW_DEBUG then pprof is enabled in the Vertex Pod. You can also enable just pprof by setting NUMAFLOW_PPROF to true . For example, run the commands like below to profile memory usage for a Vertex Pod, a web page displaying the memory information will be automatically opened. # Port-forward kubectl port-forward simple-pipeline-p1-0-7jzbn 2469 go tool pprof -http localhost:8081 https+insecure://localhost:2469/debug/pprof/heap Tracing is also available with commands below. # Add optional \"&seconds=n\" to specify the duration. curl -skq https://localhost:2469/debug/pprof/trace?debug = 1 -o trace.out go tool trace -http localhost:8082 trace.out Debug Inside the Container \u00b6 When doing local development using command lines such as make start , or make image , the built numaflow docker image is based on alpine , which allows you to execute into the container for debugging with kubectl exec -it {pod-name} -c {container-name} -- sh . This is not allowed when running pipelines with official released images, as they are based on scratch .","title":"How To Debug"},{"location":"development/debugging/#how-to-debug","text":"To enable debug logs in a Vertex Pod, set environment variable NUMAFLOW_DEBUG to true for the Vertex. For example: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : in source : generator : rpu : 100 duration : 1s - name : p1 udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always containerTemplate : env : - name : NUMAFLOW_DEBUG value : !!str \"true\" - name : out sink : log : {} edges : - from : in to : p1 - from : p1 to : out To enable debug logs in the daemon pod, set environment variable NUMAFLOW_DEBUG to true for the daemon pod. For example: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : templates : daemon : containerTemplate : env : - name : NUMAFLOW_DEBUG value : !!str \"true\"","title":"How To Debug"},{"location":"development/debugging/#profiling","text":"If your pipeline is running with NUMAFLOW_DEBUG then pprof is enabled in the Vertex Pod. You can also enable just pprof by setting NUMAFLOW_PPROF to true . For example, run the commands like below to profile memory usage for a Vertex Pod, a web page displaying the memory information will be automatically opened. # Port-forward kubectl port-forward simple-pipeline-p1-0-7jzbn 2469 go tool pprof -http localhost:8081 https+insecure://localhost:2469/debug/pprof/heap Tracing is also available with commands below. # Add optional \"&seconds=n\" to specify the duration. curl -skq https://localhost:2469/debug/pprof/trace?debug = 1 -o trace.out go tool trace -http localhost:8082 trace.out","title":"Profiling"},{"location":"development/debugging/#debug-inside-the-container","text":"When doing local development using command lines such as make start , or make image , the built numaflow docker image is based on alpine , which allows you to execute into the container for debugging with kubectl exec -it {pod-name} -c {container-name} -- sh . This is not allowed when running pipelines with official released images, as they are based on scratch .","title":"Debug Inside the Container"},{"location":"development/development/","text":"Development \u00b6 This doc explains how to set up a development environment for Numaflow. Install required tools \u00b6 go 1.24+. git . kubectl . protoc 3.19 for compiling protocol buffers. pandoc 3.2.1 for generating API markdown. Node.js\u00ae for running the UI. yarn . A local Kubernetes cluster for development usage, pick either one of k3d , kind , or minikube . Example: Create a local Kubernetes cluster with kind \u00b6 # Install kind on macOS brew install kind # Create a cluster with default name kind kind create cluster # Get kubeconfig for the cluster kind export kubeconfig Metrics Server \u00b6 Please install the metrics server if your local Kubernetes cluster does not bring it by default (e.g., Kind). Without the metrics-server , we will not be able to see the pods in the UI. kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl patch -n kube-system deployment metrics-server --type = json -p '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/args/-\",\"value\":\"--kubelet-insecure-tls\"}]' Useful Commands \u00b6 make start Build the source code, image, and install the Numaflow controller in the numaflow-system namespace. make build Binaries are placed in ./dist . make manifests Regenerate all the manifests after making any base manifest changes. This is also covered by make codegen . make codegen Run after making changes to ./pkg/api/ . make test Run unit tests. make test-* Run one e2e test suite. e.g. make test-kafka-e2e to run the kafka e2e suite. make Test* Run one e2e test case. e.g. make TestKafkaSourceSink to run the TestKafkaSourceSink case in the kafka e2e suite. make image Build container image, and import it to k3d , kind , or minikube cluster if corresponding KUBECONFIG is sourced. make docs Convert the docs to GitHub pages, check if there's any error. make docs-serve Start an HTTP server on your local to host the docs generated Github pages.","title":"Development"},{"location":"development/development/#development","text":"This doc explains how to set up a development environment for Numaflow.","title":"Development"},{"location":"development/development/#install-required-tools","text":"go 1.24+. git . kubectl . protoc 3.19 for compiling protocol buffers. pandoc 3.2.1 for generating API markdown. Node.js\u00ae for running the UI. yarn . A local Kubernetes cluster for development usage, pick either one of k3d , kind , or minikube .","title":"Install required tools"},{"location":"development/development/#example-create-a-local-kubernetes-cluster-with-kind","text":"# Install kind on macOS brew install kind # Create a cluster with default name kind kind create cluster # Get kubeconfig for the cluster kind export kubeconfig","title":"Example: Create a local Kubernetes cluster with kind"},{"location":"development/development/#metrics-server","text":"Please install the metrics server if your local Kubernetes cluster does not bring it by default (e.g., Kind). Without the metrics-server , we will not be able to see the pods in the UI. kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml kubectl patch -n kube-system deployment metrics-server --type = json -p '[{\"op\":\"add\",\"path\":\"/spec/template/spec/containers/0/args/-\",\"value\":\"--kubelet-insecure-tls\"}]'","title":"Metrics Server"},{"location":"development/development/#useful-commands","text":"make start Build the source code, image, and install the Numaflow controller in the numaflow-system namespace. make build Binaries are placed in ./dist . make manifests Regenerate all the manifests after making any base manifest changes. This is also covered by make codegen . make codegen Run after making changes to ./pkg/api/ . make test Run unit tests. make test-* Run one e2e test suite. e.g. make test-kafka-e2e to run the kafka e2e suite. make Test* Run one e2e test case. e.g. make TestKafkaSourceSink to run the TestKafkaSourceSink case in the kafka e2e suite. make image Build container image, and import it to k3d , kind , or minikube cluster if corresponding KUBECONFIG is sourced. make docs Convert the docs to GitHub pages, check if there's any error. make docs-serve Start an HTTP server on your local to host the docs generated Github pages.","title":"Useful Commands"},{"location":"development/releasing/","text":"How To Release \u00b6 Release Branch \u00b6 Always create a release branch for the releases, for example branch release-0.5 is for all the v0.5.x versions release. If it's a new release branch, simply create a branch from main . Release Steps \u00b6 Cherry-pick fixes to the release branch, skip this step if it's the first release in the branch. Run make test to make sure all test cases pass locally. Push to remote branch, and make sure all the CI jobs pass. Run make prepare-release VERSION=v{x.y.z} to update version in manifests, where x.y.x is the expected new version. Follow the output of last step, to confirm if all the changes are expected, and then run make release VERSION=v{x.y.z} . Follow the output, push a new tag to the release branch, GitHub actions will automatically build and publish the new release, this will take around 10 minutes. Test the new release, make sure everything is running as expected. Find the new release tag, and edit the release notes. [Non RC release only] Recreate a stable tag against the latest release. git tag -d stable git tag -a stable -m stable git push -d { your-remote } stable git push { your-remote } stable [Non RC release only] Release the new version in helm-charts .","title":"How To Release"},{"location":"development/releasing/#how-to-release","text":"","title":"How To Release"},{"location":"development/releasing/#release-branch","text":"Always create a release branch for the releases, for example branch release-0.5 is for all the v0.5.x versions release. If it's a new release branch, simply create a branch from main .","title":"Release Branch"},{"location":"development/releasing/#release-steps","text":"Cherry-pick fixes to the release branch, skip this step if it's the first release in the branch. Run make test to make sure all test cases pass locally. Push to remote branch, and make sure all the CI jobs pass. Run make prepare-release VERSION=v{x.y.z} to update version in manifests, where x.y.x is the expected new version. Follow the output of last step, to confirm if all the changes are expected, and then run make release VERSION=v{x.y.z} . Follow the output, push a new tag to the release branch, GitHub actions will automatically build and publish the new release, this will take around 10 minutes. Test the new release, make sure everything is running as expected. Find the new release tag, and edit the release notes. [Non RC release only] Recreate a stable tag against the latest release. git tag -d stable git tag -a stable -m stable git push -d { your-remote } stable git push { your-remote } stable [Non RC release only] Release the new version in helm-charts .","title":"Release Steps"},{"location":"development/static-code-analysis/","text":"Static Code Analysis \u00b6 We use the following static code analysis tools: golangci-lint for compile time linting. Snyk for image scanning. These are at least run daily or on each pull request.","title":"Static Code Analysis"},{"location":"development/static-code-analysis/#static-code-analysis","text":"We use the following static code analysis tools: golangci-lint for compile time linting. Snyk for image scanning. These are at least run daily or on each pull request.","title":"Static Code Analysis"},{"location":"operations/controller-configmap/","text":"Controller ConfigMap \u00b6 The controller ConfigMap is used for controller-wide settings. For a detailed example, please see numaflow-controller-config.yaml . Configuration Structure \u00b6 The configuration should be under controller-config.yaml key in the ConfigMap, as a string in yaml format: apiVersion : v1 kind : ConfigMap metadata : name : numaflow-controller-config data : controller-config.yaml : | defaults: containerResources: | ... isbsvc: jetstream: ... Default Controller Configuration \u00b6 Currently, we support configuring the init and main container resources for steps across all the pipelines. The configuration is under defaults key in the ConfigMap. For example, to set the default container resources for steps across all the pipelines: apiVersion : v1 kind : ConfigMap metadata : name : numaflow-controller-config data : controller-config.yaml : | defaults: containerResources: | limits: memory: \"256Mi\" cpu: \"200m\" requests: memory: \"128Mi\" cpu: \"100m\" ISB Service Configuration \u00b6 One of the important configuration items in the ConfigMap is about ISB Service . We currently use 3rd party technologies such as JetStream to implement ISB Services, if those applications have new releases, to make them available in Numaflow, the new versions need to be added in the ConfigMap. For example, there's a new Nats JetStream version x.y.x available, a new version configuration like below needs to be added before it can be referenced in the InterStepBufferService spec. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-controller-config data : controller-config.yaml : | isbsvc: jetstream: versions: - version: x.y.x # Name it whatever you want, it will be referenced in the InterStepBufferService spec. natsImage: nats:x.y.x metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1 configReloaderImage: natsio/nats-server-config-reloader:0.7.0 startCommand: /nats-server","title":"Controller Configuration"},{"location":"operations/controller-configmap/#controller-configmap","text":"The controller ConfigMap is used for controller-wide settings. For a detailed example, please see numaflow-controller-config.yaml .","title":"Controller ConfigMap"},{"location":"operations/controller-configmap/#configuration-structure","text":"The configuration should be under controller-config.yaml key in the ConfigMap, as a string in yaml format: apiVersion : v1 kind : ConfigMap metadata : name : numaflow-controller-config data : controller-config.yaml : | defaults: containerResources: | ... isbsvc: jetstream: ...","title":"Configuration Structure"},{"location":"operations/controller-configmap/#default-controller-configuration","text":"Currently, we support configuring the init and main container resources for steps across all the pipelines. The configuration is under defaults key in the ConfigMap. For example, to set the default container resources for steps across all the pipelines: apiVersion : v1 kind : ConfigMap metadata : name : numaflow-controller-config data : controller-config.yaml : | defaults: containerResources: | limits: memory: \"256Mi\" cpu: \"200m\" requests: memory: \"128Mi\" cpu: \"100m\"","title":"Default Controller Configuration"},{"location":"operations/controller-configmap/#isb-service-configuration","text":"One of the important configuration items in the ConfigMap is about ISB Service . We currently use 3rd party technologies such as JetStream to implement ISB Services, if those applications have new releases, to make them available in Numaflow, the new versions need to be added in the ConfigMap. For example, there's a new Nats JetStream version x.y.x available, a new version configuration like below needs to be added before it can be referenced in the InterStepBufferService spec. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-controller-config data : controller-config.yaml : | isbsvc: jetstream: versions: - version: x.y.x # Name it whatever you want, it will be referenced in the InterStepBufferService spec. natsImage: nats:x.y.x metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1 configReloaderImage: natsio/nats-server-config-reloader:0.7.0 startCommand: /nats-server","title":"ISB Service Configuration"},{"location":"operations/grafana/","text":"Grafana \u00b6 Numaflow provides prometheus metrics on top of which you can build Grafana dashboard to monitor your pipeline. Setup Grafana \u00b6 (Pre-requisite) Follow Metrics to set up prometheus operator. Follow Prometheus Tutorial to install Grafana and visualize metrics. Sample Dashboard \u00b6 You can customize your own dashboard by selecting metrics that best describe the health of your pipeline. Below is a sample dashboard which includes some basic metrics. To use the sample dashboard, download the corresponding sample dashboard template , import(before importing change the uid of the datasource in json, issue link ) it to Grafana and use the dropdown menu at top-left of the dashboard to choose which pipeline/vertex/buffer metrics to display.","title":"Grafana"},{"location":"operations/grafana/#grafana","text":"Numaflow provides prometheus metrics on top of which you can build Grafana dashboard to monitor your pipeline.","title":"Grafana"},{"location":"operations/grafana/#setup-grafana","text":"(Pre-requisite) Follow Metrics to set up prometheus operator. Follow Prometheus Tutorial to install Grafana and visualize metrics.","title":"Setup Grafana"},{"location":"operations/grafana/#sample-dashboard","text":"You can customize your own dashboard by selecting metrics that best describe the health of your pipeline. Below is a sample dashboard which includes some basic metrics. To use the sample dashboard, download the corresponding sample dashboard template , import(before importing change the uid of the datasource in json, issue link ) it to Grafana and use the dropdown menu at top-left of the dashboard to choose which pipeline/vertex/buffer metrics to display.","title":"Sample Dashboard"},{"location":"operations/installation/","text":"Installation \u00b6 Numaflow can be installed in different scopes with different approaches. Cluster Scope \u00b6 A cluster scope installation watches and executes pipelines in all the namespaces in the cluster. Run following command line to install latest stable Numaflow in cluster scope. kubectl apply -n numaflow-system -f https://raw.githubusercontent.com/numaproj/numaflow/stable/config/install.yaml If you use kustomize , use kustomization.yaml below. apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://github.com/numaproj/numaflow/config/cluster-install?ref=stable # Or specify a version namespace : numaflow-system Namespace Scope \u00b6 A namespace scoped installation only watches and executes pipelines in the namespace it is installed (typically numaflow-system ). Configure the ConfigMap numaflow-cmd-params-config to achieve namespace scoped installation. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : # Whether to run in namespaced scope, defaults to false. namespaced : \"true\" Another approach to do namespace scoped installation is to add an argument --namespaced to the numaflow-controller and numaflow-server deployments. This approach takes precedence over the ConfigMap approach. - args: - --namespaced If there are multiple namespace scoped installations in one cluster, potentially there will be backward compatibility issue when any of the installation gets upgraded to a new version that has new CRD definition. To avoid this issue, we suggest to use minimal CRD definition for namespaced installation, which does not have detailed property definitions, thus no CRD changes between different versions. # Minimal CRD kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/config/advanced-install/minimal-crds.yaml # Controller in namespaced scope kubectl apply -n numaflow-system -f https://raw.githubusercontent.com/numaproj/numaflow/stable/config/advanced-install/namespaced-controller-wo-crds.yaml If you use kustomize , kustomization.yaml looks like below. apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://github.com/numaproj/numaflow/config/advanced-install/minimal-crds?ref=stable # Or specify a version - https://github.com/numaproj/numaflow/config/advanced-install/namespaced-controller?ref=stable # Or specify a version namespace : numaflow-system Managed Namespace Scope \u00b6 A managed namespace installation watches and executes pipelines in a specific namespace. To do managed namespace installation, configure the ConfigMap numaflow-cmd-params-config as following. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : # Whether to run the controller and the UX server in namespaced scope, defaults to false. namespaced : \"true\" # The namespace that the controller and UX server watch when \"namespaced\" is true, defaults to the installation namespace. managed.namespace : numaflow-system Similarly, another approach is to add --managed-namespace and the specific namespace to the numaflow-controller and numaflow-server deployment arguments. This approach takes precedence over the ConfigMap approach. - args: - --namespaced - --managed-namespace - my-namespace High Availability \u00b6 By default, the Numaflow controller is installed with Active-Passive HA strategy enabled, which means you can run the controller with multiple replicas (defaults to 1 in the manifests). There are some parameters can be tuned for the leader election mechanism of HA. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : ### The duration that non-leader candidates will wait to force acquire leadership. # This is measured against time of last observed ack. Default is 15 seconds. # The configuration has to be: lease.duration > lease.renew.deadline > lease.renew.period controller.leader.election.lease.duration : 15s # ### The duration that the acting controlplane will retry refreshing leadership before giving up. # Default is 10 seconds. # The configuration has to be: lease.duration > lease.renew.deadline > lease.renew.period controller.leader.election.lease.renew.deadline : 10s ### The duration the LeaderElector clients should wait between tries of actions, which means every # this period of time, it tries to renew the lease. Default is 2 seconds. # The configuration has to be: lease.duration > lease.renew.deadline > lease.renew.period controller.leader.election.lease.renew.period : 2s These parameters are useful when you want to tune the frequency of leader election renewal calls to K8s API server, which are usually configured at a high priority level of API Priority and Fairness . To turn off HA, configure the ConfigMap numaflow-cmd-params-config as following. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : # Whether to disable leader election for the controller, defaults to false controller.leader.election.disabled : \"true\" If HA is turned off, the controller deployment should not run with multiple replicas. Multiple Controllers \u00b6 With in one cluster, or even in one namespace, you can run multiple Numaflow controllers by leveraging the instance configuration in the numaflow-controller-config ConfigMap. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-controller-config data : controller-config.yaml : | # Within a cluster, setting \"instance\" can be used to run N Numaflow controllers. # If configured, the controller will only watch the objects having an annotation with the key \"numaflow.numaproj.io/instance\" and the corresponding value. # If not configured (or empty string), the controller will watch all objects. instance: \"\" defaults: containerResources: | requests: memory: \"128Mi\" cpu: \"100m\" isbsvc: ... When instance is configured (e.g. my-instance ), the controller will only watch the objects ( InterStepBufferService , Pipeline and MonoVertex ) having the annotation numaflow.numaproj.io/instance: my-instance . Correspondingly, if a Pipeline object has an annotation numaflow.numaproj.io/instance: my-instance , it requires the referenced InterStepBufferService also has the same annotation, or it will fail to orchestrate the pipeline.","title":"Installation"},{"location":"operations/installation/#installation","text":"Numaflow can be installed in different scopes with different approaches.","title":"Installation"},{"location":"operations/installation/#cluster-scope","text":"A cluster scope installation watches and executes pipelines in all the namespaces in the cluster. Run following command line to install latest stable Numaflow in cluster scope. kubectl apply -n numaflow-system -f https://raw.githubusercontent.com/numaproj/numaflow/stable/config/install.yaml If you use kustomize , use kustomization.yaml below. apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://github.com/numaproj/numaflow/config/cluster-install?ref=stable # Or specify a version namespace : numaflow-system","title":"Cluster Scope"},{"location":"operations/installation/#namespace-scope","text":"A namespace scoped installation only watches and executes pipelines in the namespace it is installed (typically numaflow-system ). Configure the ConfigMap numaflow-cmd-params-config to achieve namespace scoped installation. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : # Whether to run in namespaced scope, defaults to false. namespaced : \"true\" Another approach to do namespace scoped installation is to add an argument --namespaced to the numaflow-controller and numaflow-server deployments. This approach takes precedence over the ConfigMap approach. - args: - --namespaced If there are multiple namespace scoped installations in one cluster, potentially there will be backward compatibility issue when any of the installation gets upgraded to a new version that has new CRD definition. To avoid this issue, we suggest to use minimal CRD definition for namespaced installation, which does not have detailed property definitions, thus no CRD changes between different versions. # Minimal CRD kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/config/advanced-install/minimal-crds.yaml # Controller in namespaced scope kubectl apply -n numaflow-system -f https://raw.githubusercontent.com/numaproj/numaflow/stable/config/advanced-install/namespaced-controller-wo-crds.yaml If you use kustomize , kustomization.yaml looks like below. apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - https://github.com/numaproj/numaflow/config/advanced-install/minimal-crds?ref=stable # Or specify a version - https://github.com/numaproj/numaflow/config/advanced-install/namespaced-controller?ref=stable # Or specify a version namespace : numaflow-system","title":"Namespace Scope"},{"location":"operations/installation/#managed-namespace-scope","text":"A managed namespace installation watches and executes pipelines in a specific namespace. To do managed namespace installation, configure the ConfigMap numaflow-cmd-params-config as following. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : # Whether to run the controller and the UX server in namespaced scope, defaults to false. namespaced : \"true\" # The namespace that the controller and UX server watch when \"namespaced\" is true, defaults to the installation namespace. managed.namespace : numaflow-system Similarly, another approach is to add --managed-namespace and the specific namespace to the numaflow-controller and numaflow-server deployment arguments. This approach takes precedence over the ConfigMap approach. - args: - --namespaced - --managed-namespace - my-namespace","title":"Managed Namespace Scope"},{"location":"operations/installation/#high-availability","text":"By default, the Numaflow controller is installed with Active-Passive HA strategy enabled, which means you can run the controller with multiple replicas (defaults to 1 in the manifests). There are some parameters can be tuned for the leader election mechanism of HA. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : ### The duration that non-leader candidates will wait to force acquire leadership. # This is measured against time of last observed ack. Default is 15 seconds. # The configuration has to be: lease.duration > lease.renew.deadline > lease.renew.period controller.leader.election.lease.duration : 15s # ### The duration that the acting controlplane will retry refreshing leadership before giving up. # Default is 10 seconds. # The configuration has to be: lease.duration > lease.renew.deadline > lease.renew.period controller.leader.election.lease.renew.deadline : 10s ### The duration the LeaderElector clients should wait between tries of actions, which means every # this period of time, it tries to renew the lease. Default is 2 seconds. # The configuration has to be: lease.duration > lease.renew.deadline > lease.renew.period controller.leader.election.lease.renew.period : 2s These parameters are useful when you want to tune the frequency of leader election renewal calls to K8s API server, which are usually configured at a high priority level of API Priority and Fairness . To turn off HA, configure the ConfigMap numaflow-cmd-params-config as following. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : # Whether to disable leader election for the controller, defaults to false controller.leader.election.disabled : \"true\" If HA is turned off, the controller deployment should not run with multiple replicas.","title":"High Availability"},{"location":"operations/installation/#multiple-controllers","text":"With in one cluster, or even in one namespace, you can run multiple Numaflow controllers by leveraging the instance configuration in the numaflow-controller-config ConfigMap. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-controller-config data : controller-config.yaml : | # Within a cluster, setting \"instance\" can be used to run N Numaflow controllers. # If configured, the controller will only watch the objects having an annotation with the key \"numaflow.numaproj.io/instance\" and the corresponding value. # If not configured (or empty string), the controller will watch all objects. instance: \"\" defaults: containerResources: | requests: memory: \"128Mi\" cpu: \"100m\" isbsvc: ... When instance is configured (e.g. my-instance ), the controller will only watch the objects ( InterStepBufferService , Pipeline and MonoVertex ) having the annotation numaflow.numaproj.io/instance: my-instance . Correspondingly, if a Pipeline object has an annotation numaflow.numaproj.io/instance: my-instance , it requires the referenced InterStepBufferService also has the same annotation, or it will fail to orchestrate the pipeline.","title":"Multiple Controllers"},{"location":"operations/releases/","text":"Releases \u00b6 You can find the most recent version under Github Releases . Versioning \u00b6 Versions are expressed as vx.y.z (for example, v0.5.3 ), where x is the major version, y is the minor version, and z is the patch version, following Semantic Versioning terminology. Numaflow does not use Semantic Versioning. Minor versions may contain breaking changes. Patch versions only contain bug fixes and minor features. There's a stable tag, pointing to a latest stable release, usually it is the latest patch version. Release Cycle \u00b6 TBD as Numaflow is under active development. Nightly Build \u00b6 If you want to try out the new features on main branch, Numaflow provides nightly build images from main , the images are available in the format of quay.io/numaproj/numaflow:nightly-yyyyMMdd . Nightly build images expire in 30 days.","title":"Releases \u29c9"},{"location":"operations/releases/#releases","text":"You can find the most recent version under Github Releases .","title":"Releases"},{"location":"operations/releases/#versioning","text":"Versions are expressed as vx.y.z (for example, v0.5.3 ), where x is the major version, y is the minor version, and z is the patch version, following Semantic Versioning terminology. Numaflow does not use Semantic Versioning. Minor versions may contain breaking changes. Patch versions only contain bug fixes and minor features. There's a stable tag, pointing to a latest stable release, usually it is the latest patch version.","title":"Versioning"},{"location":"operations/releases/#release-cycle","text":"TBD as Numaflow is under active development.","title":"Release Cycle"},{"location":"operations/releases/#nightly-build","text":"If you want to try out the new features on main branch, Numaflow provides nightly build images from main , the images are available in the format of quay.io/numaproj/numaflow:nightly-yyyyMMdd . Nightly build images expire in 30 days.","title":"Nightly Build"},{"location":"operations/security/","text":"Security \u00b6 Controller \u00b6 Numaflow controller can be deployed in two scopes. It can be either at the Cluster level or at the Namespace level. When the Numaflow controller is deployed at the Namespace level, it will only have access to the Namespace resources. Pipeline \u00b6 Data Movement \u00b6 Data movement happens only within the namespace (no cross-namespaces). Numaflow provides the ability to encrypt data at rest and also in transit. Controller and Data Plane \u00b6 All communications between the controller and Numaflow pipeline components are encrypted. These are uni-directional read-only communications.","title":"Security"},{"location":"operations/security/#security","text":"","title":"Security"},{"location":"operations/security/#controller","text":"Numaflow controller can be deployed in two scopes. It can be either at the Cluster level or at the Namespace level. When the Numaflow controller is deployed at the Namespace level, it will only have access to the Namespace resources.","title":"Controller"},{"location":"operations/security/#pipeline","text":"","title":"Pipeline"},{"location":"operations/security/#data-movement","text":"Data movement happens only within the namespace (no cross-namespaces). Numaflow provides the ability to encrypt data at rest and also in transit.","title":"Data Movement"},{"location":"operations/security/#controller-and-data-plane","text":"All communications between the controller and Numaflow pipeline components are encrypted. These are uni-directional read-only communications.","title":"Controller and Data Plane"},{"location":"operations/validating-webhook/","text":"Validating Admission Webhook \u00b6 This validating webhook will prevent disallowed spec changes to immutable fields of Numaflow CRDs including Pipelines and InterStepBufferServices. It also prevents creating a CRD with a faulty spec. The user sees an error immediately returned by the server explaining why the request was denied. Installation \u00b6 To install the validating webhook, run the following command line: kubectl apply -n numaflow-system -f https://raw.githubusercontent.com/numaproj/numaflow/stable/config/validating-webhook-install.yaml Examples \u00b6 Currently, the validating webhook prevents updating the pvc storage size, for example. Example spec: apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : version : latest persistence : volumeSize : 3Gi // Update it will cause the error below Error from server ( BadRequest ) : error when applying patch: { \"metadata\" : { \"annotations\" : { \"kubectl.kubernetes.io/last-applied-configuration\" : \"{\\\"apiVersion\\\":\\\"numaflow.numaproj.io/v1alpha1\\\",\\\"kind\\\":\\\"InterStepBufferService\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"default\\\",\\\"namespace\\\":\\\"numaflow-system\\\"},\\\"spec\\\":{\\\"jetstream\\\":{\\\"persistence\\\":{\\\"volumeSize\\\":\\\"5Gi\\\"},\\\"version\\\":\\\"latest\\\"}}}\\n\" }} , \"spec\" : { \"jetstream\" : { \"persistence\" : { \"volumeSize\" : \"5Gi\" }}}} to: Resource: \"numaflow.numaproj.io/v1alpha1, Resource=interstepbufferservices\" , GroupVersionKind: \"numaflow.numaproj.io/v1alpha1, Kind=InterStepBufferService\" Name: \"default\" , Namespace: \"numaflow-system\" for : \"examples/0-isbsvc-jetstream.yaml\" : error when patching \"examples/0-isbsvc-jetstream.yaml\" : admission webhook \"webhook.numaflow.numaproj.io\" denied the request: can not change persistence of Jetstream ISB Service There is also validation that prevents the interStepBufferServiceName of a Pipeline from being updated. Error from server ( BadRequest ) : error when applying patch: { \"metadata\" : { \"annotations\" : { \"kubectl.kubernetes.io/last-applied-configuration\" : \"{\\\"apiVersion\\\":\\\"numaflow.numaproj.io/v1alpha1\\\",\\\"kind\\\":\\\"Pipeline\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"simple-pipeline\\\",\\\"namespace\\\":\\\"numaflow-system\\\"},\\\"spec\\\":{\\\"edges\\\":[{\\\"from\\\":\\\"in\\\",\\\"to\\\":\\\"cat\\\"},{\\\"from\\\":\\\"cat\\\",\\\"to\\\":\\\"out\\\"}],\\\"interStepBufferServiceName\\\":\\\"change\\\",\\\"vertices\\\":[{\\\"name\\\":\\\"in\\\",\\\"source\\\":{\\\"generator\\\":{\\\"duration\\\":\\\"1s\\\",\\\"rpu\\\":5}}},{\\\"name\\\":\\\"cat\\\",\\\"udf\\\":{\\\"builtin\\\":{\\\"name\\\":\\\"cat\\\"}}},{\\\"name\\\":\\\"out\\\",\\\"sink\\\":{\\\"log\\\":{}}}]}}\\n\" }} , \"spec\" : { \"interStepBufferServiceName\" : \"change\" , \"vertices\" : [{ \"name\" : \"in\" , \"source\" : { \"generator\" : { \"duration\" : \"1s\" , \"rpu\" :5 }}} , { \"name\" : \"cat\" , \"udf\" : { \"builtin\" : { \"name\" : \"cat\" }}} , { \"name\" : \"out\" , \"sink\" : { \"log\" : {}}}]}} to: Resource: \"numaflow.numaproj.io/v1alpha1, Resource=pipelines\" , GroupVersionKind: \"numaflow.numaproj.io/v1alpha1, Kind=Pipeline\" Name: \"simple-pipeline\" , Namespace: \"numaflow-system\" for : \"examples/1-simple-pipeline.yaml\" : error when patching \"examples/1-simple-pipeline.yaml\" : admission webhook \"webhook.numaflow.numaproj.io\" denied the request: Cannot update pipeline with different interStepBufferServiceName Other validations include: Pipeline: cannot change the type of an existing vertex cannot change the partition count of a reduce vertex cannot change the storage class of a reduce vertex etc. InterStepBufferService: cannot change the persistence configuration of an ISB Service etc.","title":"Validating Webhook"},{"location":"operations/validating-webhook/#validating-admission-webhook","text":"This validating webhook will prevent disallowed spec changes to immutable fields of Numaflow CRDs including Pipelines and InterStepBufferServices. It also prevents creating a CRD with a faulty spec. The user sees an error immediately returned by the server explaining why the request was denied.","title":"Validating Admission Webhook"},{"location":"operations/validating-webhook/#installation","text":"To install the validating webhook, run the following command line: kubectl apply -n numaflow-system -f https://raw.githubusercontent.com/numaproj/numaflow/stable/config/validating-webhook-install.yaml","title":"Installation"},{"location":"operations/validating-webhook/#examples","text":"Currently, the validating webhook prevents updating the pvc storage size, for example. Example spec: apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : version : latest persistence : volumeSize : 3Gi // Update it will cause the error below Error from server ( BadRequest ) : error when applying patch: { \"metadata\" : { \"annotations\" : { \"kubectl.kubernetes.io/last-applied-configuration\" : \"{\\\"apiVersion\\\":\\\"numaflow.numaproj.io/v1alpha1\\\",\\\"kind\\\":\\\"InterStepBufferService\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"default\\\",\\\"namespace\\\":\\\"numaflow-system\\\"},\\\"spec\\\":{\\\"jetstream\\\":{\\\"persistence\\\":{\\\"volumeSize\\\":\\\"5Gi\\\"},\\\"version\\\":\\\"latest\\\"}}}\\n\" }} , \"spec\" : { \"jetstream\" : { \"persistence\" : { \"volumeSize\" : \"5Gi\" }}}} to: Resource: \"numaflow.numaproj.io/v1alpha1, Resource=interstepbufferservices\" , GroupVersionKind: \"numaflow.numaproj.io/v1alpha1, Kind=InterStepBufferService\" Name: \"default\" , Namespace: \"numaflow-system\" for : \"examples/0-isbsvc-jetstream.yaml\" : error when patching \"examples/0-isbsvc-jetstream.yaml\" : admission webhook \"webhook.numaflow.numaproj.io\" denied the request: can not change persistence of Jetstream ISB Service There is also validation that prevents the interStepBufferServiceName of a Pipeline from being updated. Error from server ( BadRequest ) : error when applying patch: { \"metadata\" : { \"annotations\" : { \"kubectl.kubernetes.io/last-applied-configuration\" : \"{\\\"apiVersion\\\":\\\"numaflow.numaproj.io/v1alpha1\\\",\\\"kind\\\":\\\"Pipeline\\\",\\\"metadata\\\":{\\\"annotations\\\":{},\\\"name\\\":\\\"simple-pipeline\\\",\\\"namespace\\\":\\\"numaflow-system\\\"},\\\"spec\\\":{\\\"edges\\\":[{\\\"from\\\":\\\"in\\\",\\\"to\\\":\\\"cat\\\"},{\\\"from\\\":\\\"cat\\\",\\\"to\\\":\\\"out\\\"}],\\\"interStepBufferServiceName\\\":\\\"change\\\",\\\"vertices\\\":[{\\\"name\\\":\\\"in\\\",\\\"source\\\":{\\\"generator\\\":{\\\"duration\\\":\\\"1s\\\",\\\"rpu\\\":5}}},{\\\"name\\\":\\\"cat\\\",\\\"udf\\\":{\\\"builtin\\\":{\\\"name\\\":\\\"cat\\\"}}},{\\\"name\\\":\\\"out\\\",\\\"sink\\\":{\\\"log\\\":{}}}]}}\\n\" }} , \"spec\" : { \"interStepBufferServiceName\" : \"change\" , \"vertices\" : [{ \"name\" : \"in\" , \"source\" : { \"generator\" : { \"duration\" : \"1s\" , \"rpu\" :5 }}} , { \"name\" : \"cat\" , \"udf\" : { \"builtin\" : { \"name\" : \"cat\" }}} , { \"name\" : \"out\" , \"sink\" : { \"log\" : {}}}]}} to: Resource: \"numaflow.numaproj.io/v1alpha1, Resource=pipelines\" , GroupVersionKind: \"numaflow.numaproj.io/v1alpha1, Kind=Pipeline\" Name: \"simple-pipeline\" , Namespace: \"numaflow-system\" for : \"examples/1-simple-pipeline.yaml\" : error when patching \"examples/1-simple-pipeline.yaml\" : admission webhook \"webhook.numaflow.numaproj.io\" denied the request: Cannot update pipeline with different interStepBufferServiceName Other validations include: Pipeline: cannot change the type of an existing vertex cannot change the partition count of a reduce vertex cannot change the storage class of a reduce vertex etc. InterStepBufferService: cannot change the persistence configuration of an ISB Service etc.","title":"Examples"},{"location":"operations/metrics/metrics/","text":"Metrics \u00b6 Numaflow provides the following prometheus metrics which we can use to monitor our pipeline and setup any alerts if needed. Golden Signals \u00b6 These metrics in combination can be used to determine the overall health of your pipeline Traffic \u00b6 These metrics can be used to determine throughput of your pipeline. Metric name Metric type Labels Description forwarder_read_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages read by a given Vertex from an Inter-Step Buffer Partition or Source forwarder_data_read_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of data messages read by a given Vertex from an Inter-Step Buffer Partition or Source forwarder_read_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of bytes read by a given Vertex from an Inter-Step Buffer Partition or Source forwarder_data_read_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of data bytes read by a given Vertex from an Inter-Step Buffer Partition or Source source_forwarder_transformer_read_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=Source replica=<replica-index> partition_name=<partition-name> Provides the total number of messages read by source transformer forwarder_write_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages written to Inter-Step Buffer by a given Vertex forwarder_write_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of bytes written to Inter-Step Buffer by a given Vertex source_forwarder_transformer_write_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=Source replica=<replica-index> partition_name=<partition-name> Provides the total number of messages written by source transformer forwarder_fbsink_write_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages written to a fallback sink forwarder_fbsink_write_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of bytes written to a fallback sink forwarder_ack_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages acknowledged by a given Vertex from an Inter-Step Buffer Partition forwarder_drop_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages dropped by a given Vertex due to a full Inter-Step Buffer Partition forwarder_drop_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of bytes dropped by a given Vertex due to a full Inter-Step Buffer Partition forwarder_udf_read_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages read by UDF forwarder_udf_write_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages written by UDF Latency \u00b6 These metrics can be used to determine the latency of your pipeline. Metric name Metric type Labels Description pipeline_processing_lag Gauge pipeline=<pipeline-name> Pipeline processing lag in milliseconds (max watermark - min watermark) pipeline_watermark_cmp_now Gauge pipeline=<pipeline-name> Max watermark of source compared with current time in milliseconds forwarder_read_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the histogram distribution of the processing times of read operations forwarder_write_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the histogram distribution of the processing times of write operations forwarder_ack_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the histogram distribution of the processing times of ack operations forwarder_fbsink_write_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the histogram distribution of the processing times of write operations to a fallback sink source_forwarder_transformer_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=Source replica=<replica-index> partition_name=<partition-name> Provides a histogram distribution of the processing times of source transformer forwarder_udf_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> Provides a histogram distribution of the processing times of User-defined Functions (UDFs) in a map vertex forwarder_forward_chunk_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> Provides a histogram distribution of the processing times of the forwarder function as a whole in a map vertex reduce_data_forward_forward_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> replica=<replica-index> Provides a histogram distribution of the forwarding times of the data from an ISB to a PBQ reduce_pbq_write_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> replica=<replica-index> Provides a histogram distribution of the processing times of write operations to a PBQ reduce_pnf_process_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> replica=<replica-index> Provides a histogram distribution of the processing times of the reducer vertex_pending_messages Gauge pipeline=<pipeline-name> vertex=<vertex-name> period=<duration> partition_name=<partition-name> Provides the average pending messages in the last period of seconds. It is the pending messages of a vertex Errors \u00b6 These metrics can be used to determine if there are any errors in the pipeline. Metric name Metric type Labels Description pipeline_data_processing_health Gauge pipeline=<pipeline-name> Pipeline data processing health status. 1: Healthy, 0: Unknown, -1: Warning, -2: Critical controller_isbsvc_health Gauge ns=<namespace> isbsvc=<isbsvc-name> A metric to indicate whether the ISB Service is healthy. '1' means healthy, '0' means unhealthy controller_pipeline_health Gauge ns=<namespace> pipeline=<pipeline-name> A metric to indicate whether the Pipeline is healthy. '1' means healthy, '0' means unhealthy controller_monovtx_health Gauge ns=<namespace> mvtx_name=<mvtx-name> A metric to indicate whether the MonoVertex is healthy. '1' means healthy, '0' means unhealthy forwarder_platform_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> Indicates any internal errors which could stop pipeline processing forwarder_read_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Indicates any errors while reading messages by the forwarder source_forwarder_transformer_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=Source replica=<replica-index> partition_name=<partition-name> Indicates source transformer errors forwarder_write_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Indicates any errors while writing messages by the forwarder forwarder_fbsink_write_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Indicates any errors while writing to a fallback sink forwarder_ack_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Indicates any errors while acknowledging messages by the forwarder kafka_sink_write_timeout_total Counter pipeline=<pipeline-name> vertex=<vertex-name> Provides the write timeouts while writing to the Kafka sink isb_jetstream_read_error_total Counter buffer=<buffer-name> Indicates any read errors with NATS Jetstream ISB isb_jetstream_write_error_total Counter buffer=<buffer-name> Indicates any write errors with NATS Jetstream ISB Saturation \u00b6 NATS JetStream ISB \u00b6 Metric name Metric type Labels Description isb_jetstream_isFull_total Counter buffer=<buffer-name> Indicates if the ISB is full. Continual increase of this counter metric indicates a potential backpressure that can be built on the pipeline isb_jetstream_buffer_soft_usage Gauge buffer=<buffer-name> Indicates the usage/utilization of a NATS Jetstream ISB isb_jetstream_buffer_solid_usage Gauge buffer=<buffer-name> Indicates the solid usage of a NATS Jetstream ISB isb_jetstream_buffer_pending Gauge buffer=<buffer-name> Indicate the number of pending messages at a given point in time. isb_jetstream_buffer_ack_pending Gauge buffer=<buffer-name> Indicates the number of messages pending acknowledge at a given point in time Others \u00b6 Metric name Metric type Labels Description build_info Gauge component=<component> component_name=<component_name> version=<version> platform=<platform> A metric with a constant value '1', labeled by Numaflow binary version, platform, and other information. The value of component could be 'daemon', 'vertex', 'mono-vertex-daemon', etc controller_build_info Gauge version=<version> platform=<platform> A metric with a constant value '1', labeled with controller version and platform from which Numaflow was built sdk_info Gauge component=<component> component_name=<component_name> type=<sdk_type> version=<sdk_version> language=<sdk_language> A metric with a constant value '1', labeled by SDK information such as version, language, and type controller_pipeline_desired_phase Gauge ns=<namespace> pipeline=<pipeline> A metric to indicate the pipeline phase. '1' means Running, '2' means Paused controller_pipeline_current_phase Gauge ns=<namespace> pipeline=<pipeline> A metric to indicate the pipeline phase. '0' means Unknown, '1' means Running, '2' means Paused, '3' means Failed, '4' means Pausing, '5' means 'Deleting' controller_monovtx_desired_phase Gauge ns=<namespace> mvtx_name=<mvtx> A metric to indicate the MonoVertex phase. '1' means Running, '2' means Paused controller_monovtx_current_phase Gauge ns=<namespace> mvtx_name=<mvtx> A metric to indicate the MonoVertex phase. '0' means Unknown, '1' means Running, '2' means Paused, '3' means Failed controller_vertex_desired_replicas Gauge ns=<namespace> pipeline=<pipeline> vertex=<vertex> A metric indicates the desired replicas of a Vertex controller_vertex_current_replicas Gauge ns=<namespace> pipeline=<pipeline> vertex=<vertex> A metric indicates the current replicas of a Vertex controller_vertex_min_replicas Gauge ns=<namespace> pipeline=<pipeline> vertex=<vertex> A metric indicates the min replicas of a Vertex controller_vertex_max_replicas Gauge ns=<namespace> pipeline=<pipeline> vertex=<vertex> A metric indicates the max replicas of a Vertex controller_monovtx_desired_replicas Gauge ns=<namespace> mvtx_name=<mvtx> A metric indicates the desired replicas of a MonoVertex controller_monovtx_current_replicas Gauge ns=<namespace> mvtx_name=<mvtx> A metric indicates the current replicas of a MonoVertex controller_monovtx_min_replicas Gauge ns=<namespace> mvtx_name=<mvtx> A metric indicates the min replicas of a MonoVertex controller_monovtx_max_replicas Gauge ns=<namespace> mvtx_name=<mvtx> A metric indicates the max replicas of a MonoVertex Prometheus Operator for Scraping Metrics: \u00b6 You can follow the prometheus operator setup guide if you would like to use prometheus operator configured in your cluster. You can also set up prometheus operator via helm . Configure the below Service/Pod Monitors for scraping your pipeline/monovertex metrics: \u00b6 apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-pipeline-metrics spec : endpoints : - scheme : https port : metrics targetPort : 2469 tlsConfig : insecureSkipVerify : true selector : matchLabels : app.kubernetes.io/component : vertex app.kubernetes.io/managed-by : vertex-controller app.kubernetes.io/part-of : numaflow matchExpressions : - key : numaflow.numaproj.io/pipeline-name operator : Exists - key : numaflow.numaproj.io/vertex-name operator : Exists --- apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-pipeline-daemon-metrics spec : endpoints : - scheme : https port : tcp targetPort : 4327 tlsConfig : insecureSkipVerify : true selector : matchLabels : app.kubernetes.io/component : daemon app.kubernetes.io/managed-by : pipeline-controller app.kubernetes.io/part-of : numaflow matchExpressions : - key : numaflow.numaproj.io/pipeline-name operator : Exists --- apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-mvtx-metrics spec : endpoints : - scheme : https port : metrics targetPort : 2469 tlsConfig : insecureSkipVerify : true selector : matchLabels : app.kubernetes.io/component : mono-vertex app.kubernetes.io/managed-by : mono-vertex-controller app.kubernetes.io/part-of : numaflow matchExpressions : - key : numaflow.numaproj.io/mono-vertex-name operator : Exists --- apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-mvtx-daemon-metrics spec : endpoints : - scheme : https port : tcp targetPort : 4327 tlsConfig : insecureSkipVerify : true selector : matchLabels : app.kubernetes.io/component : mono-vertex-daemon app.kubernetes.io/managed-by : mono-vertex-controller app.kubernetes.io/part-of : numaflow matchExpressions : - key : numaflow.numaproj.io/mono-vertex-name operator : Exists --- apiVersion : monitoring.coreos.com/v1 kind : PodMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-controller-metrics spec : podMetricsEndpoints : - scheme : http port : metrics targetPort : 9090 selector : matchLabels : app.kubernetes.io/component : controller-manager app.kubernetes.io/name : controller-manager app.kubernetes.io/part-of : numaflow Configure the below Service Monitor if you use the NATS Jetstream ISB for your NATS Jetstream metrics: \u00b6 apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-isbsvc-jetstream-metrics spec : endpoints : - scheme : http port : metrics targetPort : 7777 selector : matchLabels : app.kubernetes.io/component : isbsvc app.kubernetes.io/managed-by : isbsvc-controller app.kubernetes.io/part-of : numaflow numaflow.numaproj.io/isbsvc-type : jetstream matchExpressions : - key : numaflow.numaproj.io/isbsvc-name operator : Exists","title":"Metrics"},{"location":"operations/metrics/metrics/#metrics","text":"Numaflow provides the following prometheus metrics which we can use to monitor our pipeline and setup any alerts if needed.","title":"Metrics"},{"location":"operations/metrics/metrics/#golden-signals","text":"These metrics in combination can be used to determine the overall health of your pipeline","title":"Golden Signals"},{"location":"operations/metrics/metrics/#traffic","text":"These metrics can be used to determine throughput of your pipeline. Metric name Metric type Labels Description forwarder_read_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages read by a given Vertex from an Inter-Step Buffer Partition or Source forwarder_data_read_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of data messages read by a given Vertex from an Inter-Step Buffer Partition or Source forwarder_read_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of bytes read by a given Vertex from an Inter-Step Buffer Partition or Source forwarder_data_read_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of data bytes read by a given Vertex from an Inter-Step Buffer Partition or Source source_forwarder_transformer_read_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=Source replica=<replica-index> partition_name=<partition-name> Provides the total number of messages read by source transformer forwarder_write_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages written to Inter-Step Buffer by a given Vertex forwarder_write_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of bytes written to Inter-Step Buffer by a given Vertex source_forwarder_transformer_write_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=Source replica=<replica-index> partition_name=<partition-name> Provides the total number of messages written by source transformer forwarder_fbsink_write_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages written to a fallback sink forwarder_fbsink_write_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of bytes written to a fallback sink forwarder_ack_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages acknowledged by a given Vertex from an Inter-Step Buffer Partition forwarder_drop_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages dropped by a given Vertex due to a full Inter-Step Buffer Partition forwarder_drop_bytes_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of bytes dropped by a given Vertex due to a full Inter-Step Buffer Partition forwarder_udf_read_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages read by UDF forwarder_udf_write_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the total number of messages written by UDF","title":"Traffic"},{"location":"operations/metrics/metrics/#latency","text":"These metrics can be used to determine the latency of your pipeline. Metric name Metric type Labels Description pipeline_processing_lag Gauge pipeline=<pipeline-name> Pipeline processing lag in milliseconds (max watermark - min watermark) pipeline_watermark_cmp_now Gauge pipeline=<pipeline-name> Max watermark of source compared with current time in milliseconds forwarder_read_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the histogram distribution of the processing times of read operations forwarder_write_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the histogram distribution of the processing times of write operations forwarder_ack_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the histogram distribution of the processing times of ack operations forwarder_fbsink_write_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Provides the histogram distribution of the processing times of write operations to a fallback sink source_forwarder_transformer_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=Source replica=<replica-index> partition_name=<partition-name> Provides a histogram distribution of the processing times of source transformer forwarder_udf_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> Provides a histogram distribution of the processing times of User-defined Functions (UDFs) in a map vertex forwarder_forward_chunk_processing_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> Provides a histogram distribution of the processing times of the forwarder function as a whole in a map vertex reduce_data_forward_forward_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> replica=<replica-index> Provides a histogram distribution of the forwarding times of the data from an ISB to a PBQ reduce_pbq_write_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> replica=<replica-index> Provides a histogram distribution of the processing times of write operations to a PBQ reduce_pnf_process_time Histogram pipeline=<pipeline-name> vertex=<vertex-name> replica=<replica-index> Provides a histogram distribution of the processing times of the reducer vertex_pending_messages Gauge pipeline=<pipeline-name> vertex=<vertex-name> period=<duration> partition_name=<partition-name> Provides the average pending messages in the last period of seconds. It is the pending messages of a vertex","title":"Latency"},{"location":"operations/metrics/metrics/#errors","text":"These metrics can be used to determine if there are any errors in the pipeline. Metric name Metric type Labels Description pipeline_data_processing_health Gauge pipeline=<pipeline-name> Pipeline data processing health status. 1: Healthy, 0: Unknown, -1: Warning, -2: Critical controller_isbsvc_health Gauge ns=<namespace> isbsvc=<isbsvc-name> A metric to indicate whether the ISB Service is healthy. '1' means healthy, '0' means unhealthy controller_pipeline_health Gauge ns=<namespace> pipeline=<pipeline-name> A metric to indicate whether the Pipeline is healthy. '1' means healthy, '0' means unhealthy controller_monovtx_health Gauge ns=<namespace> mvtx_name=<mvtx-name> A metric to indicate whether the MonoVertex is healthy. '1' means healthy, '0' means unhealthy forwarder_platform_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> Indicates any internal errors which could stop pipeline processing forwarder_read_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Indicates any errors while reading messages by the forwarder source_forwarder_transformer_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=Source replica=<replica-index> partition_name=<partition-name> Indicates source transformer errors forwarder_write_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Indicates any errors while writing messages by the forwarder forwarder_fbsink_write_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Indicates any errors while writing to a fallback sink forwarder_ack_error_total Counter pipeline=<pipeline-name> vertex=<vertex-name> vertex_type=<vertex-type> replica=<replica-index> partition_name=<partition-name> Indicates any errors while acknowledging messages by the forwarder kafka_sink_write_timeout_total Counter pipeline=<pipeline-name> vertex=<vertex-name> Provides the write timeouts while writing to the Kafka sink isb_jetstream_read_error_total Counter buffer=<buffer-name> Indicates any read errors with NATS Jetstream ISB isb_jetstream_write_error_total Counter buffer=<buffer-name> Indicates any write errors with NATS Jetstream ISB","title":"Errors"},{"location":"operations/metrics/metrics/#saturation","text":"","title":"Saturation"},{"location":"operations/metrics/metrics/#nats-jetstream-isb","text":"Metric name Metric type Labels Description isb_jetstream_isFull_total Counter buffer=<buffer-name> Indicates if the ISB is full. Continual increase of this counter metric indicates a potential backpressure that can be built on the pipeline isb_jetstream_buffer_soft_usage Gauge buffer=<buffer-name> Indicates the usage/utilization of a NATS Jetstream ISB isb_jetstream_buffer_solid_usage Gauge buffer=<buffer-name> Indicates the solid usage of a NATS Jetstream ISB isb_jetstream_buffer_pending Gauge buffer=<buffer-name> Indicate the number of pending messages at a given point in time. isb_jetstream_buffer_ack_pending Gauge buffer=<buffer-name> Indicates the number of messages pending acknowledge at a given point in time","title":"NATS JetStream ISB"},{"location":"operations/metrics/metrics/#others","text":"Metric name Metric type Labels Description build_info Gauge component=<component> component_name=<component_name> version=<version> platform=<platform> A metric with a constant value '1', labeled by Numaflow binary version, platform, and other information. The value of component could be 'daemon', 'vertex', 'mono-vertex-daemon', etc controller_build_info Gauge version=<version> platform=<platform> A metric with a constant value '1', labeled with controller version and platform from which Numaflow was built sdk_info Gauge component=<component> component_name=<component_name> type=<sdk_type> version=<sdk_version> language=<sdk_language> A metric with a constant value '1', labeled by SDK information such as version, language, and type controller_pipeline_desired_phase Gauge ns=<namespace> pipeline=<pipeline> A metric to indicate the pipeline phase. '1' means Running, '2' means Paused controller_pipeline_current_phase Gauge ns=<namespace> pipeline=<pipeline> A metric to indicate the pipeline phase. '0' means Unknown, '1' means Running, '2' means Paused, '3' means Failed, '4' means Pausing, '5' means 'Deleting' controller_monovtx_desired_phase Gauge ns=<namespace> mvtx_name=<mvtx> A metric to indicate the MonoVertex phase. '1' means Running, '2' means Paused controller_monovtx_current_phase Gauge ns=<namespace> mvtx_name=<mvtx> A metric to indicate the MonoVertex phase. '0' means Unknown, '1' means Running, '2' means Paused, '3' means Failed controller_vertex_desired_replicas Gauge ns=<namespace> pipeline=<pipeline> vertex=<vertex> A metric indicates the desired replicas of a Vertex controller_vertex_current_replicas Gauge ns=<namespace> pipeline=<pipeline> vertex=<vertex> A metric indicates the current replicas of a Vertex controller_vertex_min_replicas Gauge ns=<namespace> pipeline=<pipeline> vertex=<vertex> A metric indicates the min replicas of a Vertex controller_vertex_max_replicas Gauge ns=<namespace> pipeline=<pipeline> vertex=<vertex> A metric indicates the max replicas of a Vertex controller_monovtx_desired_replicas Gauge ns=<namespace> mvtx_name=<mvtx> A metric indicates the desired replicas of a MonoVertex controller_monovtx_current_replicas Gauge ns=<namespace> mvtx_name=<mvtx> A metric indicates the current replicas of a MonoVertex controller_monovtx_min_replicas Gauge ns=<namespace> mvtx_name=<mvtx> A metric indicates the min replicas of a MonoVertex controller_monovtx_max_replicas Gauge ns=<namespace> mvtx_name=<mvtx> A metric indicates the max replicas of a MonoVertex","title":"Others"},{"location":"operations/metrics/metrics/#prometheus-operator-for-scraping-metrics","text":"You can follow the prometheus operator setup guide if you would like to use prometheus operator configured in your cluster. You can also set up prometheus operator via helm .","title":"Prometheus Operator for Scraping Metrics:"},{"location":"operations/metrics/metrics/#configure-the-below-servicepod-monitors-for-scraping-your-pipelinemonovertex-metrics","text":"apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-pipeline-metrics spec : endpoints : - scheme : https port : metrics targetPort : 2469 tlsConfig : insecureSkipVerify : true selector : matchLabels : app.kubernetes.io/component : vertex app.kubernetes.io/managed-by : vertex-controller app.kubernetes.io/part-of : numaflow matchExpressions : - key : numaflow.numaproj.io/pipeline-name operator : Exists - key : numaflow.numaproj.io/vertex-name operator : Exists --- apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-pipeline-daemon-metrics spec : endpoints : - scheme : https port : tcp targetPort : 4327 tlsConfig : insecureSkipVerify : true selector : matchLabels : app.kubernetes.io/component : daemon app.kubernetes.io/managed-by : pipeline-controller app.kubernetes.io/part-of : numaflow matchExpressions : - key : numaflow.numaproj.io/pipeline-name operator : Exists --- apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-mvtx-metrics spec : endpoints : - scheme : https port : metrics targetPort : 2469 tlsConfig : insecureSkipVerify : true selector : matchLabels : app.kubernetes.io/component : mono-vertex app.kubernetes.io/managed-by : mono-vertex-controller app.kubernetes.io/part-of : numaflow matchExpressions : - key : numaflow.numaproj.io/mono-vertex-name operator : Exists --- apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-mvtx-daemon-metrics spec : endpoints : - scheme : https port : tcp targetPort : 4327 tlsConfig : insecureSkipVerify : true selector : matchLabels : app.kubernetes.io/component : mono-vertex-daemon app.kubernetes.io/managed-by : mono-vertex-controller app.kubernetes.io/part-of : numaflow matchExpressions : - key : numaflow.numaproj.io/mono-vertex-name operator : Exists --- apiVersion : monitoring.coreos.com/v1 kind : PodMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-controller-metrics spec : podMetricsEndpoints : - scheme : http port : metrics targetPort : 9090 selector : matchLabels : app.kubernetes.io/component : controller-manager app.kubernetes.io/name : controller-manager app.kubernetes.io/part-of : numaflow","title":"Configure the below Service/Pod Monitors for scraping your pipeline/monovertex metrics:"},{"location":"operations/metrics/metrics/#configure-the-below-service-monitor-if-you-use-the-nats-jetstream-isb-for-your-nats-jetstream-metrics","text":"apiVersion : monitoring.coreos.com/v1 kind : ServiceMonitor metadata : labels : app.kubernetes.io/part-of : numaflow name : numaflow-isbsvc-jetstream-metrics spec : endpoints : - scheme : http port : metrics targetPort : 7777 selector : matchLabels : app.kubernetes.io/component : isbsvc app.kubernetes.io/managed-by : isbsvc-controller app.kubernetes.io/part-of : numaflow numaflow.numaproj.io/isbsvc-type : jetstream matchExpressions : - key : numaflow.numaproj.io/isbsvc-name operator : Exists","title":"Configure the below Service Monitor if you use the NATS Jetstream ISB for your NATS Jetstream metrics:"},{"location":"operations/ui/ui-access-path/","text":"UI Access Path \u00b6 By default, Numaflow UI server will host the service at the root / ie. localhost:8443 . If a user needs to access the UI server under a different path, this can be achieved with following configuration. This is useful when the UI is hosted behind a reverse proxy or ingress controller that requires a specific path. Configure server.base.href in the ConfigMap numaflow-cmd-params-config . apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : ### Base href for Numaflow UI server, defaults to '/'. server.base.href : \"/app\" The configuration above will host the service at localhost:8443/app . Note that this new access path will work with or without a trailing slash.","title":"Access Path"},{"location":"operations/ui/ui-access-path/#ui-access-path","text":"By default, Numaflow UI server will host the service at the root / ie. localhost:8443 . If a user needs to access the UI server under a different path, this can be achieved with following configuration. This is useful when the UI is hosted behind a reverse proxy or ingress controller that requires a specific path. Configure server.base.href in the ConfigMap numaflow-cmd-params-config . apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : ### Base href for Numaflow UI server, defaults to '/'. server.base.href : \"/app\" The configuration above will host the service at localhost:8443/app . Note that this new access path will work with or without a trailing slash.","title":"UI Access Path"},{"location":"operations/ui/authn/authentication/","text":"Authentication \u00b6 Numaflow UI server provides 2 approaches for authentication. SSO with Dex Local users There's also an option to disable authentication/authorization by setting server.disable.auth: \"true\" in the ConfigMap 1numaflow-cmd-params-config`, in this case, everybody has full access and privileges to any features of the UI (not recommended).","title":"Overview"},{"location":"operations/ui/authn/authentication/#authentication","text":"Numaflow UI server provides 2 approaches for authentication. SSO with Dex Local users There's also an option to disable authentication/authorization by setting server.disable.auth: \"true\" in the ConfigMap 1numaflow-cmd-params-config`, in this case, everybody has full access and privileges to any features of the UI (not recommended).","title":"Authentication"},{"location":"operations/ui/authn/dex/","text":"Dex Server \u00b6 Numaflow comes with a Dex Server for authentication integration. Currently, the supported identity provider is Github. SSO configuration of Numaflow UI will require editing some configuration detailed below. 1. Register application for Github \u00b6 In Github, register a new OAuth application. The callback address should be the homepage of your Numaflow UI + /dex/callback . After registering this application, you will be given a client ID. You will need this value and also generate a new client secret. 2. Configuring Numaflow \u00b6 First we need to configure server.disable.auth to false in the ConfigMap numaflow-cmd-params-config . This will enable authentication and authorization for the UX server. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : ### Whether to disable authentication and authorization for the UX server, defaults to false. server.disable.auth : \"false\" # Next we need to configure the numaflow-dex-server-config ConfigMap. Change <ORG_NAME> to your organization you created the application under and include the correct teams. This file will be read by the init container of the Dex server and generate the config it will server. kind : ConfigMap apiVersion : v1 metadata : name : numaflow-dex-server-config data : config.yaml : | connectors: - type: github # https://dexidp.io/docs/connectors/github/ id: github name: GitHub config: clientID: $GITHUB_CLIENT_ID clientSecret: $GITHUB_CLIENT_SECRET orgs: - name: <ORG_NAME> teams: - admin - readonly Finally we will need to create/update the numaflow-dex-secrets Secret. You will need to add the client ID and secret you created earlier for the application here. apiVersion : v1 kind : Secret metadata : name : numaflow-dex-secrets stringData : # https://dexidp.io/docs/connectors/github/ dex-github-client-id : <GITHUB_CLIENT_ID> dex-github-client-secret : <GITHUB_CLIENT_SECRET> 3. Restarting Pods \u00b6 If you are enabling/disabling authorization and authentication for the Numaflow server, it will need to be restarted. Any changes or additions to the connectors in the numaflow-dex-server-config ConfigMap will need to be read and generated again requiring a restart as well.","title":"SSO with Dex"},{"location":"operations/ui/authn/dex/#dex-server","text":"Numaflow comes with a Dex Server for authentication integration. Currently, the supported identity provider is Github. SSO configuration of Numaflow UI will require editing some configuration detailed below.","title":"Dex Server"},{"location":"operations/ui/authn/dex/#1-register-application-for-github","text":"In Github, register a new OAuth application. The callback address should be the homepage of your Numaflow UI + /dex/callback . After registering this application, you will be given a client ID. You will need this value and also generate a new client secret.","title":"1. Register application for Github"},{"location":"operations/ui/authn/dex/#2-configuring-numaflow","text":"First we need to configure server.disable.auth to false in the ConfigMap numaflow-cmd-params-config . This will enable authentication and authorization for the UX server. apiVersion : v1 kind : ConfigMap metadata : name : numaflow-cmd-params-config data : ### Whether to disable authentication and authorization for the UX server, defaults to false. server.disable.auth : \"false\" # Next we need to configure the numaflow-dex-server-config ConfigMap. Change <ORG_NAME> to your organization you created the application under and include the correct teams. This file will be read by the init container of the Dex server and generate the config it will server. kind : ConfigMap apiVersion : v1 metadata : name : numaflow-dex-server-config data : config.yaml : | connectors: - type: github # https://dexidp.io/docs/connectors/github/ id: github name: GitHub config: clientID: $GITHUB_CLIENT_ID clientSecret: $GITHUB_CLIENT_SECRET orgs: - name: <ORG_NAME> teams: - admin - readonly Finally we will need to create/update the numaflow-dex-secrets Secret. You will need to add the client ID and secret you created earlier for the application here. apiVersion : v1 kind : Secret metadata : name : numaflow-dex-secrets stringData : # https://dexidp.io/docs/connectors/github/ dex-github-client-id : <GITHUB_CLIENT_ID> dex-github-client-secret : <GITHUB_CLIENT_SECRET>","title":"2. Configuring Numaflow"},{"location":"operations/ui/authn/dex/#3-restarting-pods","text":"If you are enabling/disabling authorization and authentication for the Numaflow server, it will need to be restarted. Any changes or additions to the connectors in the numaflow-dex-server-config ConfigMap will need to be read and generated again requiring a restart as well.","title":"3. Restarting Pods"},{"location":"operations/ui/authn/local-users/","text":"Local Users \u00b6 In addition to the authentication using Dex, we also provide an authentication mechanism for local user based on JSON Web Token (JWT). NOTE \u00b6 When you create local users, each of those users will need additional RBAC rules set up, otherwise they will fall back to the default policy specified by policy.default field of the numaflow-server-rbac-config ConfigMap. Numaflow comes with a built-in admin user that has full access to the system. It is recommended to use admin user for initial configuration then switch to local users or configure SSO integration. Accessing with admin user \u00b6 A built-in admin user comes with a randomly generated password that is stored in numaflow-server-secrets Secret: Example \u00b6 kubectl get secret numaflow-server-secrets -n <namespace> -o jsonpath = '{.data.admin\\.initial-password}' | base64 --decode Use the admin username and password obtained above to log in to the UI. Creating Users \u00b6 1. Adding the username \u00b6 Users can be created by updating the numaflow-server-local-user-config ConfigMap: Example \u00b6 apiVersion: v1 kind: ConfigMap metadata: name: numaflow-server-local-user-config data: # Format: {username}.enabled: \"true\" bob.enabled: \"true\" 2. Generating the password \u00b6 When adding new users, it is necessary to generate a bcrypt hash of their password: Example \u00b6 # Format: htpasswd -bnBC 10 \"\" <password> | tr -d ':\\n' htpasswd -bnBC 10 \"\" password | tr -d ':\\n' 3. Adding the password for the username \u00b6 To add the password generated above for the respective user, you can update the numaflow-server-secrets Secret: Example \u00b6 apiVersion: v1 kind: Secret metadata: name: numaflow-server-secrets type: Opaque stringData: # Format: {username}.password: <bcrypt_hash_password_value> bob.password: $2 y $10$0 TCvrnLHQsQtEJVdXNNL6eeXaxHmGnQO.R8zhh0Mwr2RM7s42knTK You can also update the password for admin user similarly, it will be considered over the initial password NOTE \u00b6 For the example above, the username is bob and the password is password . Disabling Users \u00b6 Users can be disabled by updating the numaflow-server-local-user-config ConfigMap, including the system generated admin user: Example \u00b6 apiVersion: v1 kind: ConfigMap metadata: name: numaflow-server-local-user-config data: # Set the value to \"false\" to disable the user. bob.enabled: \"false\" Deleting Users \u00b6 Users can be deleted by removing the corresponding entries: 1. numaflow-server-local-user-config ConfigMap \u00b6 # Format: {username}.enabled: null kubectl patch configmap <configmap-name> -n <namespace> -p '{\"data\": {\"bob.enabled\": null}}' --type merge 2. numaflow-server-secrets Secret \u00b6 # Format: {username}.password: null kubectl patch secret <secret-name> -n <namespace> -p '{\"data\": {\"bob.password\": null}}' --type merge","title":"Local Users"},{"location":"operations/ui/authn/local-users/#local-users","text":"In addition to the authentication using Dex, we also provide an authentication mechanism for local user based on JSON Web Token (JWT).","title":"Local Users"},{"location":"operations/ui/authn/local-users/#note","text":"When you create local users, each of those users will need additional RBAC rules set up, otherwise they will fall back to the default policy specified by policy.default field of the numaflow-server-rbac-config ConfigMap. Numaflow comes with a built-in admin user that has full access to the system. It is recommended to use admin user for initial configuration then switch to local users or configure SSO integration.","title":"NOTE"},{"location":"operations/ui/authn/local-users/#accessing-with-admin-user","text":"A built-in admin user comes with a randomly generated password that is stored in numaflow-server-secrets Secret:","title":"Accessing with admin user"},{"location":"operations/ui/authn/local-users/#example","text":"kubectl get secret numaflow-server-secrets -n <namespace> -o jsonpath = '{.data.admin\\.initial-password}' | base64 --decode Use the admin username and password obtained above to log in to the UI.","title":"Example"},{"location":"operations/ui/authn/local-users/#creating-users","text":"","title":"Creating Users"},{"location":"operations/ui/authn/local-users/#1-adding-the-username","text":"Users can be created by updating the numaflow-server-local-user-config ConfigMap:","title":"1. Adding the username"},{"location":"operations/ui/authn/local-users/#example_1","text":"apiVersion: v1 kind: ConfigMap metadata: name: numaflow-server-local-user-config data: # Format: {username}.enabled: \"true\" bob.enabled: \"true\"","title":"Example"},{"location":"operations/ui/authn/local-users/#2-generating-the-password","text":"When adding new users, it is necessary to generate a bcrypt hash of their password:","title":"2. Generating the password"},{"location":"operations/ui/authn/local-users/#example_2","text":"# Format: htpasswd -bnBC 10 \"\" <password> | tr -d ':\\n' htpasswd -bnBC 10 \"\" password | tr -d ':\\n'","title":"Example"},{"location":"operations/ui/authn/local-users/#3-adding-the-password-for-the-username","text":"To add the password generated above for the respective user, you can update the numaflow-server-secrets Secret:","title":"3. Adding the password for the username"},{"location":"operations/ui/authn/local-users/#example_3","text":"apiVersion: v1 kind: Secret metadata: name: numaflow-server-secrets type: Opaque stringData: # Format: {username}.password: <bcrypt_hash_password_value> bob.password: $2 y $10$0 TCvrnLHQsQtEJVdXNNL6eeXaxHmGnQO.R8zhh0Mwr2RM7s42knTK You can also update the password for admin user similarly, it will be considered over the initial password","title":"Example"},{"location":"operations/ui/authn/local-users/#note_1","text":"For the example above, the username is bob and the password is password .","title":"NOTE"},{"location":"operations/ui/authn/local-users/#disabling-users","text":"Users can be disabled by updating the numaflow-server-local-user-config ConfigMap, including the system generated admin user:","title":"Disabling Users"},{"location":"operations/ui/authn/local-users/#example_4","text":"apiVersion: v1 kind: ConfigMap metadata: name: numaflow-server-local-user-config data: # Set the value to \"false\" to disable the user. bob.enabled: \"false\"","title":"Example"},{"location":"operations/ui/authn/local-users/#deleting-users","text":"Users can be deleted by removing the corresponding entries:","title":"Deleting Users"},{"location":"operations/ui/authn/local-users/#1-numaflow-server-local-user-config-configmap","text":"# Format: {username}.enabled: null kubectl patch configmap <configmap-name> -n <namespace> -p '{\"data\": {\"bob.enabled\": null}}' --type merge","title":"1. numaflow-server-local-user-config ConfigMap"},{"location":"operations/ui/authn/local-users/#2-numaflow-server-secrets-secret","text":"# Format: {username}.password: null kubectl patch secret <secret-name> -n <namespace> -p '{\"data\": {\"bob.password\": null}}' --type merge","title":"2. numaflow-server-secrets Secret"},{"location":"operations/ui/authz/rbac/","text":"Authorization \u00b6 Numaflow UI utilizes a role-based access control (RBAC) model to manage authorization, the RBAC policy and permissions are defined in the ConfigMap numaflow-server-rbac-config . There are two main sections in the ConfigMap. Rules \u00b6 Policies and groups are the two main entities defined in rules section, both of them work in conjunction with each other. The groups are used to define a set of users with the same permissions and the policies are used to define the specific permissions for these users or groups. # Policies go here p, role:admin, *, *, * p, role:readonly, *, *, GET # Groups go here g, admin, role:admin g, my-github-org:my-github-team, role:readonly Here we have defined two policies for the custom groups role:admin and role:readonly . The first policy allows the group role:admin to access all resources in all namespaces with all actions. The second policy allows the group role:readonly to access all resources in all namespaces with the GET action. To add a new policy , add a new line in the format: p, <user/group>, <namespace>, <resource>, <action> User/Group : The user/group requesting access to a resource. This is the identifier extracted from the authentication token, such as a username, email address, or ID. Or could be a group defined in the groups section. Resource : The namespace in the cluster which is being accessed by the user. This can allow for selective access to namespaces. Object : This could be a specific resource in the namespace, such as a pipeline, isbsvc or any event based resource. Action : The action being performed on the resource using the API. These follow the standard HTTP verbs, such as GET, POST, PUT, DELETE, etc. The namespace, resource and action supports a wildcard * as an allow all function. Few examples: a policy line p, test@test.com, *, *, POST would allow the user with the given email address to access all resources in all namespaces with the POST action. a policy line p, test_user, *, *, * would allow the user with the given username to access all resources in all namespaces with all actions. a policy line p, role:admin_ns, test_ns, *, * would allow the group role:admin_ns to access all resources in the namespace test_ns with all actions. a policy line p, test_user, test_ns, *, GET would allow the user with the given username to access all resources in the namespace test_ns with the GET action. Groups can be defined by adding a new line in the format: g, <user>, <group> Here user is the identifier extracted from the authentication token, such as a username, email address, or ID. And group is the name of the group to which the user is being added. These are useful for defining a set of users with the same permissions. The group can be used in the policy definition in place of the user. And thus any user added to the group will have the same permissions as the group. Few examples: a group line g, test@test.com, role:readonly would add the user with the given email address to the group role:readonly. a group line g, test_user, role:admin would add the user with the given username to the group role:admin. Configuration \u00b6 This defines certain properties for the Casbin enforcer. The properties are defined in the following format: rbac-conf.yaml: | policy.default: role:readonly policy.scopes: groups,email,username We see two properties defined here: policy.default : This defines the default role for a user. If a user does not have any roles defined, then this role will be used for the user. This is useful for defining a default role for all users. policy.scopes : The scopes field controls which authentication scopes to examine during rbac enforcement. We can have multiple scopes, and the first scope that matches with the policy will be used. \"groups\", which means that the groups field of the user's token will be examined, This is default value and is used if no scopes are defined. \"email\", which means that the email field of the user's token will be examined \"username\", which means that the username field of the user's token will be examined Multiple scopes can be provided as a comma-separated, e.g \"groups,email,username\" This scope information is used to extract the user information from the token and then used to enforce the policies. Thus is it important to have the rules defined in the above section to map with the scopes expected in the configuration. Note : The rbac-conf.yaml file can be updated during runtime and the changes will be reflected immediately. This is useful for changing the default role for all users or adding a new scope to be used for rbac enforcement.","title":"Authorization"},{"location":"operations/ui/authz/rbac/#authorization","text":"Numaflow UI utilizes a role-based access control (RBAC) model to manage authorization, the RBAC policy and permissions are defined in the ConfigMap numaflow-server-rbac-config . There are two main sections in the ConfigMap.","title":"Authorization"},{"location":"operations/ui/authz/rbac/#rules","text":"Policies and groups are the two main entities defined in rules section, both of them work in conjunction with each other. The groups are used to define a set of users with the same permissions and the policies are used to define the specific permissions for these users or groups. # Policies go here p, role:admin, *, *, * p, role:readonly, *, *, GET # Groups go here g, admin, role:admin g, my-github-org:my-github-team, role:readonly Here we have defined two policies for the custom groups role:admin and role:readonly . The first policy allows the group role:admin to access all resources in all namespaces with all actions. The second policy allows the group role:readonly to access all resources in all namespaces with the GET action. To add a new policy , add a new line in the format: p, <user/group>, <namespace>, <resource>, <action> User/Group : The user/group requesting access to a resource. This is the identifier extracted from the authentication token, such as a username, email address, or ID. Or could be a group defined in the groups section. Resource : The namespace in the cluster which is being accessed by the user. This can allow for selective access to namespaces. Object : This could be a specific resource in the namespace, such as a pipeline, isbsvc or any event based resource. Action : The action being performed on the resource using the API. These follow the standard HTTP verbs, such as GET, POST, PUT, DELETE, etc. The namespace, resource and action supports a wildcard * as an allow all function. Few examples: a policy line p, test@test.com, *, *, POST would allow the user with the given email address to access all resources in all namespaces with the POST action. a policy line p, test_user, *, *, * would allow the user with the given username to access all resources in all namespaces with all actions. a policy line p, role:admin_ns, test_ns, *, * would allow the group role:admin_ns to access all resources in the namespace test_ns with all actions. a policy line p, test_user, test_ns, *, GET would allow the user with the given username to access all resources in the namespace test_ns with the GET action. Groups can be defined by adding a new line in the format: g, <user>, <group> Here user is the identifier extracted from the authentication token, such as a username, email address, or ID. And group is the name of the group to which the user is being added. These are useful for defining a set of users with the same permissions. The group can be used in the policy definition in place of the user. And thus any user added to the group will have the same permissions as the group. Few examples: a group line g, test@test.com, role:readonly would add the user with the given email address to the group role:readonly. a group line g, test_user, role:admin would add the user with the given username to the group role:admin.","title":"Rules"},{"location":"operations/ui/authz/rbac/#configuration","text":"This defines certain properties for the Casbin enforcer. The properties are defined in the following format: rbac-conf.yaml: | policy.default: role:readonly policy.scopes: groups,email,username We see two properties defined here: policy.default : This defines the default role for a user. If a user does not have any roles defined, then this role will be used for the user. This is useful for defining a default role for all users. policy.scopes : The scopes field controls which authentication scopes to examine during rbac enforcement. We can have multiple scopes, and the first scope that matches with the policy will be used. \"groups\", which means that the groups field of the user's token will be examined, This is default value and is used if no scopes are defined. \"email\", which means that the email field of the user's token will be examined \"username\", which means that the username field of the user's token will be examined Multiple scopes can be provided as a comma-separated, e.g \"groups,email,username\" This scope information is used to extract the user information from the token and then used to enforce the policies. Thus is it important to have the rules defined in the above section to map with the scopes expected in the configuration. Note : The rbac-conf.yaml file can be updated during runtime and the changes will be reflected immediately. This is useful for changing the default role for all users or adding a new scope to be used for rbac enforcement.","title":"Configuration"},{"location":"specifications/authorization/","text":"UI Authorization \u00b6 We utilize a role-based access control (RBAC) model to manage authorization in Numaflow. Along with this we utilize Casbin as a library for the implementation of these policies. Permissions and Policies \u00b6 The following model configuration is given to define the policies. The policy model is defined in the Casbin policy language. [request_definition] r = sub, res, obj, act [policy_definition] p = sub, res, obj, act [role_definition] g = _, _ [policy_effect] e = some(where (p.eft == allow)) [matchers] m = g(r.sub, p.sub) && patternMatch(r.res, p.res) && stringMatch(r.obj, p.obj) && stringMatch(r.act, p.act) The policy model consists of the following sections: request_definition: The request definition section defines the request attributes. In our case, the request attributes are the user, resource, action, and object. policy_definition: The policy definition section defines the policy attributes. In our case, the policy attributes are the user, resource, action, and object. role_definition: The role definition section defines the role attributes. In our case, the role attributes are the user and role. policy_effect: The policy effect defines what action is to be taken on auth, In our case, the policy effect is allow. matchers: The matcher section defines the matching logic which decides whether is a given request matches any policy or not. These matches are done in order of the definition above and shortcircuit at the first failure. There are custom functions like patternMatch and stringMatch. patternMatch: This function is used to match the resource with the policy resource using os path pattern matching along with adding support for wildcards for allowAll. stringMatch: This function is used to match the object and action and uses a simple exact string match. This also supports wildcards for allowAll The policy model for us follows the following structure for all policies defined and any requests made to th UI server: User: The user requesting access to a resource. This could be any identifier, such as a username, email address, or ID. Resource: The namespace in the cluster which is being accessed by the user. This can allow for selective access to namespaces. We have wildcard \"*\" to allow access to all namespaces. Object : This could be a specific resource in the namespace, such as a pipeline, isbsvc or any event based resource. We have wildcard \"*\" to allow access to all resources. Action: The action being performed on the resource using the API. These follow the standard HTTP verbs, such as GET, POST, PUT, DELETE, etc. We have wildcard \"*\" to allow access to all actions. Refer to the RBAC to learn more about how to configure authorization policies for Numaflow UI.","title":"UI Authorization"},{"location":"specifications/authorization/#ui-authorization","text":"We utilize a role-based access control (RBAC) model to manage authorization in Numaflow. Along with this we utilize Casbin as a library for the implementation of these policies.","title":"UI Authorization"},{"location":"specifications/authorization/#permissions-and-policies","text":"The following model configuration is given to define the policies. The policy model is defined in the Casbin policy language. [request_definition] r = sub, res, obj, act [policy_definition] p = sub, res, obj, act [role_definition] g = _, _ [policy_effect] e = some(where (p.eft == allow)) [matchers] m = g(r.sub, p.sub) && patternMatch(r.res, p.res) && stringMatch(r.obj, p.obj) && stringMatch(r.act, p.act) The policy model consists of the following sections: request_definition: The request definition section defines the request attributes. In our case, the request attributes are the user, resource, action, and object. policy_definition: The policy definition section defines the policy attributes. In our case, the policy attributes are the user, resource, action, and object. role_definition: The role definition section defines the role attributes. In our case, the role attributes are the user and role. policy_effect: The policy effect defines what action is to be taken on auth, In our case, the policy effect is allow. matchers: The matcher section defines the matching logic which decides whether is a given request matches any policy or not. These matches are done in order of the definition above and shortcircuit at the first failure. There are custom functions like patternMatch and stringMatch. patternMatch: This function is used to match the resource with the policy resource using os path pattern matching along with adding support for wildcards for allowAll. stringMatch: This function is used to match the object and action and uses a simple exact string match. This also supports wildcards for allowAll The policy model for us follows the following structure for all policies defined and any requests made to th UI server: User: The user requesting access to a resource. This could be any identifier, such as a username, email address, or ID. Resource: The namespace in the cluster which is being accessed by the user. This can allow for selective access to namespaces. We have wildcard \"*\" to allow access to all namespaces. Object : This could be a specific resource in the namespace, such as a pipeline, isbsvc or any event based resource. We have wildcard \"*\" to allow access to all resources. Action: The action being performed on the resource using the API. These follow the standard HTTP verbs, such as GET, POST, PUT, DELETE, etc. We have wildcard \"*\" to allow access to all actions. Refer to the RBAC to learn more about how to configure authorization policies for Numaflow UI.","title":"Permissions and Policies"},{"location":"specifications/autoscaling/","text":"Autoscaling \u00b6 Scale Subresource is enabled in Vertex Custom Resource , which makes it possible to scale vertex pods. To be specifically, it is enabled by adding following comments to Vertex struct model, and then corresponding CRD definition is automatically generated. // +kubebuilder:subresource:scale:specpath=.spec.replicas,statuspath=.status.replicas,selectorpath=.status.selector Pods management is done by vertex controller. With scale subresource implemented, vertex object can be scaled by either horizontal or vertical pod autoscaling. Numaflow Autoscaling \u00b6 The out of box Numaflow autoscaling is done by a scaling component running in the controller manager, you can find the source code here . The autoscaling strategy is implemented according to different type of vertices. Source Vertices \u00b6 For source vertices, we define a target processing time (in seconds) to finish processing the pending messages based on the processing rate (tps) of the vertex. targetProcessingRate = pendingMessages / targetProcessingSeconds singlePodProcessingRate = currentProcessingRate / currentReplicas desiredReplicas = targetProcessingRate / singlePodProcessingRate For example, if targetProcessingSeconds is 3, current replica number is 2 , current tps is 10000/second, and the pending messages is 60000, so we calculate the desired replica number as following: desiredReplicas = 60000 / (3 * (10000 / 2)) = 4 Numaflow autoscaling does not work for those source vertices that can not calculate pending messages. UDF and Sink Vertices \u00b6 Pending messages of a UDF or Sink vertex does not represent the real number because of the restrained writing caused by back pressure, so we use a different model to achieve autoscaling for them. For each of the vertices, we calculate the available buffer length, and consider it is contributed by all the replicas, so that we can get each replica's contribution. availableBufferLength = totalBufferLength * bufferLimit(%) - pendingMessages singleReplicaContribution = availableBufferLength / currentReplicas We define a target available buffer length, and then calculate how many replicas are needed to achieve the target. desiredReplicas = targetAvailableBufferLength / singleReplicaContribution Back Pressure Impact \u00b6 Back pressure is considered during autoscaling (which is only available for Source and UDF vertices). We measure the back pressure by defining a threshold of the buffer usage. For example, the total buffer length is 50000, buffer limit is 80%, and the back pressure threshold is 90%, if in the past period of time, the average pending messages is more than 36000 (50000 * 80% * 90%) , we consider there's back pressure. When the calculated desired replicas is greater than current replicas: For vertices which have back pressure from the directly connected vertices, instead of increasing the replica number, we decrease it by 1; For vertices which have back pressure in any of its downstream vertices, the replica number remains unchanged. Autoscaling Tuning \u00b6 Numaflow autoscaling can be tuned by updating some parameters, find the details at the doc .","title":"Autoscaling"},{"location":"specifications/autoscaling/#autoscaling","text":"Scale Subresource is enabled in Vertex Custom Resource , which makes it possible to scale vertex pods. To be specifically, it is enabled by adding following comments to Vertex struct model, and then corresponding CRD definition is automatically generated. // +kubebuilder:subresource:scale:specpath=.spec.replicas,statuspath=.status.replicas,selectorpath=.status.selector Pods management is done by vertex controller. With scale subresource implemented, vertex object can be scaled by either horizontal or vertical pod autoscaling.","title":"Autoscaling"},{"location":"specifications/autoscaling/#numaflow-autoscaling","text":"The out of box Numaflow autoscaling is done by a scaling component running in the controller manager, you can find the source code here . The autoscaling strategy is implemented according to different type of vertices.","title":"Numaflow Autoscaling"},{"location":"specifications/autoscaling/#source-vertices","text":"For source vertices, we define a target processing time (in seconds) to finish processing the pending messages based on the processing rate (tps) of the vertex. targetProcessingRate = pendingMessages / targetProcessingSeconds singlePodProcessingRate = currentProcessingRate / currentReplicas desiredReplicas = targetProcessingRate / singlePodProcessingRate For example, if targetProcessingSeconds is 3, current replica number is 2 , current tps is 10000/second, and the pending messages is 60000, so we calculate the desired replica number as following: desiredReplicas = 60000 / (3 * (10000 / 2)) = 4 Numaflow autoscaling does not work for those source vertices that can not calculate pending messages.","title":"Source Vertices"},{"location":"specifications/autoscaling/#udf-and-sink-vertices","text":"Pending messages of a UDF or Sink vertex does not represent the real number because of the restrained writing caused by back pressure, so we use a different model to achieve autoscaling for them. For each of the vertices, we calculate the available buffer length, and consider it is contributed by all the replicas, so that we can get each replica's contribution. availableBufferLength = totalBufferLength * bufferLimit(%) - pendingMessages singleReplicaContribution = availableBufferLength / currentReplicas We define a target available buffer length, and then calculate how many replicas are needed to achieve the target. desiredReplicas = targetAvailableBufferLength / singleReplicaContribution","title":"UDF and Sink Vertices"},{"location":"specifications/autoscaling/#back-pressure-impact","text":"Back pressure is considered during autoscaling (which is only available for Source and UDF vertices). We measure the back pressure by defining a threshold of the buffer usage. For example, the total buffer length is 50000, buffer limit is 80%, and the back pressure threshold is 90%, if in the past period of time, the average pending messages is more than 36000 (50000 * 80% * 90%) , we consider there's back pressure. When the calculated desired replicas is greater than current replicas: For vertices which have back pressure from the directly connected vertices, instead of increasing the replica number, we decrease it by 1; For vertices which have back pressure in any of its downstream vertices, the replica number remains unchanged.","title":"Back Pressure Impact"},{"location":"specifications/autoscaling/#autoscaling-tuning","text":"Numaflow autoscaling can be tuned by updating some parameters, find the details at the doc .","title":"Autoscaling Tuning"},{"location":"specifications/controllers/","text":"Controllers \u00b6 Currently in Numaflow , there are 3 CRDs introduced, each one has a corresponding controller. interstepbufferservices.numaflow.numaproj.io pipelines.numaflow.numaproj.io vertices.numaflow.numaproj.io The source code of the controllers is located at ./pkg/reconciler/ . Inter-Step Buffer Service Controller \u00b6 Inter-Step Buffer Service Controller is used to watch InterStepBufferService object, and manage generated resources. Pipeline Controller \u00b6 Pipeline Controller is used to watch Pipeline objects, it does following major things when there's a pipeline object created. Spawn a Kubernetes Job to create buffers and buckets in the Inter-Step Buffer Services . Create Vertex objects according to .spec.vertices defined in Pipeline object. Create some other Kubernetes objects used for the Pipeline, such as a Deployment and a Service for daemon service application. Vertex Controller \u00b6 Vertex controller watches the Vertex objects, based on the replica defined in the spec, creates a number of pods to run the workloads.","title":"Controllers"},{"location":"specifications/controllers/#controllers","text":"Currently in Numaflow , there are 3 CRDs introduced, each one has a corresponding controller. interstepbufferservices.numaflow.numaproj.io pipelines.numaflow.numaproj.io vertices.numaflow.numaproj.io The source code of the controllers is located at ./pkg/reconciler/ .","title":"Controllers"},{"location":"specifications/controllers/#inter-step-buffer-service-controller","text":"Inter-Step Buffer Service Controller is used to watch InterStepBufferService object, and manage generated resources.","title":"Inter-Step Buffer Service Controller"},{"location":"specifications/controllers/#pipeline-controller","text":"Pipeline Controller is used to watch Pipeline objects, it does following major things when there's a pipeline object created. Spawn a Kubernetes Job to create buffers and buckets in the Inter-Step Buffer Services . Create Vertex objects according to .spec.vertices defined in Pipeline object. Create some other Kubernetes objects used for the Pipeline, such as a Deployment and a Service for daemon service application.","title":"Pipeline Controller"},{"location":"specifications/controllers/#vertex-controller","text":"Vertex controller watches the Vertex objects, based on the replica defined in the spec, creates a number of pods to run the workloads.","title":"Vertex Controller"},{"location":"specifications/edges-buffers-buckets/","text":"Edges, Buffers and Buckets \u00b6 This document describes the concepts of Edge , Buffer and Bucket in a pipeline. Edges \u00b6 Edge is the connection between the vertices, specifically, edge is defined in the pipeline spec under .spec.edges . No matter if the to vertex is a Map, or a Reduce with multiple partitions, it is considered as one edge. In the following pipeline, there are 3 edges defined ( in - aoti , aoti - compute-sum , compute-sum - out ). apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : even-odd-sum spec : vertices : - name : in source : http : {} - name : atoi scale : min : 1 udf : container : image : quay.io/numaio/numaflow-go/map-even-odd:v0.5.0 - name : compute-sum partitions : 2 udf : container : image : quay.io/numaio/numaflow-go/reduce-sum:v0.5.0 groupBy : window : fixed : length : 60s keyed : true - name : out scale : min : 1 sink : log : {} edges : - from : in to : atoi - from : atoi to : compute-sum - from : compute-sum to : out Each edge could have a name for internal usage, the naming convention is {pipeline-name}-{from-vertex-name}-{to-vertex-name} . Buffers \u00b6 Buffer is InterStepBuffer . Each buffer has an owner, which is the vertex who reads from it. Each udf and sink vertex in a pipeline owns a group of partitioned buffers. Each buffer has a name with the naming convention {pipeline-name}-{vertex-name}-{index} , where the index is the partition index, starting from 0. This naming convention applies to the buffers of both map and reduce udf vertices. When multiple vertices connecting to the same vertex, if the to vertex is a Map, the data from all the from vertices will be forwarded to the group of partitoned buffers round-robinly. If the to vertex is a Reduce, the data from all the from vertices will be forwarded to the group of partitioned buffers based on the partitioning key. A Source vertex does not have any owned buffers. But a pipeline may have multiple Source vertices, followed by one vertex. Same as above, if the following vertex is a map, the data from all the Source vertices will be forwarded to the group of partitioned buffers round-robinly. If it is a reduce, the data from all the Source vertices will be forwarded to the group of partitioned buffers based on the partitioning key. Buckets \u00b6 Bucket is a K/V store (or a pair of stores) used for watermark propagation. There are 3 types of buckets in a pipeline: Edge Bucket : Each edge has a bucket, used for edge watermark propagation, no matter if the vertex that the edge leads to is a Map or a Reduce. The naming convention of an edge bucket is {pipeline-name}-{from-vertex-name}-{to-vertex-name} . Source Bucket : Each Source vertex has a source bucket, used for source watermark propagation. The naming convention of a source bucket is {pipeline-name}-{vertex-name}-SOURCE . Sink Bucket : Sitting on the right side of a Sink vertex, used for sink watermark. The naming convention of a sink bucket is {pipeline-name}-{vertex-name}-SINK . Diagrams \u00b6 Map Reduce","title":"Edges, Buffers and Buckets"},{"location":"specifications/edges-buffers-buckets/#edges-buffers-and-buckets","text":"This document describes the concepts of Edge , Buffer and Bucket in a pipeline.","title":"Edges, Buffers and Buckets"},{"location":"specifications/edges-buffers-buckets/#edges","text":"Edge is the connection between the vertices, specifically, edge is defined in the pipeline spec under .spec.edges . No matter if the to vertex is a Map, or a Reduce with multiple partitions, it is considered as one edge. In the following pipeline, there are 3 edges defined ( in - aoti , aoti - compute-sum , compute-sum - out ). apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : even-odd-sum spec : vertices : - name : in source : http : {} - name : atoi scale : min : 1 udf : container : image : quay.io/numaio/numaflow-go/map-even-odd:v0.5.0 - name : compute-sum partitions : 2 udf : container : image : quay.io/numaio/numaflow-go/reduce-sum:v0.5.0 groupBy : window : fixed : length : 60s keyed : true - name : out scale : min : 1 sink : log : {} edges : - from : in to : atoi - from : atoi to : compute-sum - from : compute-sum to : out Each edge could have a name for internal usage, the naming convention is {pipeline-name}-{from-vertex-name}-{to-vertex-name} .","title":"Edges"},{"location":"specifications/edges-buffers-buckets/#buffers","text":"Buffer is InterStepBuffer . Each buffer has an owner, which is the vertex who reads from it. Each udf and sink vertex in a pipeline owns a group of partitioned buffers. Each buffer has a name with the naming convention {pipeline-name}-{vertex-name}-{index} , where the index is the partition index, starting from 0. This naming convention applies to the buffers of both map and reduce udf vertices. When multiple vertices connecting to the same vertex, if the to vertex is a Map, the data from all the from vertices will be forwarded to the group of partitoned buffers round-robinly. If the to vertex is a Reduce, the data from all the from vertices will be forwarded to the group of partitioned buffers based on the partitioning key. A Source vertex does not have any owned buffers. But a pipeline may have multiple Source vertices, followed by one vertex. Same as above, if the following vertex is a map, the data from all the Source vertices will be forwarded to the group of partitioned buffers round-robinly. If it is a reduce, the data from all the Source vertices will be forwarded to the group of partitioned buffers based on the partitioning key.","title":"Buffers"},{"location":"specifications/edges-buffers-buckets/#buckets","text":"Bucket is a K/V store (or a pair of stores) used for watermark propagation. There are 3 types of buckets in a pipeline: Edge Bucket : Each edge has a bucket, used for edge watermark propagation, no matter if the vertex that the edge leads to is a Map or a Reduce. The naming convention of an edge bucket is {pipeline-name}-{from-vertex-name}-{to-vertex-name} . Source Bucket : Each Source vertex has a source bucket, used for source watermark propagation. The naming convention of a source bucket is {pipeline-name}-{vertex-name}-SOURCE . Sink Bucket : Sitting on the right side of a Sink vertex, used for sink watermark. The naming convention of a sink bucket is {pipeline-name}-{vertex-name}-SINK .","title":"Buckets"},{"location":"specifications/edges-buffers-buckets/#diagrams","text":"Map Reduce","title":"Diagrams"},{"location":"specifications/overview/","text":"Numaflow Dataplane High-Level Architecture \u00b6 Synopsis \u00b6 Numaflow allows developers without any special knowledge of data/stream processing to easily create massively parallel data/stream processing jobs using a programming language of their choice, with just basic knowledge of Kubernetes. Reliable data processing is highly desirable and exactly-once semantics is often required by many data processing applications. This document describes the use cases, requirements, and design for providing exactly-once semantics with Numaflow. Use Cases Continuous stream processing for unbounded streams. Efficient batch processing for bounded streams and data sets. Definitions \u00b6 Pipeline A pipeline contains multiple processors, which include source processors, data processors, and sink processors. These processors are not connected directly, but through inter-step buffers . Source The actual source for the data (not a step in the Numaflow). Sink The actual sink for the data (not a step in the Numaflow). Inter-Step Buffers Inter-step buffers are used to connect processors and they should support the following Durability Support offsets Support transactions for Exactly-Once forwarding Concurrent operations (reader group) Ability to explicitly ack each data/offset Claim pending messages (read but never acknowledged) Ability to trim (buffer size controls) Fast (high throughput and low latency) Ability to query buffer information (observability) Source Processors Source processors are the initial processors that ingest data into the Numaflow. They sit in front of the first data processor, ingest the data from the data source, and forward to inter-step buffers. Logic: Read data from the data source; Write to the inter-step buffer; Ack the data in the data source. Data Processors The data processors execute idempotent user-defined functions and will be sandwiched between source and sink processors. There could be one or more data processors. A data processor only reads from one upstream buffer, but it might write to multiple downstream buffers. Logic: Read data from the upstream inter-step buffer; Process data; Write to downstream inter-step buffers; Ack the data in the upstream buffer. Sink Processors Sink processors are the final processors used to write processed data to sinks. A sink processor only reads from one upstream buffer and writes to a single sink. Logic: Read data from the upstream inter-step buffer; Write to the sink; Ack the data in the upstream buffer. UDF (User-defined Function) Use-defined Functions run in data processors. UDFs implements a unified interface to process data. UDFs are typically implemented by end-users, but there will be some built-in functions that can be used without writing any code. UDFs can be implemented in different languages, a pseudo-interface might look like the below, where the function signatures include step context and input payload and returns a result. The Result contains the processed data as well as optional labels that will be exposed to the DSL to do complex conditional forwarding. Process(key, message, context) (result, err) UDFs should only focus on user logic, buffer message reading and writing should not be handled by this function. UDFs should be idempotent. Matrix of Operations Source Processor Sink ReadFromBuffer Read From Source Generic Generic CallUDF Void User Defined Void Forward Generic Generic Write To Sink Ack Ack Source Generic Generic Requirements \u00b6 Exactly once semantics from the source processor to the sink processor. Be able to support a variety of data buffering technologies. Numaflow is restartable if aborted or steps fail while preserving exactly-once semantics. Do not generate more output than can be used by the next stage in a reasonable amount of time, i.e., the size of buffers between steps should be limited, (aka backpressure). User code should be isolated from offset management, restart, exactly once, backpressure, etc. Streaming process systems inherently require a concept of time, this time will be either derived from the Source (LOG_APPEND_TIME in Kafka, etc.) or will be inserted at ingestion time if the source doesn't provide it. Every processor is connected by an inter-step buffer. Source processors add a \"header\" to each \"item\" received from the source in order to: Uniquely identify the item for implementing exactly-once Uniquely identify the source of the message. Sink processors should avoid writing output for the same input when possible. Numaflow should support the following types of flows: Line Tree Diamond (In Future) Multiple Sources with the same schema (In Future) Non-Requirements \u00b6 Support for non-idempotent data processors (UDFs?) Distributed transactions/checkpoints are not needed Open Issues \u00b6 None Closed Issues \u00b6 In order to be able to support various buffering technologies, we will persist and manage stream \"offsets\" rather than relying on the buffering technology (e.g., Kafka) Each processor may persist state associated with their processing no distributed transactions are needed for checkpointing If we have a tree DAG, how will we manage acknowledgments? We will use back-pressure and exactly-once schematics on the buffer to solve it. How/where will offsets be persisted? Buffer will have a \"lookup - insert - update\" as a txn What will be used to implement the inter-step buffers between processors? The interface is abstracted out, but internally we will use Redis Streams (supports streams, hash, txn) Design Details \u00b6 Duplicates \u00b6 Numaflow (like any other stream processing engine) at its core has Read -> Process -> Forward -> Acknowledge loop for every message it has to process. Given that the user-defined process is idempotent, there are two failure mode scenarios where there could be duplicates. The message has been forwarded but the information failed to reach back (we do not know whether we really have successfully forwarded the message). A retry on forwarding again could lead to duplication. Acknowledgment has been sent back to the source buffer, but we do not know whether we have really acknowledged the successful processing of the message. A retry on reading could end up in duplications (both in processing and forwarding, but we need to worry only about forwarding because processing is idempotent). To detect duplicates, make sure the delivery is Exactly-Once: A unique and immutable identifier for the message from the upstream buffer will be used as the key of the data in the downstream buffer Best effort of the transactional commit. Data processors make transactional commits for data forwarding to the next buffer, and upstream buffer acknowledgment. Source processors have no way to do similar transactional operations for data source message acknowledgment and message forwarding, but #1 will make sure there's no duplicate after retrying in case of failure. Sink processors can not do transactional operations unless there's a contract between Numaflow and the sink, which is out of the scope of this doc. We will rely on the sink to implement this (eg, \"enable.idempotent\" in Kafka producer). Unique Identifier for Message \u00b6 To detect duplicates, we first need to uniquely identify each message. We will be relying on the \"identifier\" available (e.g., \"offset\" in Kafka) in the buffer to uniquely identify each message. If such an identifier is not available, we will be creating a unique identifier (sequence numbers are tough because there are multiple readers). We can use this unique identifier to ensure that we forward only if the message has not been forwarded yet. We will only look back for a fixed window of time since this is a stream processing application on an unbounded stream of data, and we do not have infinite resources. The same offset will not be used across all the steps in Numaflow, but we will be using the current offset only while forwarding to the next step. Step N will use step N-1th offset to deduplicate. This requires each step to generate an unique ID. The reason we are not sticking to the original offset is because there will be operations in future which will require, say aggregations, where multiple messages will be grouped together and we will not be able to choose an offset from the original messages because the single output is based on multiple messages. Restarting After a Failure \u00b6 Numaflow needs to be able to recover from the failure of any step (pods) or even the complete failure of the Numaflow while preserving exactly-once semantics. When a message is successfully processed by a processor, it should have been written to the downstream buffer, and its status in the upstream buffer becomes \"Acknowledged\". So when a processor restarts, it checks if any message assigned to it in the upstream buffer is in the \"In-Flight\" state, if yes, it will read and process those messages before picking up other messages. Processing those messages follows the flowchart above, which makes sure they will only be processed once. Back Pressure \u00b6 The durable buffers allocated to the processors are not infinite but have a bounded buffer. Backpressure handling in Numaflow utilizes the buffer. At any time t, the durable buffer should contain messages in the following states: Acked messages - processed messages to be deleted Inflight messages - messages being handled by downstream processor Pending messages - messages to be read by the downstream processor The buffer acts like a sliding window, new messages will always be written to the right, and there's some automation to clean up the acknowledged messages on the left. If the processor is too slow, the pending messages will buffer up, and the space available for writing will become limited. Every time (or periodically for better throughput) before the upstream processor writes a message to the buffer, it checks if there's any available space, or else it stops writing (or slows down the processing while approaching the buffer limit). This buffer pressure will then pass back to the beginning of the pipeline, which is the buffer used by the source processor so that the entire flow will stop (or slow down).","title":"Overview"},{"location":"specifications/overview/#numaflow-dataplane-high-level-architecture","text":"","title":"Numaflow Dataplane High-Level Architecture"},{"location":"specifications/overview/#synopsis","text":"Numaflow allows developers without any special knowledge of data/stream processing to easily create massively parallel data/stream processing jobs using a programming language of their choice, with just basic knowledge of Kubernetes. Reliable data processing is highly desirable and exactly-once semantics is often required by many data processing applications. This document describes the use cases, requirements, and design for providing exactly-once semantics with Numaflow. Use Cases Continuous stream processing for unbounded streams. Efficient batch processing for bounded streams and data sets.","title":"Synopsis"},{"location":"specifications/overview/#definitions","text":"Pipeline A pipeline contains multiple processors, which include source processors, data processors, and sink processors. These processors are not connected directly, but through inter-step buffers . Source The actual source for the data (not a step in the Numaflow). Sink The actual sink for the data (not a step in the Numaflow). Inter-Step Buffers Inter-step buffers are used to connect processors and they should support the following Durability Support offsets Support transactions for Exactly-Once forwarding Concurrent operations (reader group) Ability to explicitly ack each data/offset Claim pending messages (read but never acknowledged) Ability to trim (buffer size controls) Fast (high throughput and low latency) Ability to query buffer information (observability) Source Processors Source processors are the initial processors that ingest data into the Numaflow. They sit in front of the first data processor, ingest the data from the data source, and forward to inter-step buffers. Logic: Read data from the data source; Write to the inter-step buffer; Ack the data in the data source. Data Processors The data processors execute idempotent user-defined functions and will be sandwiched between source and sink processors. There could be one or more data processors. A data processor only reads from one upstream buffer, but it might write to multiple downstream buffers. Logic: Read data from the upstream inter-step buffer; Process data; Write to downstream inter-step buffers; Ack the data in the upstream buffer. Sink Processors Sink processors are the final processors used to write processed data to sinks. A sink processor only reads from one upstream buffer and writes to a single sink. Logic: Read data from the upstream inter-step buffer; Write to the sink; Ack the data in the upstream buffer. UDF (User-defined Function) Use-defined Functions run in data processors. UDFs implements a unified interface to process data. UDFs are typically implemented by end-users, but there will be some built-in functions that can be used without writing any code. UDFs can be implemented in different languages, a pseudo-interface might look like the below, where the function signatures include step context and input payload and returns a result. The Result contains the processed data as well as optional labels that will be exposed to the DSL to do complex conditional forwarding. Process(key, message, context) (result, err) UDFs should only focus on user logic, buffer message reading and writing should not be handled by this function. UDFs should be idempotent. Matrix of Operations Source Processor Sink ReadFromBuffer Read From Source Generic Generic CallUDF Void User Defined Void Forward Generic Generic Write To Sink Ack Ack Source Generic Generic","title":"Definitions"},{"location":"specifications/overview/#requirements","text":"Exactly once semantics from the source processor to the sink processor. Be able to support a variety of data buffering technologies. Numaflow is restartable if aborted or steps fail while preserving exactly-once semantics. Do not generate more output than can be used by the next stage in a reasonable amount of time, i.e., the size of buffers between steps should be limited, (aka backpressure). User code should be isolated from offset management, restart, exactly once, backpressure, etc. Streaming process systems inherently require a concept of time, this time will be either derived from the Source (LOG_APPEND_TIME in Kafka, etc.) or will be inserted at ingestion time if the source doesn't provide it. Every processor is connected by an inter-step buffer. Source processors add a \"header\" to each \"item\" received from the source in order to: Uniquely identify the item for implementing exactly-once Uniquely identify the source of the message. Sink processors should avoid writing output for the same input when possible. Numaflow should support the following types of flows: Line Tree Diamond (In Future) Multiple Sources with the same schema (In Future)","title":"Requirements"},{"location":"specifications/overview/#non-requirements","text":"Support for non-idempotent data processors (UDFs?) Distributed transactions/checkpoints are not needed","title":"Non-Requirements"},{"location":"specifications/overview/#open-issues","text":"None","title":"Open Issues"},{"location":"specifications/overview/#closed-issues","text":"In order to be able to support various buffering technologies, we will persist and manage stream \"offsets\" rather than relying on the buffering technology (e.g., Kafka) Each processor may persist state associated with their processing no distributed transactions are needed for checkpointing If we have a tree DAG, how will we manage acknowledgments? We will use back-pressure and exactly-once schematics on the buffer to solve it. How/where will offsets be persisted? Buffer will have a \"lookup - insert - update\" as a txn What will be used to implement the inter-step buffers between processors? The interface is abstracted out, but internally we will use Redis Streams (supports streams, hash, txn)","title":"Closed Issues"},{"location":"specifications/overview/#design-details","text":"","title":"Design Details"},{"location":"specifications/overview/#duplicates","text":"Numaflow (like any other stream processing engine) at its core has Read -> Process -> Forward -> Acknowledge loop for every message it has to process. Given that the user-defined process is idempotent, there are two failure mode scenarios where there could be duplicates. The message has been forwarded but the information failed to reach back (we do not know whether we really have successfully forwarded the message). A retry on forwarding again could lead to duplication. Acknowledgment has been sent back to the source buffer, but we do not know whether we have really acknowledged the successful processing of the message. A retry on reading could end up in duplications (both in processing and forwarding, but we need to worry only about forwarding because processing is idempotent). To detect duplicates, make sure the delivery is Exactly-Once: A unique and immutable identifier for the message from the upstream buffer will be used as the key of the data in the downstream buffer Best effort of the transactional commit. Data processors make transactional commits for data forwarding to the next buffer, and upstream buffer acknowledgment. Source processors have no way to do similar transactional operations for data source message acknowledgment and message forwarding, but #1 will make sure there's no duplicate after retrying in case of failure. Sink processors can not do transactional operations unless there's a contract between Numaflow and the sink, which is out of the scope of this doc. We will rely on the sink to implement this (eg, \"enable.idempotent\" in Kafka producer).","title":"Duplicates"},{"location":"specifications/overview/#unique-identifier-for-message","text":"To detect duplicates, we first need to uniquely identify each message. We will be relying on the \"identifier\" available (e.g., \"offset\" in Kafka) in the buffer to uniquely identify each message. If such an identifier is not available, we will be creating a unique identifier (sequence numbers are tough because there are multiple readers). We can use this unique identifier to ensure that we forward only if the message has not been forwarded yet. We will only look back for a fixed window of time since this is a stream processing application on an unbounded stream of data, and we do not have infinite resources. The same offset will not be used across all the steps in Numaflow, but we will be using the current offset only while forwarding to the next step. Step N will use step N-1th offset to deduplicate. This requires each step to generate an unique ID. The reason we are not sticking to the original offset is because there will be operations in future which will require, say aggregations, where multiple messages will be grouped together and we will not be able to choose an offset from the original messages because the single output is based on multiple messages.","title":"Unique Identifier for Message"},{"location":"specifications/overview/#restarting-after-a-failure","text":"Numaflow needs to be able to recover from the failure of any step (pods) or even the complete failure of the Numaflow while preserving exactly-once semantics. When a message is successfully processed by a processor, it should have been written to the downstream buffer, and its status in the upstream buffer becomes \"Acknowledged\". So when a processor restarts, it checks if any message assigned to it in the upstream buffer is in the \"In-Flight\" state, if yes, it will read and process those messages before picking up other messages. Processing those messages follows the flowchart above, which makes sure they will only be processed once.","title":"Restarting After a Failure"},{"location":"specifications/overview/#back-pressure","text":"The durable buffers allocated to the processors are not infinite but have a bounded buffer. Backpressure handling in Numaflow utilizes the buffer. At any time t, the durable buffer should contain messages in the following states: Acked messages - processed messages to be deleted Inflight messages - messages being handled by downstream processor Pending messages - messages to be read by the downstream processor The buffer acts like a sliding window, new messages will always be written to the right, and there's some automation to clean up the acknowledged messages on the left. If the processor is too slow, the pending messages will buffer up, and the space available for writing will become limited. Every time (or periodically for better throughput) before the upstream processor writes a message to the buffer, it checks if there's any available space, or else it stops writing (or slows down the processing while approaching the buffer limit). This buffer pressure will then pass back to the beginning of the pipeline, which is the buffer used by the source processor so that the entire flow will stop (or slow down).","title":"Back Pressure"},{"location":"specifications/side-inputs/","text":"Side Inputs \u00b6 Side Inputs allow the user-defined functions (including UDF, UDSink, Transformer, etc.) to access slow updated data or configuration (such as database, file system, etc.) without needing to load it during each message processing. Side Inputs are read-only and can be used in both batch and streaming jobs. Requirements \u00b6 The Side Inputs should be programmable with any language. The Side Inputs should be updated centralized (for a pipeline), and be able to broadcast to each of the vertex pods in an efficient manner. The Side Inputs update could be based on a configurable interval. Assumptions \u00b6 Size of a Side Input data could be up to 1MB. The Side Inputs data is updated at a low frequency (minutes level). As a platform, Numaflow has no idea about the data format of the Side Inputs, instead, the pipeline owner (programmer) is responsible for parsing the data. Design Proposal \u00b6 Data Format \u00b6 Numaflow processes the Side Inputs data as bytes array, thus there\u2019s no data format requirement for it, the pipeline developers are supposed to parse the Side Inputs data from bytes array to any format they expect. Architecture \u00b6 There will be the following components introduced when a pipeline has Side Inputs enabled. A Side Inputs Manager - a service for Side Inputs data updating. A Side Inputs watcher sidecar - a container enabled for each of the vertex pods to receive updated Side Inputs. Side Inputs data store - a data store to store the latest Side Inputs data. Data Store \u00b6 Data store is the place where the latest retrieved Side Inputs data stays. The data is published by the Side Inputs Manager after retrieving from the Side Inputs data source, and consumed by each of the vertex Pods. The data store implementation could be a Key/Value store in JetStream, which by default supports maximum 1MB - 64MB size data. Extended implementation could be Key/Value store + object store, which makes it possible to store large sizes of data. Data Store management is supposed to be done by the controller, through the same Kubernetes Job to create/delete Inter-Step Buffers and Buckets. Side Inputs Manager \u00b6 A Side Inputs Manager is a pod (or a group of pods with active-passive HA), created by the Numaflow controller, used to run cron like jobs to retrieve the Side Inputs data and save to a data store. Each Side Inputs Manager is only responsible for corresponding pipeline, and is only created when Side Inputs is enabled for the pipeline. A pipeline may have multiple Side Inputs sources, each of them will have a Side Inputs Manger. Each of the Side Inputs Manager pods contains: An init container, which checks if the data store is ready. A user-defined container, which runs a predefined Numaflow SDK to start a service, calling a user implemented function to get Side Input data. A numa container, which runs a cron like job to call the service in the user-defined container, and store the returned data in the data store. The communication protocol between the 2 containers could be based on UDS or FIFO (TBD). High Availability \u00b6 Side Inputs Manager needs to run with Active-Passive HA, which requires a leader election mechanism support. Kubernetes has a native leader election API backed by etcd, but it requires extra RBAC privileges to use it. Considering a similar leader election mechanism is needed in some other scenarios such as Active-Passive User-defined Source, a proposal is to implement our own leader election mechanism by leveraging ISB Service. Why NOT CronJob? \u00b6 Using Kubernetes CronJob could also achieve the cron like job orchestration, but there are few downsides. A K8s Job has to be used together with the CronJob to solve the immediate starting problem - A CronJob can not trigger a job immediately after it\u2019s created, it has to wait until the first trigger condition meets. Using K8s CronJob/Job will be a challenge when ServiceMesh (Istio) is enabled. Vertex Pod Sidecar \u00b6 When Side Inputs is enabled for a pipeline, each of its vertex pods will have a second init container added, the init container will have a shared volume (emptyDir) mounted, and the same volume will be mounted to the User-defined Function/Source/Sink/Transformer container. The init container reads from the data store, and saves to the shared volume. A sidecar container will also be injected by the controller, and it mounts the same volume as above. The sidecar runs a service provided by numaflow, watching the Side Inputs data from the data store, if there\u2019s any update, reads the data and updates the shared volume. In the User-defined Function/Source/Sink/Transformer container, a helper function will be provided by Numaflow SDK, to return the Side Input data. The helper function caches the Side Inputs data in the memory, but performs thread safe updates if it watches the changes in the shared volume. Numaflow SDK \u00b6 Some new features will be added to the Numaflow SDK. Interface for the users to implement the Side Inputs retrievement. A pseudo interface might look like below. RetrieveSideInput () ([] bytes , error ) A main function to start the service in the Side Inputs Manager user container. A helper function to be used in the udf/udsink/transformer containers to get the Side Inputs, which reads, watches and caches the data from the shared volume. SideInput [ T any ]( name string , parseFunc func ([] byte ) ( T , error )) ( T , error ) User Spec \u00b6 Side Inputs support is exposed through sideInputs in the pipeline spec, it\u2019s updated based on cron like schedule, specified in the pipeline spec with a trigger field. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : sideInputs : - name : devPortal container : image : my-sideinputs-image:v1 trigger : schedule : \"*/15 * * * *\" # interval: 180s # timezone: America/Los_Angeles vertices : - name : my-vertex sideInputs : - devPortal Open Issues \u00b6 To support multiple ways to trigger Side Inputs updating other than cron only? Event based side inputs where the changes are coming via a stream?","title":"Side Inputs"},{"location":"specifications/side-inputs/#side-inputs","text":"Side Inputs allow the user-defined functions (including UDF, UDSink, Transformer, etc.) to access slow updated data or configuration (such as database, file system, etc.) without needing to load it during each message processing. Side Inputs are read-only and can be used in both batch and streaming jobs.","title":"Side Inputs"},{"location":"specifications/side-inputs/#requirements","text":"The Side Inputs should be programmable with any language. The Side Inputs should be updated centralized (for a pipeline), and be able to broadcast to each of the vertex pods in an efficient manner. The Side Inputs update could be based on a configurable interval.","title":"Requirements"},{"location":"specifications/side-inputs/#assumptions","text":"Size of a Side Input data could be up to 1MB. The Side Inputs data is updated at a low frequency (minutes level). As a platform, Numaflow has no idea about the data format of the Side Inputs, instead, the pipeline owner (programmer) is responsible for parsing the data.","title":"Assumptions"},{"location":"specifications/side-inputs/#design-proposal","text":"","title":"Design Proposal"},{"location":"specifications/side-inputs/#data-format","text":"Numaflow processes the Side Inputs data as bytes array, thus there\u2019s no data format requirement for it, the pipeline developers are supposed to parse the Side Inputs data from bytes array to any format they expect.","title":"Data Format"},{"location":"specifications/side-inputs/#architecture","text":"There will be the following components introduced when a pipeline has Side Inputs enabled. A Side Inputs Manager - a service for Side Inputs data updating. A Side Inputs watcher sidecar - a container enabled for each of the vertex pods to receive updated Side Inputs. Side Inputs data store - a data store to store the latest Side Inputs data.","title":"Architecture"},{"location":"specifications/side-inputs/#data-store","text":"Data store is the place where the latest retrieved Side Inputs data stays. The data is published by the Side Inputs Manager after retrieving from the Side Inputs data source, and consumed by each of the vertex Pods. The data store implementation could be a Key/Value store in JetStream, which by default supports maximum 1MB - 64MB size data. Extended implementation could be Key/Value store + object store, which makes it possible to store large sizes of data. Data Store management is supposed to be done by the controller, through the same Kubernetes Job to create/delete Inter-Step Buffers and Buckets.","title":"Data Store"},{"location":"specifications/side-inputs/#side-inputs-manager","text":"A Side Inputs Manager is a pod (or a group of pods with active-passive HA), created by the Numaflow controller, used to run cron like jobs to retrieve the Side Inputs data and save to a data store. Each Side Inputs Manager is only responsible for corresponding pipeline, and is only created when Side Inputs is enabled for the pipeline. A pipeline may have multiple Side Inputs sources, each of them will have a Side Inputs Manger. Each of the Side Inputs Manager pods contains: An init container, which checks if the data store is ready. A user-defined container, which runs a predefined Numaflow SDK to start a service, calling a user implemented function to get Side Input data. A numa container, which runs a cron like job to call the service in the user-defined container, and store the returned data in the data store. The communication protocol between the 2 containers could be based on UDS or FIFO (TBD).","title":"Side Inputs Manager"},{"location":"specifications/side-inputs/#high-availability","text":"Side Inputs Manager needs to run with Active-Passive HA, which requires a leader election mechanism support. Kubernetes has a native leader election API backed by etcd, but it requires extra RBAC privileges to use it. Considering a similar leader election mechanism is needed in some other scenarios such as Active-Passive User-defined Source, a proposal is to implement our own leader election mechanism by leveraging ISB Service.","title":"High Availability"},{"location":"specifications/side-inputs/#why-not-cronjob","text":"Using Kubernetes CronJob could also achieve the cron like job orchestration, but there are few downsides. A K8s Job has to be used together with the CronJob to solve the immediate starting problem - A CronJob can not trigger a job immediately after it\u2019s created, it has to wait until the first trigger condition meets. Using K8s CronJob/Job will be a challenge when ServiceMesh (Istio) is enabled.","title":"Why NOT CronJob?"},{"location":"specifications/side-inputs/#vertex-pod-sidecar","text":"When Side Inputs is enabled for a pipeline, each of its vertex pods will have a second init container added, the init container will have a shared volume (emptyDir) mounted, and the same volume will be mounted to the User-defined Function/Source/Sink/Transformer container. The init container reads from the data store, and saves to the shared volume. A sidecar container will also be injected by the controller, and it mounts the same volume as above. The sidecar runs a service provided by numaflow, watching the Side Inputs data from the data store, if there\u2019s any update, reads the data and updates the shared volume. In the User-defined Function/Source/Sink/Transformer container, a helper function will be provided by Numaflow SDK, to return the Side Input data. The helper function caches the Side Inputs data in the memory, but performs thread safe updates if it watches the changes in the shared volume.","title":"Vertex Pod Sidecar"},{"location":"specifications/side-inputs/#numaflow-sdk","text":"Some new features will be added to the Numaflow SDK. Interface for the users to implement the Side Inputs retrievement. A pseudo interface might look like below. RetrieveSideInput () ([] bytes , error ) A main function to start the service in the Side Inputs Manager user container. A helper function to be used in the udf/udsink/transformer containers to get the Side Inputs, which reads, watches and caches the data from the shared volume. SideInput [ T any ]( name string , parseFunc func ([] byte ) ( T , error )) ( T , error )","title":"Numaflow SDK"},{"location":"specifications/side-inputs/#user-spec","text":"Side Inputs support is exposed through sideInputs in the pipeline spec, it\u2019s updated based on cron like schedule, specified in the pipeline spec with a trigger field. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : sideInputs : - name : devPortal container : image : my-sideinputs-image:v1 trigger : schedule : \"*/15 * * * *\" # interval: 180s # timezone: America/Los_Angeles vertices : - name : my-vertex sideInputs : - devPortal","title":"User Spec"},{"location":"specifications/side-inputs/#open-issues","text":"To support multiple ways to trigger Side Inputs updating other than cron only? Event based side inputs where the changes are coming via a stream?","title":"Open Issues"},{"location":"user-guide/FAQ/","text":"Frequently Asked Questions \u00b6 Welcome to the Numaflow FAQ! Here you'll find answers to common questions about getting started, compatibility, troubleshooting, and more. 1. How do I get started with Numaflow? \u00b6 To get started, follow the Quickstart Guide . It provides step-by-step instructions for setting up a basic Numaflow pipeline. 2. How can I check SDK compatibility with Numaflow versions? \u00b6 Refer to the compatibility matrix here to ensure your SDK version works with your Numaflow deployment. 3. Where can I learn about the latest releases of Numaflow? \u00b6 Visit the Numaflow Releases page for the latest updates. Join our Slack channel to stay up to date with announcements and community discussions. 4. I see Server Info File not ready log in the numa container. What does this mean? \u00b6 This message indicates that the server has not started yet. Common reasons include: The server was not started correctly in the udcontainer . The server is still initializing (for example, the application code is downloading cache files or performing setup tasks). 5. What is partitions in Numaflow? \u00b6 The field partitions is the number of streams between the vertices. Larger the parition number, higher the TPS that vertex can take. Note that partitions are owned by the vertex reading the data, to create a multi-partitioed edge we need to configure the vertex reading the data to have multiple partitions. If you have additional questions, please refer to the documentation or reach out on Slack !","title":"FAQs"},{"location":"user-guide/FAQ/#frequently-asked-questions","text":"Welcome to the Numaflow FAQ! Here you'll find answers to common questions about getting started, compatibility, troubleshooting, and more.","title":"Frequently Asked Questions"},{"location":"user-guide/FAQ/#1-how-do-i-get-started-with-numaflow","text":"To get started, follow the Quickstart Guide . It provides step-by-step instructions for setting up a basic Numaflow pipeline.","title":"1. How do I get started with Numaflow?"},{"location":"user-guide/FAQ/#2-how-can-i-check-sdk-compatibility-with-numaflow-versions","text":"Refer to the compatibility matrix here to ensure your SDK version works with your Numaflow deployment.","title":"2. How can I check SDK compatibility with Numaflow versions?"},{"location":"user-guide/FAQ/#3-where-can-i-learn-about-the-latest-releases-of-numaflow","text":"Visit the Numaflow Releases page for the latest updates. Join our Slack channel to stay up to date with announcements and community discussions.","title":"3. Where can I learn about the latest releases of Numaflow?"},{"location":"user-guide/FAQ/#4-i-see-server-info-file-not-ready-log-in-the-numa-container-what-does-this-mean","text":"This message indicates that the server has not started yet. Common reasons include: The server was not started correctly in the udcontainer . The server is still initializing (for example, the application code is downloading cache files or performing setup tasks).","title":"4. I see Server Info File not ready log in the numa container. What does this mean?"},{"location":"user-guide/FAQ/#5-what-is-partitions-in-numaflow","text":"The field partitions is the number of streams between the vertices. Larger the parition number, higher the TPS that vertex can take. Note that partitions are owned by the vertex reading the data, to create a multi-partitioed edge we need to configure the vertex reading the data to have multiple partitions. If you have additional questions, please refer to the documentation or reach out on Slack !","title":"5. What is partitions in Numaflow?"},{"location":"user-guide/UI/errors/","text":"Errors \u00b6 The Numaflow UI provides an Errors Tab that lists the recent errors (up to 10 per container) that may have occurred in the user-defined function (UDF) code. This feature helps users quickly identify and debug application errors. In this guide, we will simulate a panic in a transformer and demonstrate how the errors are displayed in the Errors Tab for a monovertex. Note : The Errors Tab and its related functionality are currently available only for MonoVertex. Support for Pipelines is planned for future releases. High-Level Architecture \u00b6 Simulating an Error in the Transformer \u00b6 To simulate an error, we will introduce a panic in the Go SDK for the transformer by modifying the Transform function. You can find the relevant code here . Below is the modified code snippet: // AssignEventTime is a source transformer that assigns event time to the message. type AssignEventTime struct { counter uint64 } func ( a * AssignEventTime ) Transform ( ctx context . Context , keys [] string , d sourcetransformer . Datum ) sourcetransformer . Messages { newCount := atomic . AddUint64 ( & a . counter , 1 ) // Trigger a panic if the counter is a multiple of 5 if newCount % 5 == 0 { panic ( \"Counter reached a multiple of 5\" ) } // Update the message's event time to the current time eventTime := time . Now () // Log the action for testing purposes log . Printf ( \"AssignEventTime: Assigning event time %v to message %s\" , eventTime , string ( d . Value ())) return sourcetransformer . MessagesBuilder (). Append ( sourcetransformer . NewMessage ( d . Value (), eventTime ). WithKeys ( keys )) } In this example, the code triggers a panic whenever the counter reaches a multiple of 5. This simulates an error scenario that will be captured and displayed in the Errors Tab. The UI Errors Tab \u00b6 The Errors Tab provides the following features: Error Count : Displays the total number of errors across all pods and containers at the top level. Pod and Container Filters : Allows users to filter errors by selecting a specific pod or container for more focused debugging. Tabulated Errors : Errors are displayed in a tabular format with the following columns: Pod Name Container Message Last Occurred Details : Expanding an error entry reveals a Details section that includes the stack trace of the error, providing deeper insights for debugging. Types of Errors Displayed in the Errors Tab \u00b6 The Errors Tab in the Numaflow UI captures and displays various types of errors that may occur during the execution of user-defined functions (UDFs). Below are the common categories of errors you can expect: 1. Exceptions in User Code \u00b6 Any exception that occurs within the user-defined function (UDF) code will be captured and displayed in the Errors Tab. These errors typically indicate issues in the logic or implementation of the UDF. 2. Partial Responses \u00b6 Errors may occur when the user code sends partial or incorrect responses. For example: Returning null instead of Message.toDrop in a UDSink when a message needs to be dropped. 3. Critical Errors Persisted by the User \u00b6 Users can invoke the PersistCriticalError utility function to log critical errors in the emptyDir volume. This is useful for capturing severe issues that require immediate attention. Below are links to the implementations in different SDKs: Go Implementation Java Implementation Python Implementation","title":"Errors"},{"location":"user-guide/UI/errors/#errors","text":"The Numaflow UI provides an Errors Tab that lists the recent errors (up to 10 per container) that may have occurred in the user-defined function (UDF) code. This feature helps users quickly identify and debug application errors. In this guide, we will simulate a panic in a transformer and demonstrate how the errors are displayed in the Errors Tab for a monovertex. Note : The Errors Tab and its related functionality are currently available only for MonoVertex. Support for Pipelines is planned for future releases.","title":"Errors"},{"location":"user-guide/UI/errors/#high-level-architecture","text":"","title":"High-Level Architecture"},{"location":"user-guide/UI/errors/#simulating-an-error-in-the-transformer","text":"To simulate an error, we will introduce a panic in the Go SDK for the transformer by modifying the Transform function. You can find the relevant code here . Below is the modified code snippet: // AssignEventTime is a source transformer that assigns event time to the message. type AssignEventTime struct { counter uint64 } func ( a * AssignEventTime ) Transform ( ctx context . Context , keys [] string , d sourcetransformer . Datum ) sourcetransformer . Messages { newCount := atomic . AddUint64 ( & a . counter , 1 ) // Trigger a panic if the counter is a multiple of 5 if newCount % 5 == 0 { panic ( \"Counter reached a multiple of 5\" ) } // Update the message's event time to the current time eventTime := time . Now () // Log the action for testing purposes log . Printf ( \"AssignEventTime: Assigning event time %v to message %s\" , eventTime , string ( d . Value ())) return sourcetransformer . MessagesBuilder (). Append ( sourcetransformer . NewMessage ( d . Value (), eventTime ). WithKeys ( keys )) } In this example, the code triggers a panic whenever the counter reaches a multiple of 5. This simulates an error scenario that will be captured and displayed in the Errors Tab.","title":"Simulating an Error in the Transformer"},{"location":"user-guide/UI/errors/#the-ui-errors-tab","text":"The Errors Tab provides the following features: Error Count : Displays the total number of errors across all pods and containers at the top level. Pod and Container Filters : Allows users to filter errors by selecting a specific pod or container for more focused debugging. Tabulated Errors : Errors are displayed in a tabular format with the following columns: Pod Name Container Message Last Occurred Details : Expanding an error entry reveals a Details section that includes the stack trace of the error, providing deeper insights for debugging.","title":"The UI Errors Tab"},{"location":"user-guide/UI/errors/#types-of-errors-displayed-in-the-errors-tab","text":"The Errors Tab in the Numaflow UI captures and displays various types of errors that may occur during the execution of user-defined functions (UDFs). Below are the common categories of errors you can expect:","title":"Types of Errors Displayed in the Errors Tab"},{"location":"user-guide/UI/errors/#1-exceptions-in-user-code","text":"Any exception that occurs within the user-defined function (UDF) code will be captured and displayed in the Errors Tab. These errors typically indicate issues in the logic or implementation of the UDF.","title":"1. Exceptions in User Code"},{"location":"user-guide/UI/errors/#2-partial-responses","text":"Errors may occur when the user code sends partial or incorrect responses. For example: Returning null instead of Message.toDrop in a UDSink when a message needs to be dropped.","title":"2. Partial Responses"},{"location":"user-guide/UI/errors/#3-critical-errors-persisted-by-the-user","text":"Users can invoke the PersistCriticalError utility function to log critical errors in the emptyDir volume. This is useful for capturing severe issues that require immediate attention. Below are links to the implementations in different SDKs: Go Implementation Java Implementation Python Implementation","title":"3. Critical Errors Persisted by the User"},{"location":"user-guide/UI/logs/","text":"Logs View \u00b6 The Logs View allows users to inspect the logs of a specific container within a pod of a vertex. This guide will walk you through the Logs tab and its various features. Navigating to the Logs Tab \u00b6 Select a Pod Navigate to the Pods View tab after selecting the vertex and select a pod by name from the Select a Pod by Name dropdown. Select a Container Choose a container from the Select a Container section. View Logs Open the Logs Tab on the right to view the container logs. Features \u00b6 1. Previous Terminated Container Logs \u00b6 Enable the checkbox to view logs from previously terminated containers. This is particularly useful for debugging issues. 2. Search Logs \u00b6 Filter logs by typing keywords in the Search Logs box. 3. Negate Search \u00b6 Enable the Negate Search option to exclude logs matching the search keywords from the view. 4. Wrap Lines \u00b6 Use the Wrap Lines feature to avoid horizontal scrolling for long log lines, improving readability. 5. Pause Logs \u00b6 Pause the log stream to inspect logs when there is a high volume of data. 6. Dark Mode \u00b6 Toggle between Dark Mode and Light Mode for better visibility based on your preference. 7. Ascending/Descending Order \u00b6 Switch between ascending and descending order of log timestamps for easier navigation. 8. Download Logs \u00b6 Download the last 1000 logs for offline analysis. 9. Add/Remove Timestamps \u00b6 Toggle timestamps in the logs based on your requirements. 10. Level-Based Filtering \u00b6 Filter logs by log levels such as: Info Error Warn Debug","title":"Logs"},{"location":"user-guide/UI/logs/#logs-view","text":"The Logs View allows users to inspect the logs of a specific container within a pod of a vertex. This guide will walk you through the Logs tab and its various features.","title":"Logs View"},{"location":"user-guide/UI/logs/#navigating-to-the-logs-tab","text":"Select a Pod Navigate to the Pods View tab after selecting the vertex and select a pod by name from the Select a Pod by Name dropdown. Select a Container Choose a container from the Select a Container section. View Logs Open the Logs Tab on the right to view the container logs.","title":"Navigating to the Logs Tab"},{"location":"user-guide/UI/logs/#features","text":"","title":"Features"},{"location":"user-guide/UI/logs/#1-previous-terminated-container-logs","text":"Enable the checkbox to view logs from previously terminated containers. This is particularly useful for debugging issues.","title":"1. Previous Terminated Container Logs"},{"location":"user-guide/UI/logs/#2-search-logs","text":"Filter logs by typing keywords in the Search Logs box.","title":"2. Search Logs"},{"location":"user-guide/UI/logs/#3-negate-search","text":"Enable the Negate Search option to exclude logs matching the search keywords from the view.","title":"3. Negate Search"},{"location":"user-guide/UI/logs/#4-wrap-lines","text":"Use the Wrap Lines feature to avoid horizontal scrolling for long log lines, improving readability.","title":"4. Wrap Lines"},{"location":"user-guide/UI/logs/#5-pause-logs","text":"Pause the log stream to inspect logs when there is a high volume of data.","title":"5. Pause Logs"},{"location":"user-guide/UI/logs/#6-dark-mode","text":"Toggle between Dark Mode and Light Mode for better visibility based on your preference.","title":"6. Dark Mode"},{"location":"user-guide/UI/logs/#7-ascendingdescending-order","text":"Switch between ascending and descending order of log timestamps for easier navigation.","title":"7. Ascending/Descending Order"},{"location":"user-guide/UI/logs/#8-download-logs","text":"Download the last 1000 logs for offline analysis.","title":"8. Download Logs"},{"location":"user-guide/UI/logs/#9-addremove-timestamps","text":"Toggle timestamps in the logs based on your requirements.","title":"9. Add/Remove Timestamps"},{"location":"user-guide/UI/logs/#10-level-based-filtering","text":"Filter logs by log levels such as: Info Error Warn Debug","title":"10. Level-Based Filtering"},{"location":"user-guide/UI/metrics-tab/","text":"Metrics Tab \u00b6 Numaflow provides a comprehensive set of Prometheus metrics , enabling users to monitor their pipelines effectively and set up alerts as needed. This feature enhances the debugging experience by offering contextual insights directly within the UI. By leveraging these metrics, users can identify and resolve issues more efficiently, ensuring smoother pipeline operations. Prerequisites \u00b6 Before visualizing metrics in the UI, ensure the following requirements are met: Prometheus Server Setup : A Prometheus server must be configured and running. Refer to the Prometheus Operator Installation Guide for detailed instructions. Service Monitors : Configure Service/Pod Monitors for scraping pipeline/monovertex metrics. Refer to this section for configuration details. Basic Knowledge of PromQL : Familiarity with PromQL (Prometheus Query Language) is recommended to effectively query and interpret the metrics. Meeting these prerequisites will help you make the most of the metrics visualization and monitoring features in Numaflow. Metrics Configuration \u00b6 To visualize metrics in the UI, you need to configure them using a ConfigMap. Numaflow provides a default ConfigMap that includes a variety of pre-configured metrics. These metrics cover essential pipeline operations and also include custom metrics, such as CPU and memory usage. You can customize this ConfigMap to add or modify metrics based on your specific requirements. For example, you can include additional Prometheus metrics relevant to your use case. Refer to the Numaflow Metrics Documentation for more details on available metrics and their usage. Current Configuration Snippet \u00b6 Below is a snippet of the default ConfigMap configuration: apiVersion : v1 kind : ConfigMap metadata : name : numaflow-server-metrics-proxy-config data : config.yaml : | url: http://prometheus-operated.monitoring.svc.cluster.local:9090 patterns: - name: vertex_gauge objects: - vertex title: Vertex Gauge Metrics description: This pattern represents the gauge metrics for a vertex across different dimensions. expr: | sum($metric_name{$filters}) by ($dimension, period) params: - name: start_time required: false - name: end_time required: false metrics: - metric_name: vertex_pending_messages display_name: Vertex Pending Messages metric_description: Tracks the total number of messages waiting to be processed over varying time frames (e.g., 1min, 5min, 15min). required_filters: - namespace - pipeline - vertex dimensions: - name: pod filters: - name: pod required: false - name: period required: false - name: vertex filters: - name: period required: false Key Definitions \u00b6 Prometheus metrics are categorized into three main types: Histogram, Gauge, and Counter. The configuration provided above groups metrics of a similar type under a single pattern, making it more generic and easier to manage. url: The url is a required field that specifies the Prometheus service endpoint to which the metrics proxy will connect. If the url is not set or is incorrectly configured, the Metrics Tab will not be displayed in the UI. Example : If you set up a local Prometheus operator using Helm , the url would look like this: http://my-release-kube-prometheus-prometheus.default.svc.cluster.local:9090 patterns: A list of patterns that group metrics of a similar type. name : The name of the pattern. objects : Specifies the object type, which can either be vertex or mono-vertex . For pipelines, the object is vertex , and for MonoVertex, the object is mono-vertex . title : The title of the pattern. description : A description of the pattern. expr : The PromQL expression used to construct queries, with placeholders for dynamic values. params : Common parameters for all metrics within the pattern. These may include: start_time : The start time for the PromQL query (optional). end_time : The end time for the PromQL query (optional). duration : The query window (required for histograms). quantile : The quantile value (required for histograms). The quantiles can be 0.99, 0.95, 0.90, or 0.50. metrics : A list of metrics defined within the pattern. metric_name : The actual name of the metric. display_name : A user-friendly name for the metric. Avoid editing this for existing metrics. metric_description : A detailed description of the metric, displayed as an info icon next to the metric name in the UI. required_filters : Filters that must be included in the PromQL request body. dimensions : Dimensions allow users to drill down into specific components, such as pods or containers, for more granular data. name : The name of the dimension (e.g., pod, vertex). filters : Filters applied to the data for a specific key. name : The name of the filter. required : If set to true , the filter is automatically added to $filters . If false , users can select a value for the filter key, which is then added to $filters . expr : (Optional) Overrides the top-level expr for a specific metric and dimension. This structured configuration ensures flexibility and ease of use when visualizing and analyzing metrics in the Numaflow UI. Explanation with an Example \u00b6 The above configuration might seem complex at first glance. Let\u2019s break it down with an example using the out-of-the-box metric monovtx_processing_time_bucket . Example Configuration \u00b6 - name : mono_vertex_histogram objects : - mono-vertex title : MonoVertex Histogram Metrics description : This pattern is for P99, P95, P90, and P50 quantiles for a mono-vertex across different dimensions. expr : | histogram_quantile($quantile, sum by($dimension,le) (rate($metric_name{$filters}[$duration]))) params : - name : quantile required : true - name : duration required : true - name : start_time required : false - name : end_time required : false metrics : - metric_name : monovtx_processing_time_bucket display_name : MonoVertex Processing Time Latency metric_description : This metric represents a histogram to keep track of the total time taken to forward a chunk of messages. required_filters : - namespace - mvtx_name dimensions : - name : mono-vertex - name : pod filters : - name : pod required : false Key Points \u00b6 Pattern Name : mono_vertex_histogram This indicates that the pattern includes histogram metrics for a mono-vertex. Expression ( expr ) The expression calculates quantiles (e.g., P99, P95). Learn more about quantiles in Prometheus here . Placeholders such as quantile , dimension , metric_name , filters , and duration are dynamically populated based on the configuration. Parameters ( params ) quantile and duration are required placeholders in the expression. start_time and end_time are optional parameters for defining the query duration. Metric Name ( metric_name ) This forms the $metric_name placeholder in the expression. Required Filters ( required_filters ) These filters populate the $filters placeholder in the expression. Dimensions For the mono-vertex dimension, no additional filters are applied. For the pod dimension, an additional filter ( pod ) is appended to the $filters placeholder. Example Expressions \u00b6 Below are examples of how the placeholders are replaced to form the final PromQL expressions: Dimension : mono-vertex Quantile : 0.99 Namespace : default MonoVertex Name : simple-mono-vertex Duration : 5m Resultant Expression : histogram_quantile ( 0.99 , sum by ( mvtx_name , le ) ( rate ( monovtx_processing_time_bucket { namespace = \" default \", mvtx_name = \" simple-mono-vertex \"}[ 5m ] ))) Dimension : pod Quantile : 0.99 Namespace : default MonoVertex Name : simple-mono-vertex Pod : simple-mono-vertex-mv-0-edj2s Duration : 1m Resultant Expression : histogram_quantile ( 0.99 , sum by ( pod , le ) ( rate ( monovtx_processing_time_bucket { namespace = \" default \", mvtx_name = \" simple-mono-vertex \", pod = \" simple-mono-vertex-mv-0-edj2s \"}[ 1m ] ))) This example demonstrates how the configuration translates into actionable PromQL queries, making it easier to understand and customize. How the Metrics Tab Appears in the UI \u00b6 The Metrics Tab is located next to the Logs Tab in the Pods View . The following screenshots illustrate how the UI translates the discussed configuration into a visual representation: MonoVertex Histogram Example \u00b6 The UI displays the configured metrics, such as the monovtx_processing_time_bucket histogram, with options to select dimensions and filters. Dimension Selection \u00b6 Users can choose dimensions for the metric, such as MonoVertex or Pod , to view data at different levels of granularity. After selecting Pod dimension, we see that Filters box appears which allow us to add filters for a dimension. Here, pod dimension has pod as a filter. Quantile Selection \u00b6 For histogram metrics, users can select quantile values (e.g., P99, P95) from the available options, as explained in the Key Definitions section. Query Window/Duration \u00b6 The query window specifies the time range over which the rate() function calculates the per-second average rate of increase for each histogram bucket. Time Range Selection \u00b6 Users can select a predefined time range or set a custom time range for their PromQL queries. Adding Custom Metrics \u00b6 To add metrics not provided by Numaflow, follow the configuration patterns described above. For example, you can add a custom metric similar to the pod_cpu_memory_utilization pattern here .","title":"Metrics"},{"location":"user-guide/UI/metrics-tab/#metrics-tab","text":"Numaflow provides a comprehensive set of Prometheus metrics , enabling users to monitor their pipelines effectively and set up alerts as needed. This feature enhances the debugging experience by offering contextual insights directly within the UI. By leveraging these metrics, users can identify and resolve issues more efficiently, ensuring smoother pipeline operations.","title":"Metrics Tab"},{"location":"user-guide/UI/metrics-tab/#prerequisites","text":"Before visualizing metrics in the UI, ensure the following requirements are met: Prometheus Server Setup : A Prometheus server must be configured and running. Refer to the Prometheus Operator Installation Guide for detailed instructions. Service Monitors : Configure Service/Pod Monitors for scraping pipeline/monovertex metrics. Refer to this section for configuration details. Basic Knowledge of PromQL : Familiarity with PromQL (Prometheus Query Language) is recommended to effectively query and interpret the metrics. Meeting these prerequisites will help you make the most of the metrics visualization and monitoring features in Numaflow.","title":"Prerequisites"},{"location":"user-guide/UI/metrics-tab/#metrics-configuration","text":"To visualize metrics in the UI, you need to configure them using a ConfigMap. Numaflow provides a default ConfigMap that includes a variety of pre-configured metrics. These metrics cover essential pipeline operations and also include custom metrics, such as CPU and memory usage. You can customize this ConfigMap to add or modify metrics based on your specific requirements. For example, you can include additional Prometheus metrics relevant to your use case. Refer to the Numaflow Metrics Documentation for more details on available metrics and their usage.","title":"Metrics Configuration"},{"location":"user-guide/UI/metrics-tab/#current-configuration-snippet","text":"Below is a snippet of the default ConfigMap configuration: apiVersion : v1 kind : ConfigMap metadata : name : numaflow-server-metrics-proxy-config data : config.yaml : | url: http://prometheus-operated.monitoring.svc.cluster.local:9090 patterns: - name: vertex_gauge objects: - vertex title: Vertex Gauge Metrics description: This pattern represents the gauge metrics for a vertex across different dimensions. expr: | sum($metric_name{$filters}) by ($dimension, period) params: - name: start_time required: false - name: end_time required: false metrics: - metric_name: vertex_pending_messages display_name: Vertex Pending Messages metric_description: Tracks the total number of messages waiting to be processed over varying time frames (e.g., 1min, 5min, 15min). required_filters: - namespace - pipeline - vertex dimensions: - name: pod filters: - name: pod required: false - name: period required: false - name: vertex filters: - name: period required: false","title":"Current Configuration Snippet"},{"location":"user-guide/UI/metrics-tab/#key-definitions","text":"Prometheus metrics are categorized into three main types: Histogram, Gauge, and Counter. The configuration provided above groups metrics of a similar type under a single pattern, making it more generic and easier to manage. url: The url is a required field that specifies the Prometheus service endpoint to which the metrics proxy will connect. If the url is not set or is incorrectly configured, the Metrics Tab will not be displayed in the UI. Example : If you set up a local Prometheus operator using Helm , the url would look like this: http://my-release-kube-prometheus-prometheus.default.svc.cluster.local:9090 patterns: A list of patterns that group metrics of a similar type. name : The name of the pattern. objects : Specifies the object type, which can either be vertex or mono-vertex . For pipelines, the object is vertex , and for MonoVertex, the object is mono-vertex . title : The title of the pattern. description : A description of the pattern. expr : The PromQL expression used to construct queries, with placeholders for dynamic values. params : Common parameters for all metrics within the pattern. These may include: start_time : The start time for the PromQL query (optional). end_time : The end time for the PromQL query (optional). duration : The query window (required for histograms). quantile : The quantile value (required for histograms). The quantiles can be 0.99, 0.95, 0.90, or 0.50. metrics : A list of metrics defined within the pattern. metric_name : The actual name of the metric. display_name : A user-friendly name for the metric. Avoid editing this for existing metrics. metric_description : A detailed description of the metric, displayed as an info icon next to the metric name in the UI. required_filters : Filters that must be included in the PromQL request body. dimensions : Dimensions allow users to drill down into specific components, such as pods or containers, for more granular data. name : The name of the dimension (e.g., pod, vertex). filters : Filters applied to the data for a specific key. name : The name of the filter. required : If set to true , the filter is automatically added to $filters . If false , users can select a value for the filter key, which is then added to $filters . expr : (Optional) Overrides the top-level expr for a specific metric and dimension. This structured configuration ensures flexibility and ease of use when visualizing and analyzing metrics in the Numaflow UI.","title":"Key Definitions"},{"location":"user-guide/UI/metrics-tab/#explanation-with-an-example","text":"The above configuration might seem complex at first glance. Let\u2019s break it down with an example using the out-of-the-box metric monovtx_processing_time_bucket .","title":"Explanation with an Example"},{"location":"user-guide/UI/metrics-tab/#example-configuration","text":"- name : mono_vertex_histogram objects : - mono-vertex title : MonoVertex Histogram Metrics description : This pattern is for P99, P95, P90, and P50 quantiles for a mono-vertex across different dimensions. expr : | histogram_quantile($quantile, sum by($dimension,le) (rate($metric_name{$filters}[$duration]))) params : - name : quantile required : true - name : duration required : true - name : start_time required : false - name : end_time required : false metrics : - metric_name : monovtx_processing_time_bucket display_name : MonoVertex Processing Time Latency metric_description : This metric represents a histogram to keep track of the total time taken to forward a chunk of messages. required_filters : - namespace - mvtx_name dimensions : - name : mono-vertex - name : pod filters : - name : pod required : false","title":"Example Configuration"},{"location":"user-guide/UI/metrics-tab/#key-points","text":"Pattern Name : mono_vertex_histogram This indicates that the pattern includes histogram metrics for a mono-vertex. Expression ( expr ) The expression calculates quantiles (e.g., P99, P95). Learn more about quantiles in Prometheus here . Placeholders such as quantile , dimension , metric_name , filters , and duration are dynamically populated based on the configuration. Parameters ( params ) quantile and duration are required placeholders in the expression. start_time and end_time are optional parameters for defining the query duration. Metric Name ( metric_name ) This forms the $metric_name placeholder in the expression. Required Filters ( required_filters ) These filters populate the $filters placeholder in the expression. Dimensions For the mono-vertex dimension, no additional filters are applied. For the pod dimension, an additional filter ( pod ) is appended to the $filters placeholder.","title":"Key Points"},{"location":"user-guide/UI/metrics-tab/#example-expressions","text":"Below are examples of how the placeholders are replaced to form the final PromQL expressions: Dimension : mono-vertex Quantile : 0.99 Namespace : default MonoVertex Name : simple-mono-vertex Duration : 5m Resultant Expression : histogram_quantile ( 0.99 , sum by ( mvtx_name , le ) ( rate ( monovtx_processing_time_bucket { namespace = \" default \", mvtx_name = \" simple-mono-vertex \"}[ 5m ] ))) Dimension : pod Quantile : 0.99 Namespace : default MonoVertex Name : simple-mono-vertex Pod : simple-mono-vertex-mv-0-edj2s Duration : 1m Resultant Expression : histogram_quantile ( 0.99 , sum by ( pod , le ) ( rate ( monovtx_processing_time_bucket { namespace = \" default \", mvtx_name = \" simple-mono-vertex \", pod = \" simple-mono-vertex-mv-0-edj2s \"}[ 1m ] ))) This example demonstrates how the configuration translates into actionable PromQL queries, making it easier to understand and customize.","title":"Example Expressions"},{"location":"user-guide/UI/metrics-tab/#how-the-metrics-tab-appears-in-the-ui","text":"The Metrics Tab is located next to the Logs Tab in the Pods View . The following screenshots illustrate how the UI translates the discussed configuration into a visual representation:","title":"How the Metrics Tab Appears in the UI"},{"location":"user-guide/UI/metrics-tab/#monovertex-histogram-example","text":"The UI displays the configured metrics, such as the monovtx_processing_time_bucket histogram, with options to select dimensions and filters.","title":"MonoVertex Histogram Example"},{"location":"user-guide/UI/metrics-tab/#dimension-selection","text":"Users can choose dimensions for the metric, such as MonoVertex or Pod , to view data at different levels of granularity. After selecting Pod dimension, we see that Filters box appears which allow us to add filters for a dimension. Here, pod dimension has pod as a filter.","title":"Dimension Selection"},{"location":"user-guide/UI/metrics-tab/#quantile-selection","text":"For histogram metrics, users can select quantile values (e.g., P99, P95) from the available options, as explained in the Key Definitions section.","title":"Quantile Selection"},{"location":"user-guide/UI/metrics-tab/#query-windowduration","text":"The query window specifies the time range over which the rate() function calculates the per-second average rate of increase for each histogram bucket.","title":"Query Window/Duration"},{"location":"user-guide/UI/metrics-tab/#time-range-selection","text":"Users can select a predefined time range or set a custom time range for their PromQL queries.","title":"Time Range Selection"},{"location":"user-guide/UI/metrics-tab/#adding-custom-metrics","text":"To add metrics not provided by Numaflow, follow the configuration patterns described above. For example, you can add a custom metric similar to the pod_cpu_memory_utilization pattern here .","title":"Adding Custom Metrics"},{"location":"user-guide/UI/overview/","text":"Numaflow UI Overview \u00b6 Numaflow provides a built-in user interface (UI) for monitoring and managing your data pipelines. Accessing the Numaflow UI \u00b6 To access the Numaflow UI, use the following command to port-forward the Numaflow server: kubectl -n numaflow-system port-forward deployment/numaflow-server 8443 :8443 Once port-forwarding is active, open your browser and navigate to https://localhost:8443 . UI Views \u00b6 We have already walked through some of the views of UI to monitor pipelines in the Quick Start guide. Cluster View Namespace View Simple Pipeline View Vertex View \u00b6 Vertex View provides detailed insights into each pipeline vertex. The following features are available: Pods View \u00b6 Inspect the status and details of pods running in your pipelines. Spec \u00b6 View the specification of the vertex. Processing Rates \u00b6 See the last 1m, 5m, and 15m processing rates for each partition of the vertex. If a Prometheus server is configured, you can click on the number to see more details in the Metrics tab. Kubernetes Events \u00b6 View Kubernetes events related to the vertex. Errors \u00b6 Review errors detected in your pipelines for quick debugging. Buffers \u00b6 See buffer details for every partition, including buffer length, usage, and number of pending messages. Click on the pending number to view complete pending metrics in the Metrics tab . Logs \u00b6 View logs for different containers of pods to help diagnose issues. Metrics \u00b6 Monitor pipeline metrics for performance.","title":"Overview"},{"location":"user-guide/UI/overview/#numaflow-ui-overview","text":"Numaflow provides a built-in user interface (UI) for monitoring and managing your data pipelines.","title":"Numaflow UI Overview"},{"location":"user-guide/UI/overview/#accessing-the-numaflow-ui","text":"To access the Numaflow UI, use the following command to port-forward the Numaflow server: kubectl -n numaflow-system port-forward deployment/numaflow-server 8443 :8443 Once port-forwarding is active, open your browser and navigate to https://localhost:8443 .","title":"Accessing the Numaflow UI"},{"location":"user-guide/UI/overview/#ui-views","text":"We have already walked through some of the views of UI to monitor pipelines in the Quick Start guide. Cluster View Namespace View Simple Pipeline View","title":"UI Views"},{"location":"user-guide/UI/overview/#vertex-view","text":"Vertex View provides detailed insights into each pipeline vertex. The following features are available:","title":"Vertex View"},{"location":"user-guide/UI/overview/#pods-view","text":"Inspect the status and details of pods running in your pipelines.","title":"Pods View"},{"location":"user-guide/UI/overview/#spec","text":"View the specification of the vertex.","title":"Spec"},{"location":"user-guide/UI/overview/#processing-rates","text":"See the last 1m, 5m, and 15m processing rates for each partition of the vertex. If a Prometheus server is configured, you can click on the number to see more details in the Metrics tab.","title":"Processing Rates"},{"location":"user-guide/UI/overview/#kubernetes-events","text":"View Kubernetes events related to the vertex.","title":"Kubernetes Events"},{"location":"user-guide/UI/overview/#errors","text":"Review errors detected in your pipelines for quick debugging.","title":"Errors"},{"location":"user-guide/UI/overview/#buffers","text":"See buffer details for every partition, including buffer length, usage, and number of pending messages. Click on the pending number to view complete pending metrics in the Metrics tab .","title":"Buffers"},{"location":"user-guide/UI/overview/#logs","text":"View logs for different containers of pods to help diagnose issues.","title":"Logs"},{"location":"user-guide/UI/overview/#metrics","text":"Monitor pipeline metrics for performance.","title":"Metrics"},{"location":"user-guide/UI/pods-view/","text":"Pods View \u00b6 The Pods View in the Numaflow UI provides a comprehensive overview of the pods associated with a vertex. This view is designed to help users monitor the status, resource usage, and other key details of the pods in their application. It also allows users to perform actions such as filtering and inspecting individual pods. Features of the Pods View \u00b6 1. Select a Pod by Name \u00b6 Displays a list of all pods associated with the selected vertex. Users can select a pod from the dropdown by its name. 2. Select a Pod by Resource \u00b6 CPU and Memory usage of the containers within pods are displayed in a hexagon heat map. Users can select a pod by clicking on any of these hexagons. This is particularly useful when a pod's heat map is red, indicating high resource usage, and you want to inspect that specific pod. 3. Select a Container \u00b6 Allows users to select a container within a pod to view its info and logs . 4. Container Info \u00b6 Provides detailed information about the selected container, including: Name : The name of the container. Status : The current state of the container (e.g., Waiting , Running , or Terminated ). Last Started At : The relative age of the container since it was last (re-)started. CPU : The CPU usage compared to the requested amount. Memory : The memory usage compared to the requested amount. Restart Count : The number of times the container has been restarted. Last Termination Reason : The reason for the container's last termination (if applicable). This helps debug container crashes and restarts. Last Termination Message : A message providing details about the last termination of the container (if applicable). Waiting Reason : The reason why the container is in a waiting state (if applicable). Waiting Message : A message providing details about why the container is in a waiting state (if applicable). 5. Container Logs \u00b6 Quickly access the logs of a specific container within a pod by selecting it. Refer to the Logs View section for a detailed explanation of log features. 6. Pod Info \u00b6 Provides detailed information about the selected pod, including: Name : The name of the pod. Status : The current phase of the pod. Possible values include: Pending Running Succeeded Failed Unknown Note : Learn more about pod phases here . Restart Count : The total number of restarts for all containers within the pod. Since pods are recreated upon termination, this value is an aggregate of container restarts. CPU : The aggregated CPU usage of all containers within the pod compared to the requested amount. Memory : The aggregated memory usage of all containers within the pod compared to the requested amount. Reason : A brief, CamelCase message indicating why the pod is in its current state. This field is populated in cases such as pod scheduling issues, eviction, termination, or node-related problems. Message : A human-readable message providing additional details about the pod's condition. This field is populated in scenarios like pod scheduling issues, eviction, termination, or node-related problems. 7. Metrics Tab \u00b6 The Pods View includes a Metrics Tab located next to the Logs Tab. For a detailed discussion about the Metrics Tab, refer to the Metrics section.","title":"Pods View"},{"location":"user-guide/UI/pods-view/#pods-view","text":"The Pods View in the Numaflow UI provides a comprehensive overview of the pods associated with a vertex. This view is designed to help users monitor the status, resource usage, and other key details of the pods in their application. It also allows users to perform actions such as filtering and inspecting individual pods.","title":"Pods View"},{"location":"user-guide/UI/pods-view/#features-of-the-pods-view","text":"","title":"Features of the Pods View"},{"location":"user-guide/UI/pods-view/#1-select-a-pod-by-name","text":"Displays a list of all pods associated with the selected vertex. Users can select a pod from the dropdown by its name.","title":"1. Select a Pod by Name"},{"location":"user-guide/UI/pods-view/#2-select-a-pod-by-resource","text":"CPU and Memory usage of the containers within pods are displayed in a hexagon heat map. Users can select a pod by clicking on any of these hexagons. This is particularly useful when a pod's heat map is red, indicating high resource usage, and you want to inspect that specific pod.","title":"2. Select a Pod by Resource"},{"location":"user-guide/UI/pods-view/#3-select-a-container","text":"Allows users to select a container within a pod to view its info and logs .","title":"3. Select a Container"},{"location":"user-guide/UI/pods-view/#4-container-info","text":"Provides detailed information about the selected container, including: Name : The name of the container. Status : The current state of the container (e.g., Waiting , Running , or Terminated ). Last Started At : The relative age of the container since it was last (re-)started. CPU : The CPU usage compared to the requested amount. Memory : The memory usage compared to the requested amount. Restart Count : The number of times the container has been restarted. Last Termination Reason : The reason for the container's last termination (if applicable). This helps debug container crashes and restarts. Last Termination Message : A message providing details about the last termination of the container (if applicable). Waiting Reason : The reason why the container is in a waiting state (if applicable). Waiting Message : A message providing details about why the container is in a waiting state (if applicable).","title":"4. Container Info"},{"location":"user-guide/UI/pods-view/#5-container-logs","text":"Quickly access the logs of a specific container within a pod by selecting it. Refer to the Logs View section for a detailed explanation of log features.","title":"5. Container Logs"},{"location":"user-guide/UI/pods-view/#6-pod-info","text":"Provides detailed information about the selected pod, including: Name : The name of the pod. Status : The current phase of the pod. Possible values include: Pending Running Succeeded Failed Unknown Note : Learn more about pod phases here . Restart Count : The total number of restarts for all containers within the pod. Since pods are recreated upon termination, this value is an aggregate of container restarts. CPU : The aggregated CPU usage of all containers within the pod compared to the requested amount. Memory : The aggregated memory usage of all containers within the pod compared to the requested amount. Reason : A brief, CamelCase message indicating why the pod is in its current state. This field is populated in cases such as pod scheduling issues, eviction, termination, or node-related problems. Message : A human-readable message providing additional details about the pod's condition. This field is populated in scenarios like pod scheduling issues, eviction, termination, or node-related problems.","title":"6. Pod Info"},{"location":"user-guide/UI/pods-view/#7-metrics-tab","text":"The Pods View includes a Metrics Tab located next to the Logs Tab. For a detailed discussion about the Metrics Tab, refer to the Metrics section.","title":"7. Metrics Tab"},{"location":"user-guide/reference/autoscaling/","text":"Autoscaling \u00b6 Numaflow Pipeline and MonoVertex are both able to run with Horizontal Pod Autoscaling and Vertical Pod Autoscaling . Horizontal Pod Autoscaling \u00b6 Horizontal Pod Autoscaling approaches supported in Numaflow include: Numaflow Autoscaling Kubernetes HPA Third Party Autoscaling (such as KEDA ) Numaflow Autoscaling \u00b6 Numaflow provides 0 - N autoscaling capability out of the box, it's available for all the MonoVertices and Pipeline vertices including UDF , Sink and most of the Source types (please check each source for more details). Numaflow autoscaling is enabled by default, there are some parameters can be fine-tuned to achieve better results. # A Pipeline example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex scale : disabled : false # Optional, defaults to false. min : 0 # Optional, minimum replicas, defaults to 0. max : 20 # Optional, maximum replicas, defaults to 50. lookbackSeconds : 120 # Optional, defaults to 120. scaleUpCooldownSeconds : 90 # Optional, defaults to 90. scaleDownCooldownSeconds : 90 # Optional, defaults to 90. zeroReplicaSleepSeconds : 120 # Optional, defaults to 120. targetProcessingSeconds : 20 # Optional, defaults to 20. targetBufferAvailability : 50 # Optional, defaults to 50. replicasPerScaleUp : 2 # Optional, defaults to 2. replicasPerScaleDown : 2 # Optional, defaults to 2. --- # A MonoVertex example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : scale : disabled : false # Optional, defaults to false. min : 0 # Optional, minimum replicas, defaults to 0. max : 20 # Optional, maximum replicas, defaults to 50. lookbackSeconds : 120 # Optional, defaults to 120. scaleUpCooldownSeconds : 90 # Optional, defaults to 90. scaleDownCooldownSeconds : 90 # Optional, defaults to 90. zeroReplicaSleepSeconds : 120 # Optional, defaults to 120. targetProcessingSeconds : 20 # Optional, defaults to 20. replicasPerScaleUp : 2 # Optional, defaults to 2. replicasPerScaleDown : 2 # Optional, defaults to 2. disabled - Whether to disable Numaflow autoscaling, defaults to false . min - Minimum replicas, valid value could be an integer >= 0. Defaults to 0 , which means it could be scaled down to 0. max - Maximum replicas, positive integer which should not be less than min , defaults to 50 . if max and min are the same, that will be the fixed replica number. lookbackSeconds - How many seconds to lookback for average processing rate (tps) and pending messages calculation, defaults to 120 . Rate and pending messages metrics are critical for autoscaling, you might need to tune this parameter a bit to see better results. For example, your data source only have 1 minute data input in every 5 minutes, and you don't want the vertices to be scaled down to 0 . In this case, you need to increase lookbackSeconds to overlap 5 minutes, so that the calculated average rate and pending messages won't be 0 during the silent period, in order to prevent from scaling down to 0. The max value allowed to be configured is 600 . On top of this, we have dynamic lookback adjustment which tunes this parameter based on the realtime processing data. scaleUpCooldownSeconds - After a scaling operation, how many seconds to wait for the same Vertex or MonoVertex, if the follow-up operation is a scaling up, defaults to 90 . Please make sure that the time is greater than the pod to be Running and start processing, because the autoscaling algorithm will divide the TPS by the number of pods even if the pod is not Running . scaleDownCooldownSeconds - After a scaling operation, how many seconds to wait for the same Vertex or MonoVertex, if the follow-up operation is a scaling down, defaults to 90 . zeroReplicaSleepSeconds - After scaling a Source Vertex (or MonoVertex) replicas down to 0 , how many seconds to wait before scaling up to 1 replica to peek, defaults to 120 . Numaflow autoscaler periodically scales up a source vertex (or MonoVertex) pod to \"peek\" the incoming data, this is the period of time to wait before peeking. targetProcessingSeconds - It is used to tune the aggressiveness of autoscaling for source vertices (or MonoVertex), it measures how fast you want the vertex to process all the pending messages, defaults to 20 . It is only effective for the MonoVertices or Source vertices in a Pipeline that support autoscaling, typically increasing the value leads to lower processing rate, thus less replicas. targetBufferAvailability - [ Pipeline Only] Targeted buffer availability in percentage, defaults to 50 . It is only effective for UDF and Sink vertices of a Pipeline, it determines how aggressive you want to do for autoscaling, increasing the value will bring more replicas. The value could be > 100 if you would like to achieve very aggressive autoscaling. replicasPerScaleUp - Maximum number of replica change happens in one scale up operation, defaults to 2 . For example, if current replica number is 3, the calculated desired replica number is 8; instead of scaling up the vertex to 8, it only does 5. replicasPerScaleDown - Maximum number of replica change happens in one scale down operation, defaults to 2 . For example, if current replica number is 9, the calculated desired replica number is 4; instead of scaling down the vertex to 4, it only does 7. replicasPerScale - (Deprecated: Use replicasPerScaleUp and replicasPerScaleDown instead, will be removed in v1.5) Maximum number of replica change happens in one scale up or down operation, defaults to 2 . To disable Numaflow autoscaling, set disabled: true as following. # A Pipeline example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex scale : disabled : true --- # A MonoVertex example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : scale : disabled : true Notes Numaflow autoscaling does not apply to reduce vertices of a Pipeline, and the source vertices which do not have a way to calculate their pending messages. Generator HTTP Nats For User-defined Sources, if the function Pending() returns a negative value, autoscaling will not be applied. Kubernetes HPA \u00b6 Kubernetes HPA is supported in Numaflow for any type of Vertex. To use HPA, remember to point the scaleTargetRef to the vertex as below, and disable Numaflow autoscaling in your Pipeline spec. # A Pipeline example. apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : my-vertex-hpa spec : minReplicas : 1 maxReplicas : 3 metrics : - resource : name : cpu targetAverageUtilization : 50 type : Resource scaleTargetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : Vertex name : my-vertex --- # A MonoVertex example. apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : my-mvtx-hpa spec : minReplicas : 1 maxReplicas : 3 metrics : - resource : name : cpu targetAverageUtilization : 50 type : Resource scaleTargetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex name : my-mvtx With the configuration above, Kubernetes HPA controller will keep the target utilization of the pods of the Vertex at 50%. Kubernetes HPA autoscaling is useful for those Source vertices not able to count pending messages, such as HTTP . Third Party Autoscaling \u00b6 Third party autoscaling tools like KEDA are also supported in Numaflow, which can be used to autoscale any type of vertex with the scalers it supports. To use KEDA for vertex autoscaling, same as Kubernetes HPA, point the scaleTargetRef to your vertex, and disable Numaflow autoscaling in your Pipeline spec. # A Pipeline example. apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : my-keda-scaler spec : scaleTargetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : Vertex name : my-vertex ... ... --- # A MonoVertex example. apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : my-keda-scaler spec : scaleTargetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex name : my-mvtx ... ... Vertical Pod Autoscaling \u00b6 Vertical Pod Autoscaling can be achieved by setting the targetRef to Vertex objects as following. # A Pipeline example. spec : targetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : Vertex name : my-vertex --- # A MonoVertex example. spec : targetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex name : my-mvtx","title":"Autoscaling"},{"location":"user-guide/reference/autoscaling/#autoscaling","text":"Numaflow Pipeline and MonoVertex are both able to run with Horizontal Pod Autoscaling and Vertical Pod Autoscaling .","title":"Autoscaling"},{"location":"user-guide/reference/autoscaling/#horizontal-pod-autoscaling","text":"Horizontal Pod Autoscaling approaches supported in Numaflow include: Numaflow Autoscaling Kubernetes HPA Third Party Autoscaling (such as KEDA )","title":"Horizontal Pod Autoscaling"},{"location":"user-guide/reference/autoscaling/#numaflow-autoscaling","text":"Numaflow provides 0 - N autoscaling capability out of the box, it's available for all the MonoVertices and Pipeline vertices including UDF , Sink and most of the Source types (please check each source for more details). Numaflow autoscaling is enabled by default, there are some parameters can be fine-tuned to achieve better results. # A Pipeline example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex scale : disabled : false # Optional, defaults to false. min : 0 # Optional, minimum replicas, defaults to 0. max : 20 # Optional, maximum replicas, defaults to 50. lookbackSeconds : 120 # Optional, defaults to 120. scaleUpCooldownSeconds : 90 # Optional, defaults to 90. scaleDownCooldownSeconds : 90 # Optional, defaults to 90. zeroReplicaSleepSeconds : 120 # Optional, defaults to 120. targetProcessingSeconds : 20 # Optional, defaults to 20. targetBufferAvailability : 50 # Optional, defaults to 50. replicasPerScaleUp : 2 # Optional, defaults to 2. replicasPerScaleDown : 2 # Optional, defaults to 2. --- # A MonoVertex example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : scale : disabled : false # Optional, defaults to false. min : 0 # Optional, minimum replicas, defaults to 0. max : 20 # Optional, maximum replicas, defaults to 50. lookbackSeconds : 120 # Optional, defaults to 120. scaleUpCooldownSeconds : 90 # Optional, defaults to 90. scaleDownCooldownSeconds : 90 # Optional, defaults to 90. zeroReplicaSleepSeconds : 120 # Optional, defaults to 120. targetProcessingSeconds : 20 # Optional, defaults to 20. replicasPerScaleUp : 2 # Optional, defaults to 2. replicasPerScaleDown : 2 # Optional, defaults to 2. disabled - Whether to disable Numaflow autoscaling, defaults to false . min - Minimum replicas, valid value could be an integer >= 0. Defaults to 0 , which means it could be scaled down to 0. max - Maximum replicas, positive integer which should not be less than min , defaults to 50 . if max and min are the same, that will be the fixed replica number. lookbackSeconds - How many seconds to lookback for average processing rate (tps) and pending messages calculation, defaults to 120 . Rate and pending messages metrics are critical for autoscaling, you might need to tune this parameter a bit to see better results. For example, your data source only have 1 minute data input in every 5 minutes, and you don't want the vertices to be scaled down to 0 . In this case, you need to increase lookbackSeconds to overlap 5 minutes, so that the calculated average rate and pending messages won't be 0 during the silent period, in order to prevent from scaling down to 0. The max value allowed to be configured is 600 . On top of this, we have dynamic lookback adjustment which tunes this parameter based on the realtime processing data. scaleUpCooldownSeconds - After a scaling operation, how many seconds to wait for the same Vertex or MonoVertex, if the follow-up operation is a scaling up, defaults to 90 . Please make sure that the time is greater than the pod to be Running and start processing, because the autoscaling algorithm will divide the TPS by the number of pods even if the pod is not Running . scaleDownCooldownSeconds - After a scaling operation, how many seconds to wait for the same Vertex or MonoVertex, if the follow-up operation is a scaling down, defaults to 90 . zeroReplicaSleepSeconds - After scaling a Source Vertex (or MonoVertex) replicas down to 0 , how many seconds to wait before scaling up to 1 replica to peek, defaults to 120 . Numaflow autoscaler periodically scales up a source vertex (or MonoVertex) pod to \"peek\" the incoming data, this is the period of time to wait before peeking. targetProcessingSeconds - It is used to tune the aggressiveness of autoscaling for source vertices (or MonoVertex), it measures how fast you want the vertex to process all the pending messages, defaults to 20 . It is only effective for the MonoVertices or Source vertices in a Pipeline that support autoscaling, typically increasing the value leads to lower processing rate, thus less replicas. targetBufferAvailability - [ Pipeline Only] Targeted buffer availability in percentage, defaults to 50 . It is only effective for UDF and Sink vertices of a Pipeline, it determines how aggressive you want to do for autoscaling, increasing the value will bring more replicas. The value could be > 100 if you would like to achieve very aggressive autoscaling. replicasPerScaleUp - Maximum number of replica change happens in one scale up operation, defaults to 2 . For example, if current replica number is 3, the calculated desired replica number is 8; instead of scaling up the vertex to 8, it only does 5. replicasPerScaleDown - Maximum number of replica change happens in one scale down operation, defaults to 2 . For example, if current replica number is 9, the calculated desired replica number is 4; instead of scaling down the vertex to 4, it only does 7. replicasPerScale - (Deprecated: Use replicasPerScaleUp and replicasPerScaleDown instead, will be removed in v1.5) Maximum number of replica change happens in one scale up or down operation, defaults to 2 . To disable Numaflow autoscaling, set disabled: true as following. # A Pipeline example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex scale : disabled : true --- # A MonoVertex example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : scale : disabled : true Notes Numaflow autoscaling does not apply to reduce vertices of a Pipeline, and the source vertices which do not have a way to calculate their pending messages. Generator HTTP Nats For User-defined Sources, if the function Pending() returns a negative value, autoscaling will not be applied.","title":"Numaflow Autoscaling"},{"location":"user-guide/reference/autoscaling/#kubernetes-hpa","text":"Kubernetes HPA is supported in Numaflow for any type of Vertex. To use HPA, remember to point the scaleTargetRef to the vertex as below, and disable Numaflow autoscaling in your Pipeline spec. # A Pipeline example. apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : my-vertex-hpa spec : minReplicas : 1 maxReplicas : 3 metrics : - resource : name : cpu targetAverageUtilization : 50 type : Resource scaleTargetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : Vertex name : my-vertex --- # A MonoVertex example. apiVersion : autoscaling/v2 kind : HorizontalPodAutoscaler metadata : name : my-mvtx-hpa spec : minReplicas : 1 maxReplicas : 3 metrics : - resource : name : cpu targetAverageUtilization : 50 type : Resource scaleTargetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex name : my-mvtx With the configuration above, Kubernetes HPA controller will keep the target utilization of the pods of the Vertex at 50%. Kubernetes HPA autoscaling is useful for those Source vertices not able to count pending messages, such as HTTP .","title":"Kubernetes HPA"},{"location":"user-guide/reference/autoscaling/#third-party-autoscaling","text":"Third party autoscaling tools like KEDA are also supported in Numaflow, which can be used to autoscale any type of vertex with the scalers it supports. To use KEDA for vertex autoscaling, same as Kubernetes HPA, point the scaleTargetRef to your vertex, and disable Numaflow autoscaling in your Pipeline spec. # A Pipeline example. apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : my-keda-scaler spec : scaleTargetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : Vertex name : my-vertex ... ... --- # A MonoVertex example. apiVersion : keda.sh/v1alpha1 kind : ScaledObject metadata : name : my-keda-scaler spec : scaleTargetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex name : my-mvtx ... ...","title":"Third Party Autoscaling"},{"location":"user-guide/reference/autoscaling/#vertical-pod-autoscaling","text":"Vertical Pod Autoscaling can be achieved by setting the targetRef to Vertex objects as following. # A Pipeline example. spec : targetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : Vertex name : my-vertex --- # A MonoVertex example. spec : targetRef : apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex name : my-mvtx","title":"Vertical Pod Autoscaling"},{"location":"user-guide/reference/conditional-forwarding/","text":"Conditional Forwarding \u00b6 In a pipeline , after processing the data, conditional forwarding is doable based on the tags returned in the result. Below is a list of different logic operations that can be done on tags. and - forwards the message if all the tags specified are present in Message's tags. or - forwards the message if one of the tags specified is present in Message's tags. not - forwards the message if all the tags specified are not present in Message's tags. For example, there's a UDF used to process numbers, and forward the result to different vertices based on the number is even or odd. In this case, you can set the tag to even-tag or odd-tag in each of the returned messages, and define the edges as below: Default Behavior \u00b6 If no conditions are specified in the spec, the message will be forwarded to all the downstream vertices (independent of the tags in the Messages ). In the code, if the Messages are not tagged but conditions are configured, we will still honour the edge conditions. Syntax \u00b6 edges : - from : ... to : ... conditions : tags : operator : ... values : - ... Example \u00b6 edges : - from : p1 to : even-vertex conditions : tags : operator : or # Optional, defaults to \"or\". values : - even-tag - from : p1 to : odd-vertex conditions : tags : operator : not values : - odd-tag - from : p1 to : all conditions : tags : operator : and values : - odd-tag - even-tag","title":"Conditional Forwarding"},{"location":"user-guide/reference/conditional-forwarding/#conditional-forwarding","text":"In a pipeline , after processing the data, conditional forwarding is doable based on the tags returned in the result. Below is a list of different logic operations that can be done on tags. and - forwards the message if all the tags specified are present in Message's tags. or - forwards the message if one of the tags specified is present in Message's tags. not - forwards the message if all the tags specified are not present in Message's tags. For example, there's a UDF used to process numbers, and forward the result to different vertices based on the number is even or odd. In this case, you can set the tag to even-tag or odd-tag in each of the returned messages, and define the edges as below:","title":"Conditional Forwarding"},{"location":"user-guide/reference/conditional-forwarding/#default-behavior","text":"If no conditions are specified in the spec, the message will be forwarded to all the downstream vertices (independent of the tags in the Messages ). In the code, if the Messages are not tagged but conditions are configured, we will still honour the edge conditions.","title":"Default Behavior"},{"location":"user-guide/reference/conditional-forwarding/#syntax","text":"edges : - from : ... to : ... conditions : tags : operator : ... values : - ...","title":"Syntax"},{"location":"user-guide/reference/conditional-forwarding/#example","text":"edges : - from : p1 to : even-vertex conditions : tags : operator : or # Optional, defaults to \"or\". values : - even-tag - from : p1 to : odd-vertex conditions : tags : operator : not values : - odd-tag - from : p1 to : all conditions : tags : operator : and values : - odd-tag - even-tag","title":"Example"},{"location":"user-guide/reference/gpu/","text":"Configuring GPU \u00b6 GPU resources can be configured on a Pipeline Vertex or a MonoVertex. Prerequisites \u00b6 Your cluster must support GPU scheduling and have the appropriate device plugin installed. See the Kubernetes device plugins documentation and NVIDIA device plugin guide for details. Adding Annotations (If Required) \u00b6 In most clusters, specifying the GPU resource in the limits field is enough. Some environments may require additional annotations for GPU access or monitoring. Check with your administrator if unsure. Example: vertices : - name : gpu-vertex metadata : annotations : mycompany.com/gpu-enabled : \"true\" # Replace with your annotation key and value udf : container : image : my-ml-image:latest resources : limits : nvidia.com/gpu : 1 Specifying GPU Resource Requests and Limits \u00b6 Request a GPU by setting the nvidia.com/gpu resource in the limits field under the container section: resources : limits : nvidia.com/gpu : 1 Important: For GPUs, Kubernetes requires that requests and limits must be the same (or specify only limits ). Example: Vertex Requesting a GPU (with Annotations and Node Selector) \u00b6 vertices : - name : gpu-vertex metadata : annotations : mycompany.com/gpu-enabled : \"true\" # Example annotation, use only if required by your cluster nodeSelector : nvidia.com/gpu.present : \"true\" # Replace with your cluster's GPU node label udf : container : image : my-ml-image:latest resources : limits : nvidia.com/gpu : 1 Adjust the nodeSelector and annotations as required by your cluster setup. Dynamic Resource Allocation (Advanced) \u00b6 For advanced GPU scheduling using Dynamic Resource Allocation (DRA), see Dynamic Resource Allocation documentation . Troubleshooting \u00b6 If your vertex is not using GPU resources as expected: - Pod Pending: Check for available GPU nodes and device plugin status. - Pod does not detect GPU: Ensure your container image includes necessary GPU drivers and libraries (e.g., CUDA). - Still having issues? Consult your cluster documentation or administrator. References \u00b6 Container Resources Dynamic Resource Allocation Kubernetes: Managing Resources for Containers","title":"Configuring GPU"},{"location":"user-guide/reference/gpu/#configuring-gpu","text":"GPU resources can be configured on a Pipeline Vertex or a MonoVertex.","title":"Configuring GPU"},{"location":"user-guide/reference/gpu/#prerequisites","text":"Your cluster must support GPU scheduling and have the appropriate device plugin installed. See the Kubernetes device plugins documentation and NVIDIA device plugin guide for details.","title":"Prerequisites"},{"location":"user-guide/reference/gpu/#adding-annotations-if-required","text":"In most clusters, specifying the GPU resource in the limits field is enough. Some environments may require additional annotations for GPU access or monitoring. Check with your administrator if unsure. Example: vertices : - name : gpu-vertex metadata : annotations : mycompany.com/gpu-enabled : \"true\" # Replace with your annotation key and value udf : container : image : my-ml-image:latest resources : limits : nvidia.com/gpu : 1","title":"Adding Annotations (If Required)"},{"location":"user-guide/reference/gpu/#specifying-gpu-resource-requests-and-limits","text":"Request a GPU by setting the nvidia.com/gpu resource in the limits field under the container section: resources : limits : nvidia.com/gpu : 1 Important: For GPUs, Kubernetes requires that requests and limits must be the same (or specify only limits ).","title":"Specifying GPU Resource Requests and Limits"},{"location":"user-guide/reference/gpu/#example-vertex-requesting-a-gpu-with-annotations-and-node-selector","text":"vertices : - name : gpu-vertex metadata : annotations : mycompany.com/gpu-enabled : \"true\" # Example annotation, use only if required by your cluster nodeSelector : nvidia.com/gpu.present : \"true\" # Replace with your cluster's GPU node label udf : container : image : my-ml-image:latest resources : limits : nvidia.com/gpu : 1 Adjust the nodeSelector and annotations as required by your cluster setup.","title":"Example: Vertex Requesting a GPU (with Annotations and Node Selector)"},{"location":"user-guide/reference/gpu/#dynamic-resource-allocation-advanced","text":"For advanced GPU scheduling using Dynamic Resource Allocation (DRA), see Dynamic Resource Allocation documentation .","title":"Dynamic Resource Allocation (Advanced)"},{"location":"user-guide/reference/gpu/#troubleshooting","text":"If your vertex is not using GPU resources as expected: - Pod Pending: Check for available GPU nodes and device plugin status. - Pod does not detect GPU: Ensure your container image includes necessary GPU drivers and libraries (e.g., CUDA). - Still having issues? Consult your cluster documentation or administrator.","title":"Troubleshooting"},{"location":"user-guide/reference/gpu/#references","text":"Container Resources Dynamic Resource Allocation Kubernetes: Managing Resources for Containers","title":"References"},{"location":"user-guide/reference/join-vertex/","text":"Joins and Cycles \u00b6 Numaflow Pipeline Edges can be defined such that multiple Vertices can forward messages to a single vertex. Quick Start \u00b6 Please see the following examples: Join on Map Vertex Join on Reduce Vertex Join on Sink Vertex Cycle to Self Cycle to Previous Why do we need JOIN \u00b6 Without JOIN \u00b6 Without JOIN, Numaflow could only allow users to build pipelines where vertices could only read from previous one vertex. This meant that Numaflow could only support simple pipelines or tree-like pipelines. Supporting pipelines where you had to read from multiple sources or UDFs were cumbersome and required creating redundant vertices. With JOIN \u00b6 Join vertices allow users the flexibility to read from multiple sources, process data from multiple UDFs, and even write to a single sink. The Pipeline Spec doesn't change at all with JOIN, now you can create multiple Edges that have the same \u201cTo\u201d Vertex, which would have otherwise been prohibited. There is no limitation on which vertices can be joined. For instance, one can join Map or Reduce vertices as shown below: Benefits \u00b6 The introduction of Join Vertex allows users to eliminate redundancy in their pipelines. It supports many-to-one data flow without needing multiple vertices performing the same job. Examples \u00b6 Join on Sink Vertex \u00b6 By joining the sink vertices, we now only need a single vertex responsible for sending to the data sink. Example \u00b6 Join on Sink Vertex Join on Map Vertex \u00b6 Two different Sources containing similar data that can be processed the same way can now point to a single vertex. Example \u00b6 Join on Map Vertex Join on Reduce Vertex \u00b6 This feature allows for efficient aggregation of data from multiple sources. Example \u00b6 Join on Reduce Vertex Cycles \u00b6 A special case of a \"Join\" is a Cycle (a Vertex which can send either to itself or to a previous Vertex.) An example use of this is a Map UDF which does some sort of reprocessing of data under certain conditions such as a transient error. Cycles are permitted, except in the case that there's a Reduce Vertex at or downstream of the cycle. (This is because a cycle inevitably produces late data, which would get dropped by the Reduce Vertex. For this reason, cycles should be used sparingly.) The following examples are of Cycles: Cycle to Self Cycle to Previous","title":"Joins and Cycles"},{"location":"user-guide/reference/join-vertex/#joins-and-cycles","text":"Numaflow Pipeline Edges can be defined such that multiple Vertices can forward messages to a single vertex.","title":"Joins and Cycles"},{"location":"user-guide/reference/join-vertex/#quick-start","text":"Please see the following examples: Join on Map Vertex Join on Reduce Vertex Join on Sink Vertex Cycle to Self Cycle to Previous","title":"Quick Start"},{"location":"user-guide/reference/join-vertex/#why-do-we-need-join","text":"","title":"Why do we need JOIN"},{"location":"user-guide/reference/join-vertex/#without-join","text":"Without JOIN, Numaflow could only allow users to build pipelines where vertices could only read from previous one vertex. This meant that Numaflow could only support simple pipelines or tree-like pipelines. Supporting pipelines where you had to read from multiple sources or UDFs were cumbersome and required creating redundant vertices.","title":"Without JOIN"},{"location":"user-guide/reference/join-vertex/#with-join","text":"Join vertices allow users the flexibility to read from multiple sources, process data from multiple UDFs, and even write to a single sink. The Pipeline Spec doesn't change at all with JOIN, now you can create multiple Edges that have the same \u201cTo\u201d Vertex, which would have otherwise been prohibited. There is no limitation on which vertices can be joined. For instance, one can join Map or Reduce vertices as shown below:","title":"With JOIN"},{"location":"user-guide/reference/join-vertex/#benefits","text":"The introduction of Join Vertex allows users to eliminate redundancy in their pipelines. It supports many-to-one data flow without needing multiple vertices performing the same job.","title":"Benefits"},{"location":"user-guide/reference/join-vertex/#examples","text":"","title":"Examples"},{"location":"user-guide/reference/join-vertex/#join-on-sink-vertex","text":"By joining the sink vertices, we now only need a single vertex responsible for sending to the data sink.","title":"Join on Sink Vertex"},{"location":"user-guide/reference/join-vertex/#example","text":"Join on Sink Vertex","title":"Example"},{"location":"user-guide/reference/join-vertex/#join-on-map-vertex","text":"Two different Sources containing similar data that can be processed the same way can now point to a single vertex.","title":"Join on Map Vertex"},{"location":"user-guide/reference/join-vertex/#example_1","text":"Join on Map Vertex","title":"Example"},{"location":"user-guide/reference/join-vertex/#join-on-reduce-vertex","text":"This feature allows for efficient aggregation of data from multiple sources.","title":"Join on Reduce Vertex"},{"location":"user-guide/reference/join-vertex/#example_2","text":"Join on Reduce Vertex","title":"Example"},{"location":"user-guide/reference/join-vertex/#cycles","text":"A special case of a \"Join\" is a Cycle (a Vertex which can send either to itself or to a previous Vertex.) An example use of this is a Map UDF which does some sort of reprocessing of data under certain conditions such as a transient error. Cycles are permitted, except in the case that there's a Reduce Vertex at or downstream of the cycle. (This is because a cycle inevitably produces late data, which would get dropped by the Reduce Vertex. For this reason, cycles should be used sparingly.) The following examples are of Cycles: Cycle to Self Cycle to Previous","title":"Cycles"},{"location":"user-guide/reference/multi-partition/","text":"Multi-partitioned Edges \u00b6 To achieve higher throughput(> 10K but < 30K tps), users can create pipelines with multi-partitioned edges. Multi-partitioned edges are only supported for pipelines with JetStream as Inter-Step Buffer . Please ensure that the JetStream is provisioned with more nodes to support higher throughput. Since partitions are owned by the vertex reading the data, to create a multi-partitioned edge we need to configure the vertex reading the data (to-vertex) to have multiple partitions. The following code snippet provides an example of how to configure a vertex (in this case, the cat vertex) to have multiple partitions, which enables it ( cat vertex) to read at a higher throughput. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : cat partitions : 3 udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always","title":"Multi-partitioned Edges"},{"location":"user-guide/reference/multi-partition/#multi-partitioned-edges","text":"To achieve higher throughput(> 10K but < 30K tps), users can create pipelines with multi-partitioned edges. Multi-partitioned edges are only supported for pipelines with JetStream as Inter-Step Buffer . Please ensure that the JetStream is provisioned with more nodes to support higher throughput. Since partitions are owned by the vertex reading the data, to create a multi-partitioned edge we need to configure the vertex reading the data (to-vertex) to have multiple partitions. The following code snippet provides an example of how to configure a vertex (in this case, the cat vertex) to have multiple partitions, which enables it ( cat vertex) to read at a higher throughput. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : cat partitions : 3 udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always","title":"Multi-partitioned Edges"},{"location":"user-guide/reference/mvtx-operations/","text":"MonoVertex Operations \u00b6 Pause a MonoVertex \u00b6 To pause a MonoVertex , use the command below, it will bring the MonoVertex to Paused state, and terminate all the running MonoVertex pods. kubectl patch mvtx my-mvtx --type = merge --patch '{\"spec\": {\"lifecycle\": {\"desiredPhase\": \"Paused\"}}}' Resume a MonoVertex \u00b6 The command below will bring the MonoVertex back to Running state, resuming with the same number of pods it had prior to being paused. kubectl patch mvtx my-mvtx --type = merge --patch '{\"spec\": {\"lifecycle\": {\"desiredPhase\": \"Running\"}}}' Run the following command if you would like to resume the MonoVertex with the minimal number of pods (the number defined in spec.scale.min , otherwise 1). kubectl patch mvtx my-mvtx --type = merge --patch '{\"spec\": {\"lifecycle\": {\"desiredPhase\": \"Running\"}, \"replicas\": null}}'","title":"MonoVertex Operations"},{"location":"user-guide/reference/mvtx-operations/#monovertex-operations","text":"","title":"MonoVertex Operations"},{"location":"user-guide/reference/mvtx-operations/#pause-a-monovertex","text":"To pause a MonoVertex , use the command below, it will bring the MonoVertex to Paused state, and terminate all the running MonoVertex pods. kubectl patch mvtx my-mvtx --type = merge --patch '{\"spec\": {\"lifecycle\": {\"desiredPhase\": \"Paused\"}}}'","title":"Pause a MonoVertex"},{"location":"user-guide/reference/mvtx-operations/#resume-a-monovertex","text":"The command below will bring the MonoVertex back to Running state, resuming with the same number of pods it had prior to being paused. kubectl patch mvtx my-mvtx --type = merge --patch '{\"spec\": {\"lifecycle\": {\"desiredPhase\": \"Running\"}}}' Run the following command if you would like to resume the MonoVertex with the minimal number of pods (the number defined in spec.scale.min , otherwise 1). kubectl patch mvtx my-mvtx --type = merge --patch '{\"spec\": {\"lifecycle\": {\"desiredPhase\": \"Running\"}, \"replicas\": null}}'","title":"Resume a MonoVertex"},{"location":"user-guide/reference/mvtx-tuning/","text":"MonoVertex Tuning \u00b6 Similar to pipeline tuning , certain parameters can be fine-tuned for the data processing using MonoVertex . Each MonoVertex keeps running the cycle of reading data from a data source, processing the data, and writing to a sink. There are some parameters can be adjusted for this cycle. readBatchSize - How many messages to read for each cycle, defaults to 500 . It works together with readTimeout during a read operation, concluding when either limit is reached first. readTimeout - Read timeout from the source, defaults to 1s . These parameters can be customized under spec.limits as below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : limits : readBatchSize : 100 readTimeout : 500ms","title":"MonoVertex Tuning"},{"location":"user-guide/reference/mvtx-tuning/#monovertex-tuning","text":"Similar to pipeline tuning , certain parameters can be fine-tuned for the data processing using MonoVertex . Each MonoVertex keeps running the cycle of reading data from a data source, processing the data, and writing to a sink. There are some parameters can be adjusted for this cycle. readBatchSize - How many messages to read for each cycle, defaults to 500 . It works together with readTimeout during a read operation, concluding when either limit is reached first. readTimeout - Read timeout from the source, defaults to 1s . These parameters can be customized under spec.limits as below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : limits : readBatchSize : 100 readTimeout : 500ms","title":"MonoVertex Tuning"},{"location":"user-guide/reference/pipeline-operations/","text":"Pipeline Operations \u00b6 Update a Pipeline \u00b6 You might want to make some changes to an existing pipeline, for example, updating request CPU, or changing the minimal replicas for a vertex. Updating a pipeline is as simple as applying the new pipeline spec to the existing one. But there are some scenarios that you'd better not update the pipeline, instead, you should delete and recreate it. The scenarios include but are not limited to: Topology changes such as adding or removing vertices, or updating the edges between vertices. Updating the partitions for a keyed reduce vertex. Updating the user-defined container image for a vertex, while the new image can not properly handle the unprocessed data in its backlog. To summarize, if there are unprocessed messages in the pipeline, and the new pipeline spec will change the way how the messages are processed, then you should delete and recreate the pipeline. Pause a Pipeline \u00b6 To pause a pipeline, use the command below, it will bring the pipeline to Paused status, and terminate all the running vertex pods. kubectl patch pl my-pipeline --type = merge --patch '{\"spec\": {\"lifecycle\": {\"desiredPhase\": \"Paused\"}}}' Pausing a pipeline will not cause data loss. It does not clean up the unprocessed data in the pipeline, but just terminates the running pods. When the pipeline is resumed, the pods will be restarted and continue processing the unprocessed data. When pausing a pipeline, it will shutdown the source vertex pods first, and then wait for the other vertices to finish the backlog before terminating them. However, it will not wait forever and will terminate the pods after pauseGracePeriodSeconds . This is default set to 30 and can be customized by setting spec.lifecycle.pauseGracePeriodSeconds . If there's a reduce vertex in the pipeline, please make sure it uses Persistent Volume Claim for storage, otherwise the data will be lost. Resume a Pipeline \u00b6 The command below will bring the pipeline back to Running status. This will resume all vertices with the same number of pods when they were running before pausing kubectl patch pl my-pipeline \\ --type = merge \\ --patch '{\"spec\":{\"lifecycle\":{\"desiredPhase\":\"Running\"}},\"metadata\":{\"annotations\":{\"numaflow.numaproj.io/resume-strategy\":\"fast\"}}}' Run the following command if you would like to resume the Pipeline vertices with the minimal number of pods (the number defined in spec.scale.min , otherwise 1). kubectl patch pl my-pipeline \\ --type = merge \\ --patch '{\"spec\":{\"lifecycle\":{\"desiredPhase\":\"Running\"}},\"metadata\":{\"annotations\":{\"numaflow.numaproj.io/resume-strategy\":\"slow\"}}}' Delete a Pipeline \u00b6 When deleting a pipeline, before terminating all the pods, it will try to wait for all the backlog messages that have already been ingested into the pipeline to be processed. However, it will not wait forever, if the backlog is too large, it will terminate the pods after deletionGracePeriodSeconds , which defaults to 30, and can be customized by setting spec.lifecycle.deletionGracePeriodSeconds .","title":"Pipeline Operations"},{"location":"user-guide/reference/pipeline-operations/#pipeline-operations","text":"","title":"Pipeline Operations"},{"location":"user-guide/reference/pipeline-operations/#update-a-pipeline","text":"You might want to make some changes to an existing pipeline, for example, updating request CPU, or changing the minimal replicas for a vertex. Updating a pipeline is as simple as applying the new pipeline spec to the existing one. But there are some scenarios that you'd better not update the pipeline, instead, you should delete and recreate it. The scenarios include but are not limited to: Topology changes such as adding or removing vertices, or updating the edges between vertices. Updating the partitions for a keyed reduce vertex. Updating the user-defined container image for a vertex, while the new image can not properly handle the unprocessed data in its backlog. To summarize, if there are unprocessed messages in the pipeline, and the new pipeline spec will change the way how the messages are processed, then you should delete and recreate the pipeline.","title":"Update a Pipeline"},{"location":"user-guide/reference/pipeline-operations/#pause-a-pipeline","text":"To pause a pipeline, use the command below, it will bring the pipeline to Paused status, and terminate all the running vertex pods. kubectl patch pl my-pipeline --type = merge --patch '{\"spec\": {\"lifecycle\": {\"desiredPhase\": \"Paused\"}}}' Pausing a pipeline will not cause data loss. It does not clean up the unprocessed data in the pipeline, but just terminates the running pods. When the pipeline is resumed, the pods will be restarted and continue processing the unprocessed data. When pausing a pipeline, it will shutdown the source vertex pods first, and then wait for the other vertices to finish the backlog before terminating them. However, it will not wait forever and will terminate the pods after pauseGracePeriodSeconds . This is default set to 30 and can be customized by setting spec.lifecycle.pauseGracePeriodSeconds . If there's a reduce vertex in the pipeline, please make sure it uses Persistent Volume Claim for storage, otherwise the data will be lost.","title":"Pause a Pipeline"},{"location":"user-guide/reference/pipeline-operations/#resume-a-pipeline","text":"The command below will bring the pipeline back to Running status. This will resume all vertices with the same number of pods when they were running before pausing kubectl patch pl my-pipeline \\ --type = merge \\ --patch '{\"spec\":{\"lifecycle\":{\"desiredPhase\":\"Running\"}},\"metadata\":{\"annotations\":{\"numaflow.numaproj.io/resume-strategy\":\"fast\"}}}' Run the following command if you would like to resume the Pipeline vertices with the minimal number of pods (the number defined in spec.scale.min , otherwise 1). kubectl patch pl my-pipeline \\ --type = merge \\ --patch '{\"spec\":{\"lifecycle\":{\"desiredPhase\":\"Running\"}},\"metadata\":{\"annotations\":{\"numaflow.numaproj.io/resume-strategy\":\"slow\"}}}'","title":"Resume a Pipeline"},{"location":"user-guide/reference/pipeline-operations/#delete-a-pipeline","text":"When deleting a pipeline, before terminating all the pods, it will try to wait for all the backlog messages that have already been ingested into the pipeline to be processed. However, it will not wait forever, if the backlog is too large, it will terminate the pods after deletionGracePeriodSeconds , which defaults to 30, and can be customized by setting spec.lifecycle.deletionGracePeriodSeconds .","title":"Delete a Pipeline"},{"location":"user-guide/reference/pipeline-tuning/","text":"Pipeline Tuning \u00b6 In a data processing pipeline , certain parameters can be fine-tuned according to the specific use case of the data processing. Vertex Tuning \u00b6 Each vertex keeps running the cycle of reading data from an Inter-Step Buffer (or data source), processing the data, and writing to the next Inter-Step Buffers (or sinks). There are some parameters can be adjusted for this data processing cycle. readBatchSize - The number of messages to read in each cycle, with a default value of 500 . It works together with readTimeout during a read operation, concluding when either limit is reached first. readTimeout - Read timeout from the source or Inter-Step Buffer, defaults to 1s . It works in conjunction with readBatchSize . bufferMaxLength - How many unprocessed messages can be existing in the Inter-Step Buffer, defaults to 30000 . bufferUsageLimit - The percentage of the buffer usage limit, a valid number should be less than 100. Default value is 80 , which means 80% . These parameters can be customized under spec.limits as below, once defined, they apply to all the vertices and Inter-Step Buffers of the pipeline. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : limits : readBatchSize : 100 readTimeout : 200ms bufferMaxLength : 30000 bufferUsageLimit : 85 They also can be defined in a vertex level, which will override the pipeline level settings. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : limits : # Default limits for all the vertices and edges (buffers) of this pipeline readBatchSize : 100 readTimeout : 200ms bufferMaxLength : 30000 bufferUsageLimit : 85 vertices : - name : in source : generator : rpu : 5 duration : 1s - name : cat udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always limits : readBatchSize : 200 # It overrides the default limit \"100\" bufferMaxLength : 20000 # It overrides the default limit \"30000\" for the buffers owned by this vertex bufferUsageLimit : 70 # It overrides the default limit \"85\" for the buffers owned by this vertex - name : out sink : log : {} edges : - from : in to : cat - from : cat to : out Edge Tuning \u00b6 There is an edge level setting to drop the messages if the buffer.isFull == true . Even if the UDF or UDSink drops a message due to some internal error in the user-defined code, the processing latency will spike up causing a natural back pressure. A kill switch to drop messages can help alleviate/avoid any repercussions on the rest of the DAG. This setting is an edge-level setting and can be enabled by onFull and the default is retryUntilSuccess (other option is discardLatest ). This is a data loss scenario but can be useful in cases where we are doing user-introduced experimentations, like A/B testing, on the pipeline. It is totally okay for the experimentation side of the DAG to have data loss while the production is unaffected. discardLatest \u00b6 Setting onFull to discardLatest will drop the incoming message on the floor if the edge is full. edges : - from : a to : b onFull : discardLatest retryUntilSuccess \u00b6 The default setting for onFull in retryUntilSuccess which will make sure the incoming message is retried until successful. edges : - from : a to : b onFull : retryUntilSuccess","title":"Pipeline Tuning"},{"location":"user-guide/reference/pipeline-tuning/#pipeline-tuning","text":"In a data processing pipeline , certain parameters can be fine-tuned according to the specific use case of the data processing.","title":"Pipeline Tuning"},{"location":"user-guide/reference/pipeline-tuning/#vertex-tuning","text":"Each vertex keeps running the cycle of reading data from an Inter-Step Buffer (or data source), processing the data, and writing to the next Inter-Step Buffers (or sinks). There are some parameters can be adjusted for this data processing cycle. readBatchSize - The number of messages to read in each cycle, with a default value of 500 . It works together with readTimeout during a read operation, concluding when either limit is reached first. readTimeout - Read timeout from the source or Inter-Step Buffer, defaults to 1s . It works in conjunction with readBatchSize . bufferMaxLength - How many unprocessed messages can be existing in the Inter-Step Buffer, defaults to 30000 . bufferUsageLimit - The percentage of the buffer usage limit, a valid number should be less than 100. Default value is 80 , which means 80% . These parameters can be customized under spec.limits as below, once defined, they apply to all the vertices and Inter-Step Buffers of the pipeline. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : limits : readBatchSize : 100 readTimeout : 200ms bufferMaxLength : 30000 bufferUsageLimit : 85 They also can be defined in a vertex level, which will override the pipeline level settings. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : limits : # Default limits for all the vertices and edges (buffers) of this pipeline readBatchSize : 100 readTimeout : 200ms bufferMaxLength : 30000 bufferUsageLimit : 85 vertices : - name : in source : generator : rpu : 5 duration : 1s - name : cat udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always limits : readBatchSize : 200 # It overrides the default limit \"100\" bufferMaxLength : 20000 # It overrides the default limit \"30000\" for the buffers owned by this vertex bufferUsageLimit : 70 # It overrides the default limit \"85\" for the buffers owned by this vertex - name : out sink : log : {} edges : - from : in to : cat - from : cat to : out","title":"Vertex Tuning"},{"location":"user-guide/reference/pipeline-tuning/#edge-tuning","text":"There is an edge level setting to drop the messages if the buffer.isFull == true . Even if the UDF or UDSink drops a message due to some internal error in the user-defined code, the processing latency will spike up causing a natural back pressure. A kill switch to drop messages can help alleviate/avoid any repercussions on the rest of the DAG. This setting is an edge-level setting and can be enabled by onFull and the default is retryUntilSuccess (other option is discardLatest ). This is a data loss scenario but can be useful in cases where we are doing user-introduced experimentations, like A/B testing, on the pipeline. It is totally okay for the experimentation side of the DAG to have data loss while the production is unaffected.","title":"Edge Tuning"},{"location":"user-guide/reference/pipeline-tuning/#discardlatest","text":"Setting onFull to discardLatest will drop the incoming message on the floor if the edge is full. edges : - from : a to : b onFull : discardLatest","title":"discardLatest"},{"location":"user-guide/reference/pipeline-tuning/#retryuntilsuccess","text":"The default setting for onFull in retryUntilSuccess which will make sure the incoming message is retried until successful. edges : - from : a to : b onFull : retryUntilSuccess","title":"retryUntilSuccess"},{"location":"user-guide/reference/side-inputs/","text":"Side Inputs \u00b6 For an unbounded pipeline in Numaflow that never terminates, there are many cases where users want to update a configuration of the UDF without restarting the pipeline. Numaflow enables it by the Side Inputs feature where we can broadcast changes to vertices automatically. The Side Inputs feature achieves this by allowing users to write custom UDFs to broadcast changes to the vertices that are listening in for updates. Using Side Inputs in Numaflow \u00b6 The Side Inputs are updated based on a cron-like schedule, specified in the pipeline spec with a trigger field. Multiple side inputs are supported as well. Below is an example of pipeline spec with side inputs, which runs the custom UDFs every 15 mins and broadcasts the changes if there is any change to be broadcasted. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : sideInputs : - name : s3 container : image : my-sideinputs-s3-image:v1 trigger : schedule : \"*/15 * * * *\" # timezone: America/Los_Angeles - name : redis container : image : my-sideinputs-redis-image:v1 trigger : schedule : \"*/15 * * * *\" # timezone: America/Los_Angeles vertices : - name : my-vertex sideInputs : - s3 - name : my-vertex-multiple-side-inputs sideInputs : - s3 - redis Implementing User-defined Side Inputs \u00b6 To use the Side Inputs feature, a User-defined function implementing an interface defined in the Numaflow SDK ( Go , Python , Java ) is needed to retrieve the data. You can choose the SDK of your choice to create a User-defined Side Input image which implements the Side Inputs Update. Example in Golang \u00b6 Here is an example of how to write a User-defined Side Input in Golang, // handle is the side input handler function. func handle ( _ context . Context ) sideinputsdk . Message { t := time . Now () // val is the side input message value. This would be the value that the side input vertex receives. val := \"an example: \" + string ( t . String ()) // randomly drop side input message. Note that the side input message is not retried. // NoBroadcastMessage() is used to drop the message and not to // broadcast it to other side input vertices. counter = ( counter + 1 ) % 10 if counter % 2 == 0 { return sideinputsdk . NoBroadcastMessage () } // BroadcastMessage() is used to broadcast the message with the given value to other side input vertices. // val must be converted to []byte. return sideinputsdk . BroadcastMessage ([] byte ( val )) } Similarly, this can be written in Python and Java as well. After performing the retrieval/update, the side input value is then broadcasted to all vertices that use the side input. // BroadcastMessage() is used to broadcast the message with the given value. sideinputsdk . BroadcastMessage ([] byte ( val )) In some cased the user may want to drop the message and not to broadcast the side input value further. // NoBroadcastMessage() is used to drop the message and not to broadcast it further sideinputsdk . NoBroadcastMessage () UDF \u00b6 Users need to add a watcher on the filesystem to fetch the updated side inputs in their User-defined Source/Function/Sink in order to apply the new changes into the data process. For each side input there will be a file with the given path and after any update to the side input value the file will be updated. The directory is fixed and can be accessed through a sideinput constant and the file name is the name of the side input. sideinput . DirPath - > \"/var/numaflow/side-inputs\" sideInputFileName - > \"/var/numaflow/side-inputs/sideInputName\" Here are some examples of watching the side input filesystem for changes in Golang , Python and Java .","title":"Side Inputs"},{"location":"user-guide/reference/side-inputs/#side-inputs","text":"For an unbounded pipeline in Numaflow that never terminates, there are many cases where users want to update a configuration of the UDF without restarting the pipeline. Numaflow enables it by the Side Inputs feature where we can broadcast changes to vertices automatically. The Side Inputs feature achieves this by allowing users to write custom UDFs to broadcast changes to the vertices that are listening in for updates.","title":"Side Inputs"},{"location":"user-guide/reference/side-inputs/#using-side-inputs-in-numaflow","text":"The Side Inputs are updated based on a cron-like schedule, specified in the pipeline spec with a trigger field. Multiple side inputs are supported as well. Below is an example of pipeline spec with side inputs, which runs the custom UDFs every 15 mins and broadcasts the changes if there is any change to be broadcasted. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : sideInputs : - name : s3 container : image : my-sideinputs-s3-image:v1 trigger : schedule : \"*/15 * * * *\" # timezone: America/Los_Angeles - name : redis container : image : my-sideinputs-redis-image:v1 trigger : schedule : \"*/15 * * * *\" # timezone: America/Los_Angeles vertices : - name : my-vertex sideInputs : - s3 - name : my-vertex-multiple-side-inputs sideInputs : - s3 - redis","title":"Using Side Inputs in Numaflow"},{"location":"user-guide/reference/side-inputs/#implementing-user-defined-side-inputs","text":"To use the Side Inputs feature, a User-defined function implementing an interface defined in the Numaflow SDK ( Go , Python , Java ) is needed to retrieve the data. You can choose the SDK of your choice to create a User-defined Side Input image which implements the Side Inputs Update.","title":"Implementing User-defined Side Inputs"},{"location":"user-guide/reference/side-inputs/#example-in-golang","text":"Here is an example of how to write a User-defined Side Input in Golang, // handle is the side input handler function. func handle ( _ context . Context ) sideinputsdk . Message { t := time . Now () // val is the side input message value. This would be the value that the side input vertex receives. val := \"an example: \" + string ( t . String ()) // randomly drop side input message. Note that the side input message is not retried. // NoBroadcastMessage() is used to drop the message and not to // broadcast it to other side input vertices. counter = ( counter + 1 ) % 10 if counter % 2 == 0 { return sideinputsdk . NoBroadcastMessage () } // BroadcastMessage() is used to broadcast the message with the given value to other side input vertices. // val must be converted to []byte. return sideinputsdk . BroadcastMessage ([] byte ( val )) } Similarly, this can be written in Python and Java as well. After performing the retrieval/update, the side input value is then broadcasted to all vertices that use the side input. // BroadcastMessage() is used to broadcast the message with the given value. sideinputsdk . BroadcastMessage ([] byte ( val )) In some cased the user may want to drop the message and not to broadcast the side input value further. // NoBroadcastMessage() is used to drop the message and not to broadcast it further sideinputsdk . NoBroadcastMessage ()","title":"Example in Golang"},{"location":"user-guide/reference/side-inputs/#udf","text":"Users need to add a watcher on the filesystem to fetch the updated side inputs in their User-defined Source/Function/Sink in order to apply the new changes into the data process. For each side input there will be a file with the given path and after any update to the side input value the file will be updated. The directory is fixed and can be accessed through a sideinput constant and the file name is the name of the side input. sideinput . DirPath - > \"/var/numaflow/side-inputs\" sideInputFileName - > \"/var/numaflow/side-inputs/sideInputName\" Here are some examples of watching the side input filesystem for changes in Golang , Python and Java .","title":"UDF"},{"location":"user-guide/reference/configuration/container-resources/","text":"Container Resources \u00b6 Container Resources can be customized for all the types of vertices. For configuring container resources on pods not owned by a vertex, see Pipeline Customization . Numa Container \u00b6 To specify resources for the numa container of vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex containerTemplate : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim UDF Container \u00b6 To specify resources for udf container of vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex udf : container : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim UDSource Container \u00b6 To specify resources for udsource container of a source vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex source : udsource : container : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim Source Transformer Container \u00b6 To specify resources for transformer container of a source vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex source : transformer : container : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim UDSink Container \u00b6 To specify resources for udsink container of vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex resourceClaims : - name : my-claim xxx sink : udsink : container : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim Init Container \u00b6 To specify resources for the init init-container of vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex initContainerTemplate : resources : limits : cpu : \"2\" memory : 2Gi requests : cpu : \"1\" memory : 1Gi Container resources for user init-containers are instead specified at .spec.vertices[*].initContainers[*].resources .","title":"Container Resources"},{"location":"user-guide/reference/configuration/container-resources/#container-resources","text":"Container Resources can be customized for all the types of vertices. For configuring container resources on pods not owned by a vertex, see Pipeline Customization .","title":"Container Resources"},{"location":"user-guide/reference/configuration/container-resources/#numa-container","text":"To specify resources for the numa container of vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex containerTemplate : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim","title":"Numa Container"},{"location":"user-guide/reference/configuration/container-resources/#udf-container","text":"To specify resources for udf container of vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex udf : container : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim","title":"UDF Container"},{"location":"user-guide/reference/configuration/container-resources/#udsource-container","text":"To specify resources for udsource container of a source vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex source : udsource : container : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim","title":"UDSource Container"},{"location":"user-guide/reference/configuration/container-resources/#source-transformer-container","text":"To specify resources for transformer container of a source vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex source : transformer : container : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim","title":"Source Transformer Container"},{"location":"user-guide/reference/configuration/container-resources/#udsink-container","text":"To specify resources for udsink container of vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex resourceClaims : - name : my-claim xxx sink : udsink : container : resources : limits : cpu : \"3\" memory : 6Gi requests : cpu : \"1\" memory : 4Gi claims : - name : my-claim","title":"UDSink Container"},{"location":"user-guide/reference/configuration/container-resources/#init-container","text":"To specify resources for the init init-container of vertex pods: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex initContainerTemplate : resources : limits : cpu : \"2\" memory : 2Gi requests : cpu : \"1\" memory : 1Gi Container resources for user init-containers are instead specified at .spec.vertices[*].initContainers[*].resources .","title":"Init Container"},{"location":"user-guide/reference/configuration/dra/","text":"Dynamic Resource Allocation \u00b6 Dynamic Resource Allocation is supported in both Pipeline and MonoVertex . Check the examples below. apiVersion : resource.k8s.io/v1alpha3 kind : ResourceClaimTemplate metadata : name : my-gpu spec : spec : devices : requests : - name : gpu deviceClassName : gpu.nvidia.com --- apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : inference udf : container : image : my-image resources : claims : - name : gpu resourceClaims : - name : gpu resourceClaimTemplateName : my-gpu --- apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : source : udsource : container : image : my-source resources : claims : - name : gpu-1 sink : udsink : container : image : my-sink resources : claims : - name : gpu-2 resourceClaims : - name : gpu-1 resourceClaimTemplateName : my-gpu - name : gpu-2 resourceClaimTemplateName : my-gpu","title":"Dynamic Resource Allocation"},{"location":"user-guide/reference/configuration/dra/#dynamic-resource-allocation","text":"Dynamic Resource Allocation is supported in both Pipeline and MonoVertex . Check the examples below. apiVersion : resource.k8s.io/v1alpha3 kind : ResourceClaimTemplate metadata : name : my-gpu spec : spec : devices : requests : - name : gpu deviceClassName : gpu.nvidia.com --- apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : inference udf : container : image : my-image resources : claims : - name : gpu resourceClaims : - name : gpu resourceClaimTemplateName : my-gpu --- apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : source : udsource : container : image : my-source resources : claims : - name : gpu-1 sink : udsink : container : image : my-sink resources : claims : - name : gpu-2 resourceClaims : - name : gpu-1 resourceClaimTemplateName : my-gpu - name : gpu-2 resourceClaimTemplateName : my-gpu","title":"Dynamic Resource Allocation"},{"location":"user-guide/reference/configuration/environment-variables/","text":"Environment Variables \u00b6 For the numa container of vertex pods, environment variable NUMAFLOW_DEBUG can be set to true for debugging . In udf , udsink and transformer containers, there are some preset environment variables that can be used directly. NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex. NUMAFLOW_CPU_REQUEST - resources.requests.cpu , roundup to N cores, 0 if missing. NUMAFLOW_CPU_LIMIT - resources.limits.cpu , roundup to N cores, use host cpu cores if missing. NUMAFLOW_MEMORY_REQUEST - resources.requests.memory in bytes, 0 if missing. NUMAFLOW_MEMORY_LIMIT - resources.limits.memory in bytes, use host memory if missing. For setting environment variables on pods not owned by a vertex, see Pipeline Customization . Your Own Environment Variables \u00b6 To add your own environment variables to udf or udsink containers, check the example below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf udf : container : image : my-function:latest env : - name : env01 value : value01 - name : env02 valueFrom : secretKeyRef : name : my-secret key : my-key - name : my-sink sink : udsink : container : image : my-sink:latest env : - name : env03 value : value03 Similarly, envFrom also can be specified in udf or udsink containers. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf udf : container : image : my-function:latest envFrom : - configMapRef : name : my-config - name : my-sink sink : udsink : container : image : my-sink:latest envFrom : - secretRef : name : my-secret","title":"Environment Variables"},{"location":"user-guide/reference/configuration/environment-variables/#environment-variables","text":"For the numa container of vertex pods, environment variable NUMAFLOW_DEBUG can be set to true for debugging . In udf , udsink and transformer containers, there are some preset environment variables that can be used directly. NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex. NUMAFLOW_CPU_REQUEST - resources.requests.cpu , roundup to N cores, 0 if missing. NUMAFLOW_CPU_LIMIT - resources.limits.cpu , roundup to N cores, use host cpu cores if missing. NUMAFLOW_MEMORY_REQUEST - resources.requests.memory in bytes, 0 if missing. NUMAFLOW_MEMORY_LIMIT - resources.limits.memory in bytes, use host memory if missing. For setting environment variables on pods not owned by a vertex, see Pipeline Customization .","title":"Environment Variables"},{"location":"user-guide/reference/configuration/environment-variables/#your-own-environment-variables","text":"To add your own environment variables to udf or udsink containers, check the example below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf udf : container : image : my-function:latest env : - name : env01 value : value01 - name : env02 valueFrom : secretKeyRef : name : my-secret key : my-key - name : my-sink sink : udsink : container : image : my-sink:latest env : - name : env03 value : value03 Similarly, envFrom also can be specified in udf or udsink containers. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf udf : container : image : my-function:latest envFrom : - configMapRef : name : my-config - name : my-sink sink : udsink : container : image : my-sink:latest envFrom : - secretRef : name : my-secret","title":"Your Own Environment Variables"},{"location":"user-guide/reference/configuration/init-containers/","text":"Init Containers \u00b6 Init Containers can be provided for all the types of vertices. The following example shows how to add an init-container to a udf vertex. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf initContainers : - name : my-init image : busybox:latest command : [ \"/bin/sh\" , \"-c\" , \"echo \\\"my-init is running!\\\" && sleep 60\" ] udf : container : image : my-function:latest The following example shows how to use init-containers and volumes together to provide a udf container files on startup. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf volumes : - name : my-udf-data emptyDir : {} initContainers : - name : my-init image : amazon/aws-cli:latest command : [ \"/bin/sh\" , \"-c\" , \"aws s3 sync s3://path/to/my-s3-data /path/to/my-init-data\" ] volumeMounts : - mountPath : /path/to/my-init-data name : my-udf-data udf : container : image : my-function:latest volumeMounts : - mountPath : /path/to/my-data name : my-udf-data","title":"Init Containers"},{"location":"user-guide/reference/configuration/init-containers/#init-containers","text":"Init Containers can be provided for all the types of vertices. The following example shows how to add an init-container to a udf vertex. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf initContainers : - name : my-init image : busybox:latest command : [ \"/bin/sh\" , \"-c\" , \"echo \\\"my-init is running!\\\" && sleep 60\" ] udf : container : image : my-function:latest The following example shows how to use init-containers and volumes together to provide a udf container files on startup. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf volumes : - name : my-udf-data emptyDir : {} initContainers : - name : my-init image : amazon/aws-cli:latest command : [ \"/bin/sh\" , \"-c\" , \"aws s3 sync s3://path/to/my-s3-data /path/to/my-init-data\" ] volumeMounts : - mountPath : /path/to/my-init-data name : my-udf-data udf : container : image : my-function:latest volumeMounts : - mountPath : /path/to/my-data name : my-udf-data","title":"Init Containers"},{"location":"user-guide/reference/configuration/istio/","text":"Running on Istio \u00b6 If you want to get pipeline vertices running on Istio, so that they are able to talk to other services with Istio enabled, one approach is to whitelist the ports that Numaflow uses. To whitelist the ports, add traffic.sidecar.istio.io/excludeInboundPorts and traffic.sidecar.istio.io/excludeOutboundPorts annotations to your vertex configuration: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex metadata : annotations : sidecar.istio.io/inject : \"true\" traffic.sidecar.istio.io/excludeOutboundPorts : \"4222\" # Nats JetStream port traffic.sidecar.istio.io/excludeInboundPorts : \"2469\" # Numaflow vertex metrics port udf : container : image : my-udf-image:latest ... If you want to apply same configuration to all the vertices, use Vertex Template : apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : vertex : metadata : annotations : sidecar.istio.io/inject : \"true\" traffic.sidecar.istio.io/excludeOutboundPorts : \"4222\" # Nats JetStream port traffic.sidecar.istio.io/excludeInboundPorts : \"2469\" # Numaflow vertex metrics port vertices : - name : my-vertex-1 udf : container : image : my-udf-1-image:latest - name : my-vertex-2 udf : container : image : my-udf-2-image:latest ...","title":"Running on Istio"},{"location":"user-guide/reference/configuration/istio/#running-on-istio","text":"If you want to get pipeline vertices running on Istio, so that they are able to talk to other services with Istio enabled, one approach is to whitelist the ports that Numaflow uses. To whitelist the ports, add traffic.sidecar.istio.io/excludeInboundPorts and traffic.sidecar.istio.io/excludeOutboundPorts annotations to your vertex configuration: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex metadata : annotations : sidecar.istio.io/inject : \"true\" traffic.sidecar.istio.io/excludeOutboundPorts : \"4222\" # Nats JetStream port traffic.sidecar.istio.io/excludeInboundPorts : \"2469\" # Numaflow vertex metrics port udf : container : image : my-udf-image:latest ... If you want to apply same configuration to all the vertices, use Vertex Template : apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : vertex : metadata : annotations : sidecar.istio.io/inject : \"true\" traffic.sidecar.istio.io/excludeOutboundPorts : \"4222\" # Nats JetStream port traffic.sidecar.istio.io/excludeInboundPorts : \"2469\" # Numaflow vertex metrics port vertices : - name : my-vertex-1 udf : container : image : my-udf-1-image:latest - name : my-vertex-2 udf : container : image : my-udf-2-image:latest ...","title":"Running on Istio"},{"location":"user-guide/reference/configuration/labels-and-annotations/","text":"Labels And Annotations \u00b6 Sometimes customized Labels or Annotations are needed for the vertices, for example, adding an annotation to enable or disable Istio sidecar injection. To do that, a metadata with labels or annotations can be added to the Pipeline Vertex or MonoVertex. A Pipeline example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex metadata : labels : key1 : val1 key2 : val2 annotations : key3 : val3 key4 : val4 A MonoVertex example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : metadata : labels : key1 : val1 key2 : val2 annotations : key3 : val3 key4 : val4","title":"Labels And Annotations"},{"location":"user-guide/reference/configuration/labels-and-annotations/#labels-and-annotations","text":"Sometimes customized Labels or Annotations are needed for the vertices, for example, adding an annotation to enable or disable Istio sidecar injection. To do that, a metadata with labels or annotations can be added to the Pipeline Vertex or MonoVertex. A Pipeline example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-vertex metadata : labels : key1 : val1 key2 : val2 annotations : key3 : val3 key4 : val4 A MonoVertex example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : metadata : labels : key1 : val1 key2 : val2 annotations : key3 : val3 key4 : val4","title":"Labels And Annotations"},{"location":"user-guide/reference/configuration/liveness-and-readiness/","text":"Liveness and Readiness \u00b6 Liveness and Readiness probes have been pre-configured in the pods orchestrated in Numaflow, including the containers of Vertex and MonoVertex pods. For these probes, the probe handlers are not allowed to be customized, but the other configurations are. initialDelaySeconds timeoutSeconds periodSeconds successThreshold failureThreshold Here is an example for Pipeline customization, similar configuration can be applied to containers including udf , udsource , transformer , udsink and fb-udsink . apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-source containerTemplate : # For \"numa\" container readinessProbe : initialDelaySeconds : 30 periodSeconds : 60 livenessProbe : initialDelaySeconds : 60 periodSeconds : 120 volumes : - name : my-udsource-config configMap : name : udsource-config source : udsource : container : image : my-source:latest volumeMounts : - mountPath : /path/to/my-source-config name : my-udsource-config # For User-Defined source livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 - name : my-udf containerTemplate : # For \"numa\" container readinessProbe : initialDelaySeconds : 20 periodSeconds : 60 livenessProbe : initialDelaySeconds : 180 periodSeconds : 60 timeoutSeconds : 50 volumes : - name : my-udf-config configMap : name : udf-config udf : container : image : my-function:latest volumeMounts : - mountPath : /path/to/my-function-config name : my-udf-config # For \"udf\" livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 The customization for numa container is also available with a Vertex Template defined in spec.templates.vertex , which is going to be applied to all the vertices of a pipeline. A MonoVertex example is as below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : simple-mono-vertex spec : containerTemplate : # For \"numa\" container readinessProbe : initialDelaySeconds : 20 periodSeconds : 60 livenessProbe : initialDelaySeconds : 180 periodSeconds : 60 source : udsource : container : image : quay.io/numaio/numaflow-java/source-simple-source:stable # For User-Defined source livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 timeoutSeconds : 40 transformer : container : image : quay.io/numaio/numaflow-rs/source-transformer-now:stable # For transformer livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 sink : udsink : container : image : quay.io/numaio/numaflow-java/simple-sink:stable # For User-Defined Sink livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 fallback : udsink : container : image : my-sink:latest # # For Fallback Sink livenessProbe : initialDelaySeconds : 40 failureThreshold : 5","title":"Liveness and Readiness"},{"location":"user-guide/reference/configuration/liveness-and-readiness/#liveness-and-readiness","text":"Liveness and Readiness probes have been pre-configured in the pods orchestrated in Numaflow, including the containers of Vertex and MonoVertex pods. For these probes, the probe handlers are not allowed to be customized, but the other configurations are. initialDelaySeconds timeoutSeconds periodSeconds successThreshold failureThreshold Here is an example for Pipeline customization, similar configuration can be applied to containers including udf , udsource , transformer , udsink and fb-udsink . apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-source containerTemplate : # For \"numa\" container readinessProbe : initialDelaySeconds : 30 periodSeconds : 60 livenessProbe : initialDelaySeconds : 60 periodSeconds : 120 volumes : - name : my-udsource-config configMap : name : udsource-config source : udsource : container : image : my-source:latest volumeMounts : - mountPath : /path/to/my-source-config name : my-udsource-config # For User-Defined source livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 - name : my-udf containerTemplate : # For \"numa\" container readinessProbe : initialDelaySeconds : 20 periodSeconds : 60 livenessProbe : initialDelaySeconds : 180 periodSeconds : 60 timeoutSeconds : 50 volumes : - name : my-udf-config configMap : name : udf-config udf : container : image : my-function:latest volumeMounts : - mountPath : /path/to/my-function-config name : my-udf-config # For \"udf\" livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 The customization for numa container is also available with a Vertex Template defined in spec.templates.vertex , which is going to be applied to all the vertices of a pipeline. A MonoVertex example is as below. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : simple-mono-vertex spec : containerTemplate : # For \"numa\" container readinessProbe : initialDelaySeconds : 20 periodSeconds : 60 livenessProbe : initialDelaySeconds : 180 periodSeconds : 60 source : udsource : container : image : quay.io/numaio/numaflow-java/source-simple-source:stable # For User-Defined source livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 timeoutSeconds : 40 transformer : container : image : quay.io/numaio/numaflow-rs/source-transformer-now:stable # For transformer livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 sink : udsink : container : image : quay.io/numaio/numaflow-java/simple-sink:stable # For User-Defined Sink livenessProbe : initialDelaySeconds : 40 failureThreshold : 5 fallback : udsink : container : image : my-sink:latest # # For Fallback Sink livenessProbe : initialDelaySeconds : 40 failureThreshold : 5","title":"Liveness and Readiness"},{"location":"user-guide/reference/configuration/max-message-size/","text":"Maximum Message Size \u00b6 The default maximum message size is 1MB . There's a way to increase this limit in case you want to, but please think it through before doing so. The safest action might be to enable compression . The max message size is determined by: Max messages size supported by gRPC (default value is 64MB in Numaflow). Max messages size supported by the Inter-Step Buffer implementation. If JetStream is used as the Inter-Step Buffer implementation, the default max message size for it is configured as 1MB . You can change it by setting the spec.jetstream.settings in the InterStepBufferService specification. apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : settings : | max_payload: 8388608 # 8MB It's not recommended to use values over 8388608 (8MB) but max_payload can be set up to 67108864 (64MB). Please be aware that if you increase the max message size of the InterStepBufferService , you probably will also need to change some other limits. For example, if the size of each messages is as large as 8MB, then 100 messages flowing in the pipeline will make each of the Inter-Step Buffer need at least 800MB of disk space to store the messages, and the memory consumption will also be high, that will probably cause the Inter-Step Buffer Service to crash. In that case, you might need to update the retention policy in the Inter-Step Buffer Service to make sure the messages are not stored for too long. Check out the Inter-Step Buffer Service for more details. Enable Compression \u00b6 Numaflow supports automatic compression while writing and reading the messages to and from the Inter-Step Buffer, this can help to reduce the storage and network cost to ISB. Enabling compression will help in ISB stability and should be used if the payload is large (e.g, > 1MB). This is transparent to the user-defined functions, compression and decompression is taken care by Numaflow before writing to the ISB and after reading from the ISB. Available compression types are: - none (default) - gzip - zstd - lz4 Performance Numbers \u00b6 The tests were run with fixed CPU 300m CPU using random 1KB payload. Compression Throughput (msg/s) Disk Usage by ISB (GB) None 1000 7 ~ 7.2 GZIP 132 1.2 ~ 1.4 ZSTD 900 4.5 ~ 4.7 LZ4 1000 2.8 ~ 3 Clearly the best compression (least disk usage) is gzip , but it has the lowest throughput. lz4 has the best throughput and zstd is in the middle. If you want to use gzip , you might need to increase the CPU of numa container to get better performance. Configuration \u00b6 You can enable it by setting the compression field in the Pipeline specification. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : interStepBuffer : compression : type : COMPRESSION_TYPE","title":"Maximum Message Size"},{"location":"user-guide/reference/configuration/max-message-size/#maximum-message-size","text":"The default maximum message size is 1MB . There's a way to increase this limit in case you want to, but please think it through before doing so. The safest action might be to enable compression . The max message size is determined by: Max messages size supported by gRPC (default value is 64MB in Numaflow). Max messages size supported by the Inter-Step Buffer implementation. If JetStream is used as the Inter-Step Buffer implementation, the default max message size for it is configured as 1MB . You can change it by setting the spec.jetstream.settings in the InterStepBufferService specification. apiVersion : numaflow.numaproj.io/v1alpha1 kind : InterStepBufferService metadata : name : default spec : jetstream : settings : | max_payload: 8388608 # 8MB It's not recommended to use values over 8388608 (8MB) but max_payload can be set up to 67108864 (64MB). Please be aware that if you increase the max message size of the InterStepBufferService , you probably will also need to change some other limits. For example, if the size of each messages is as large as 8MB, then 100 messages flowing in the pipeline will make each of the Inter-Step Buffer need at least 800MB of disk space to store the messages, and the memory consumption will also be high, that will probably cause the Inter-Step Buffer Service to crash. In that case, you might need to update the retention policy in the Inter-Step Buffer Service to make sure the messages are not stored for too long. Check out the Inter-Step Buffer Service for more details.","title":"Maximum Message Size"},{"location":"user-guide/reference/configuration/max-message-size/#enable-compression","text":"Numaflow supports automatic compression while writing and reading the messages to and from the Inter-Step Buffer, this can help to reduce the storage and network cost to ISB. Enabling compression will help in ISB stability and should be used if the payload is large (e.g, > 1MB). This is transparent to the user-defined functions, compression and decompression is taken care by Numaflow before writing to the ISB and after reading from the ISB. Available compression types are: - none (default) - gzip - zstd - lz4","title":"Enable Compression"},{"location":"user-guide/reference/configuration/max-message-size/#performance-numbers","text":"The tests were run with fixed CPU 300m CPU using random 1KB payload. Compression Throughput (msg/s) Disk Usage by ISB (GB) None 1000 7 ~ 7.2 GZIP 132 1.2 ~ 1.4 ZSTD 900 4.5 ~ 4.7 LZ4 1000 2.8 ~ 3 Clearly the best compression (least disk usage) is gzip , but it has the lowest throughput. lz4 has the best throughput and zstd is in the middle. If you want to use gzip , you might need to increase the CPU of numa container to get better performance.","title":"Performance Numbers"},{"location":"user-guide/reference/configuration/max-message-size/#configuration","text":"You can enable it by setting the compression field in the Pipeline specification. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : interStepBuffer : compression : type : COMPRESSION_TYPE","title":"Configuration"},{"location":"user-guide/reference/configuration/pipeline-customization/","text":"Pipeline Customization \u00b6 There is an optional .spec.templates field in the Pipeline resource which may be used to customize kubernetes resources owned by the Pipeline. Vertex customization is described separately in more detail (i.e. Environment Variables , Container Resources , etc.). Daemon Deployment \u00b6 The following example shows how to configure a Daemon Deployment with all currently supported fields. The .spec.templates.daemon field and all fields directly under it are optional. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : daemon : # Deployment spec replicas : 3 # Pod metadata metadata : labels : my-label-name : my-label-value annotations : my-annotation-name : my-annotation-value # Pod spec nodeSelector : my-node-label-name : my-node-label-value tolerations : - key : \"my-example-key\" operator : \"Exists\" effect : \"NoSchedule\" securityContext : {} imagePullSecrets : - name : regcred priorityClassName : my-priority-class-name priority : 50 serviceAccountName : my-service-account resourceClaims : - name : my-claim xxx affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : app.kubernetes.io/component operator : In values : - daemon - key : numaflow.numaproj.io/pipeline-name operator : In values : - my-pipeline topologyKey : kubernetes.io/hostname # Containers containerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi claims : - name : my-claim initContainerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi Jobs \u00b6 The following example shows how to configure kubernetes Jobs owned by a Pipeline with all currently supported fields. The .spec.templates.job field and all fields directly under it are optional. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : job : # Job spec ttlSecondsAfterFinished : 600 # numaflow defaults to 30 backoffLimit : 5 # numaflow defaults to 20 # Pod metadata metadata : labels : my-label-name : my-label-value annotations : my-annotation-name : my-annotation-value # Pod spec nodeSelector : my-node-label-name : my-node-label-value tolerations : - key : \"my-example-key\" operator : \"Exists\" effect : \"NoSchedule\" securityContext : {} imagePullSecrets : - name : regcred priorityClassName : my-priority-class-name priority : 50 serviceAccountName : my-service-account affinity : {} # Container containerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi Vertices \u00b6 The following example shows how to configure the all the vertex pods owned by a pipeline with all currently supported fields. Be aware these configurations applied to all vertex pods can be overridden by the vertex configuration. The .spec.templates.vertex field and all fields directly under it are optional. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : vertex : # Pod metadata metadata : labels : my-label-name : my-label-value annotations : my-annotation-name : my-annotation-value nodeSelector : my-node-label-name : my-node-label-value tolerations : - key : \"my-example-key\" operator : \"Exists\" effect : \"NoSchedule\" securityContext : {} imagePullSecrets : - name : regcred priorityClassName : my-priority-class-name priority : 50 serviceAccountName : my-service-account affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : numaflow.numaproj.io/pipeline-name operator : In values : - my-pipeline topologyKey : kubernetes.io/hostname # Containers containerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi initContainerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi Side Inputs \u00b6 The following example shows how to configure the all the Side Inputs Manager pods owned by a pipeline with all currently supported fields. The .spec.templates.sideInputsManager field and all fields directly under it are optional. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : sideInputsManager : # Pod metadata metadata : labels : my-label-name : my-label-value annotations : my-annotation-name : my-annotation-value # Pod spec nodeSelector : my-node-label-name : my-node-label-value tolerations : - key : \"my-example-key\" operator : \"Exists\" effect : \"NoSchedule\" securityContext : {} imagePullSecrets : - name : regcred priorityClassName : my-priority-class-name priority : 50 serviceAccountName : my-service-account resourceClaims : - name : my-claim xxx affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : numaflow.numaproj.io/pipeline-name operator : In values : - my-pipeline topologyKey : kubernetes.io/hostname # Containers containerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi claims : - name : my-claim initContainerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi","title":"Pipeline Customization"},{"location":"user-guide/reference/configuration/pipeline-customization/#pipeline-customization","text":"There is an optional .spec.templates field in the Pipeline resource which may be used to customize kubernetes resources owned by the Pipeline. Vertex customization is described separately in more detail (i.e. Environment Variables , Container Resources , etc.).","title":"Pipeline Customization"},{"location":"user-guide/reference/configuration/pipeline-customization/#daemon-deployment","text":"The following example shows how to configure a Daemon Deployment with all currently supported fields. The .spec.templates.daemon field and all fields directly under it are optional. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : daemon : # Deployment spec replicas : 3 # Pod metadata metadata : labels : my-label-name : my-label-value annotations : my-annotation-name : my-annotation-value # Pod spec nodeSelector : my-node-label-name : my-node-label-value tolerations : - key : \"my-example-key\" operator : \"Exists\" effect : \"NoSchedule\" securityContext : {} imagePullSecrets : - name : regcred priorityClassName : my-priority-class-name priority : 50 serviceAccountName : my-service-account resourceClaims : - name : my-claim xxx affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : app.kubernetes.io/component operator : In values : - daemon - key : numaflow.numaproj.io/pipeline-name operator : In values : - my-pipeline topologyKey : kubernetes.io/hostname # Containers containerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi claims : - name : my-claim initContainerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi","title":"Daemon Deployment"},{"location":"user-guide/reference/configuration/pipeline-customization/#jobs","text":"The following example shows how to configure kubernetes Jobs owned by a Pipeline with all currently supported fields. The .spec.templates.job field and all fields directly under it are optional. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : job : # Job spec ttlSecondsAfterFinished : 600 # numaflow defaults to 30 backoffLimit : 5 # numaflow defaults to 20 # Pod metadata metadata : labels : my-label-name : my-label-value annotations : my-annotation-name : my-annotation-value # Pod spec nodeSelector : my-node-label-name : my-node-label-value tolerations : - key : \"my-example-key\" operator : \"Exists\" effect : \"NoSchedule\" securityContext : {} imagePullSecrets : - name : regcred priorityClassName : my-priority-class-name priority : 50 serviceAccountName : my-service-account affinity : {} # Container containerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi","title":"Jobs"},{"location":"user-guide/reference/configuration/pipeline-customization/#vertices","text":"The following example shows how to configure the all the vertex pods owned by a pipeline with all currently supported fields. Be aware these configurations applied to all vertex pods can be overridden by the vertex configuration. The .spec.templates.vertex field and all fields directly under it are optional. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : vertex : # Pod metadata metadata : labels : my-label-name : my-label-value annotations : my-annotation-name : my-annotation-value nodeSelector : my-node-label-name : my-node-label-value tolerations : - key : \"my-example-key\" operator : \"Exists\" effect : \"NoSchedule\" securityContext : {} imagePullSecrets : - name : regcred priorityClassName : my-priority-class-name priority : 50 serviceAccountName : my-service-account affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : numaflow.numaproj.io/pipeline-name operator : In values : - my-pipeline topologyKey : kubernetes.io/hostname # Containers containerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi initContainerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi","title":"Vertices"},{"location":"user-guide/reference/configuration/pipeline-customization/#side-inputs","text":"The following example shows how to configure the all the Side Inputs Manager pods owned by a pipeline with all currently supported fields. The .spec.templates.sideInputsManager field and all fields directly under it are optional. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : templates : sideInputsManager : # Pod metadata metadata : labels : my-label-name : my-label-value annotations : my-annotation-name : my-annotation-value # Pod spec nodeSelector : my-node-label-name : my-node-label-value tolerations : - key : \"my-example-key\" operator : \"Exists\" effect : \"NoSchedule\" securityContext : {} imagePullSecrets : - name : regcred priorityClassName : my-priority-class-name priority : 50 serviceAccountName : my-service-account resourceClaims : - name : my-claim xxx affinity : podAntiAffinity : requiredDuringSchedulingIgnoredDuringExecution : - labelSelector : matchExpressions : - key : numaflow.numaproj.io/pipeline-name operator : In values : - my-pipeline topologyKey : kubernetes.io/hostname # Containers containerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi claims : - name : my-claim initContainerTemplate : env : - name : MY_ENV_NAME value : my-env-value resources : limits : memory : 500Mi","title":"Side Inputs"},{"location":"user-guide/reference/configuration/pod-specifications/","text":"Pod Specifications \u00b6 Most of the Kunernetes Pod specification fields are supported in the spec of Pipeline , MonoVertex and InterStepBufferService . Those fields include: nodeSelector tolerations securityContext imagePullSecrets priorityClassName priority affinity serviceAccountName runtimeClassName automountServiceAccountToken dnsPolicy dnsConfig resourceClaims All the fields above are optional, click here to see full list of supported fields. These fields can be specified in the Pipeline spec under: spec.vertices[*] spec.templates.daemon spec.templates.job spec.templates.sideInputsManager spec.templates.vertex Or in the MonoVertex spec under: spec spec.daemonTemplate Or in InterStepBufferService spec at: spec.jetstream","title":"Pod Specifications"},{"location":"user-guide/reference/configuration/pod-specifications/#pod-specifications","text":"Most of the Kunernetes Pod specification fields are supported in the spec of Pipeline , MonoVertex and InterStepBufferService . Those fields include: nodeSelector tolerations securityContext imagePullSecrets priorityClassName priority affinity serviceAccountName runtimeClassName automountServiceAccountToken dnsPolicy dnsConfig resourceClaims All the fields above are optional, click here to see full list of supported fields. These fields can be specified in the Pipeline spec under: spec.vertices[*] spec.templates.daemon spec.templates.job spec.templates.sideInputsManager spec.templates.vertex Or in the MonoVertex spec under: spec spec.daemonTemplate Or in InterStepBufferService spec at: spec.jetstream","title":"Pod Specifications"},{"location":"user-guide/reference/configuration/sidecar-containers/","text":"Sidecar Containers \u00b6 Additional \" sidecar \" containers can be provided for source , udf and sink vertices. The following example shows how to add a sidecar container to a udf vertex. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf sidecars : - name : my-sidecar image : busybox:latest command : [ \"/bin/sh\" , \"-c\" , 'echo \"my-sidecar is running!\" && tail -f /dev/null' , ] udf : container : image : my-function:latest There are various use-cases for sidecars. One possible use-case is a udf container that needs functionality from a library written in a different language. The library's functionality could be made available through gRPC over Unix Domain Socket. The following example shows how that could be accomplished using a shared volume . It is the sidecar owner's responsibility to come up with a protocol that can be used with the UDF. It could be volume, gRPC, TCP, HTTP 1.x, etc., apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf-vertex volumes : - name : my-udf-volume emptyDir : medium : Memory sidecars : - name : my-sidecar image : alpine:latest command : [ \"/bin/sh\" , \"-c\" , \"apk add socat && socat UNIX-LISTEN:/path/to/my-sidecar-mount-path/my.sock - && tail -f /dev/null\" , ] volumeMounts : - mountPath : /path/to/my-sidecar-mount-path name : my-udf-volume udf : container : image : alpine:latest command : [ \"/bin/sh\" , \"-c\" , 'apk add socat && echo \"hello\" | socat UNIX-CONNECT:/path/to/my-udf-mount-path/my.sock,forever - && tail -f /dev/null' , ] volumeMounts : - mountPath : /path/to/my-udf-mount-path name : my-udf-volume","title":"Sidecar Containers"},{"location":"user-guide/reference/configuration/sidecar-containers/#sidecar-containers","text":"Additional \" sidecar \" containers can be provided for source , udf and sink vertices. The following example shows how to add a sidecar container to a udf vertex. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf sidecars : - name : my-sidecar image : busybox:latest command : [ \"/bin/sh\" , \"-c\" , 'echo \"my-sidecar is running!\" && tail -f /dev/null' , ] udf : container : image : my-function:latest There are various use-cases for sidecars. One possible use-case is a udf container that needs functionality from a library written in a different language. The library's functionality could be made available through gRPC over Unix Domain Socket. The following example shows how that could be accomplished using a shared volume . It is the sidecar owner's responsibility to come up with a protocol that can be used with the UDF. It could be volume, gRPC, TCP, HTTP 1.x, etc., apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-udf-vertex volumes : - name : my-udf-volume emptyDir : medium : Memory sidecars : - name : my-sidecar image : alpine:latest command : [ \"/bin/sh\" , \"-c\" , \"apk add socat && socat UNIX-LISTEN:/path/to/my-sidecar-mount-path/my.sock - && tail -f /dev/null\" , ] volumeMounts : - mountPath : /path/to/my-sidecar-mount-path name : my-udf-volume udf : container : image : alpine:latest command : [ \"/bin/sh\" , \"-c\" , 'apk add socat && echo \"hello\" | socat UNIX-CONNECT:/path/to/my-udf-mount-path/my.sock,forever - && tail -f /dev/null' , ] volumeMounts : - mountPath : /path/to/my-udf-mount-path name : my-udf-volume","title":"Sidecar Containers"},{"location":"user-guide/reference/configuration/update-strategy/","text":"Update Strategy \u00b6 When spec changes, the RollingUpdate update strategy is used to update pods in a Pipeline or MonoVertex by default, which means that the update is done in a rolling fashion. The default configuration is as below. updateStrategy : rollingUpdate : maxUnavailable : 25% type : RollingUpdate maxUnavailable : The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5 ) or a percentage of total pods at the start of update (ex: 10% ). Absolute number is calculated from percentage by rounding up. Defaults to 25% . How It Works \u00b6 The RollingUpdate strategy in Numaflow works more like the RollingUpdate strategy in StatefulSet rather than Deployment . It does not create maxUnavailable new pods and wait for them to be ready before terminating the old pods. Instead it replaces maxUnavailable number of pods with the new spec, then waits for them to be ready before updating the next batch. For example, if there are 20 pods running, and maxUnavailable is set to the default 25% , during the update, 5 pods will be unavailable at the same time. The update will be done in 4 batches. If your application has a long startup time, and you are sensitive to the unavailability caused tail latency, you should set maxUnavailable to a smaller value, and adjust the scale.min if it's needed. During rolling update, autoscaling will not be triggered for that particular Vertex or MonoVertex. Examples \u00b6 A Pipeline example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : my-vertex updateStrategy : rollingUpdate : maxUnavailable : 25% type : RollingUpdate A MonoVertex example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : source : udsource : container : image : my-image1 sink : udsink : container : image : my-image2 updateStrategy : rollingUpdate : maxUnavailable : 2 type : RollingUpdate","title":"Update Strategy"},{"location":"user-guide/reference/configuration/update-strategy/#update-strategy","text":"When spec changes, the RollingUpdate update strategy is used to update pods in a Pipeline or MonoVertex by default, which means that the update is done in a rolling fashion. The default configuration is as below. updateStrategy : rollingUpdate : maxUnavailable : 25% type : RollingUpdate maxUnavailable : The maximum number of pods that can be unavailable during the update. Value can be an absolute number (ex: 5 ) or a percentage of total pods at the start of update (ex: 10% ). Absolute number is calculated from percentage by rounding up. Defaults to 25% .","title":"Update Strategy"},{"location":"user-guide/reference/configuration/update-strategy/#how-it-works","text":"The RollingUpdate strategy in Numaflow works more like the RollingUpdate strategy in StatefulSet rather than Deployment . It does not create maxUnavailable new pods and wait for them to be ready before terminating the old pods. Instead it replaces maxUnavailable number of pods with the new spec, then waits for them to be ready before updating the next batch. For example, if there are 20 pods running, and maxUnavailable is set to the default 25% , during the update, 5 pods will be unavailable at the same time. The update will be done in 4 batches. If your application has a long startup time, and you are sensitive to the unavailability caused tail latency, you should set maxUnavailable to a smaller value, and adjust the scale.min if it's needed. During rolling update, autoscaling will not be triggered for that particular Vertex or MonoVertex.","title":"How It Works"},{"location":"user-guide/reference/configuration/update-strategy/#examples","text":"A Pipeline example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : my-vertex updateStrategy : rollingUpdate : maxUnavailable : 25% type : RollingUpdate A MonoVertex example. apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : my-mvtx spec : source : udsource : container : image : my-image1 sink : udsink : container : image : my-image2 updateStrategy : rollingUpdate : maxUnavailable : 2 type : RollingUpdate","title":"Examples"},{"location":"user-guide/reference/configuration/volumes/","text":"Volumes \u00b6 Volumes can be mounted to udsource , udf or udsink containers. Following example shows how to mount a ConfigMap to an udsource vertex, an udf vertex and an udsink vertex. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-source volumes : - name : my-udsource-config configMap : name : udsource-config source : udsource : container : image : my-source:latest volumeMounts : - mountPath : /path/to/my-source-config name : my-udsource-config - name : my-udf volumes : - name : my-udf-config configMap : name : udf-config udf : container : image : my-function:latest volumeMounts : - mountPath : /path/to/my-function-config name : my-udf-config - name : my-sink volumes : - name : my-udsink-config configMap : name : udsink-config sink : udsink : container : image : my-sink:latest volumeMounts : - mountPath : /path/to/my-sink-config name : my-udsink-config PVC Example \u00b6 Example to show how to attach a Persistent Volume Claim (PVC) to a container. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-source volumes : - name : mypd persistentVolumeClaim : claimName : myclaim source : udsource : container : image : my-source:latest volumeMounts : - mountPath : /path/to/my-source-config name : mypd","title":"Volumes"},{"location":"user-guide/reference/configuration/volumes/#volumes","text":"Volumes can be mounted to udsource , udf or udsink containers. Following example shows how to mount a ConfigMap to an udsource vertex, an udf vertex and an udsink vertex. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-source volumes : - name : my-udsource-config configMap : name : udsource-config source : udsource : container : image : my-source:latest volumeMounts : - mountPath : /path/to/my-source-config name : my-udsource-config - name : my-udf volumes : - name : my-udf-config configMap : name : udf-config udf : container : image : my-function:latest volumeMounts : - mountPath : /path/to/my-function-config name : my-udf-config - name : my-sink volumes : - name : my-udsink-config configMap : name : udsink-config sink : udsink : container : image : my-sink:latest volumeMounts : - mountPath : /path/to/my-sink-config name : my-udsink-config","title":"Volumes"},{"location":"user-guide/reference/configuration/volumes/#pvc-example","text":"Example to show how to attach a Persistent Volume Claim (PVC) to a container. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : my-source volumes : - name : mypd persistentVolumeClaim : claimName : myclaim source : udsource : container : image : my-source:latest volumeMounts : - mountPath : /path/to/my-source-config name : mypd","title":"PVC Example"},{"location":"user-guide/reference/kustomize/kustomize/","text":"Kustomize Integration \u00b6 Transformers \u00b6 Kustomize Transformer Configurations can be used to do lots of powerful operations such as ConfigMap and Secret generations, applying common labels and annotations, updating image names and tags. To use these features with Numaflow CRD objects, download numaflow-transformer-config.yaml into your kustomize directory, and add it to configurations section. kind : Kustomization apiVersion : kustomize.config.k8s.io/v1beta1 configurations : - numaflow-transformer-config.yaml # Or reference the remote configuration directly. # - https://raw.githubusercontent.com/numaproj/numaflow/main/docs/user-guide/reference/kustomize/numaflow-transformer-config.yaml Here is an example to use transformers with a Pipeline. Patch \u00b6 Starting from version 4.5.5, kustomize can use Kubernetes OpenAPI schema to provide merge key and patch strategy information. To use that with Numaflow CRD objects, download schema.json into your kustomize directory, and add it to openapi section. apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization openapi : path : schema.json # Or reference the remote configuration directly. # path: https://raw.githubusercontent.com/numaproj/numaflow/main/api/json-schema/schema.json For example, given the following Pipeline spec: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : in source : generator : rpu : 5 duration : 1s - name : my-udf udf : container : image : my-pipeline/my-udf:v0.1 - name : out sink : log : {} edges : - from : in to : my-udf - from : my-udf to : out You can update the source spec via a patch in a kustomize file. apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - my-pipeline.yaml openapi : path : https://raw.githubusercontent.com/numaproj/numaflow/main/api/json-schema/schema.json patches : - patch : |- apiVersion: numaflow.numaproj.io/v1alpha1 kind: Pipeline metadata: name: my-pipeline spec: vertices: - name: in source: generator: rpu: 500 See the full example here .","title":"Kustomize Integration"},{"location":"user-guide/reference/kustomize/kustomize/#kustomize-integration","text":"","title":"Kustomize Integration"},{"location":"user-guide/reference/kustomize/kustomize/#transformers","text":"Kustomize Transformer Configurations can be used to do lots of powerful operations such as ConfigMap and Secret generations, applying common labels and annotations, updating image names and tags. To use these features with Numaflow CRD objects, download numaflow-transformer-config.yaml into your kustomize directory, and add it to configurations section. kind : Kustomization apiVersion : kustomize.config.k8s.io/v1beta1 configurations : - numaflow-transformer-config.yaml # Or reference the remote configuration directly. # - https://raw.githubusercontent.com/numaproj/numaflow/main/docs/user-guide/reference/kustomize/numaflow-transformer-config.yaml Here is an example to use transformers with a Pipeline.","title":"Transformers"},{"location":"user-guide/reference/kustomize/kustomize/#patch","text":"Starting from version 4.5.5, kustomize can use Kubernetes OpenAPI schema to provide merge key and patch strategy information. To use that with Numaflow CRD objects, download schema.json into your kustomize directory, and add it to openapi section. apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization openapi : path : schema.json # Or reference the remote configuration directly. # path: https://raw.githubusercontent.com/numaproj/numaflow/main/api/json-schema/schema.json For example, given the following Pipeline spec: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : my-pipeline spec : vertices : - name : in source : generator : rpu : 5 duration : 1s - name : my-udf udf : container : image : my-pipeline/my-udf:v0.1 - name : out sink : log : {} edges : - from : in to : my-udf - from : my-udf to : out You can update the source spec via a patch in a kustomize file. apiVersion : kustomize.config.k8s.io/v1beta1 kind : Kustomization resources : - my-pipeline.yaml openapi : path : https://raw.githubusercontent.com/numaproj/numaflow/main/api/json-schema/schema.json patches : - patch : |- apiVersion: numaflow.numaproj.io/v1alpha1 kind: Pipeline metadata: name: my-pipeline spec: vertices: - name: in source: generator: rpu: 500 See the full example here .","title":"Patch"},{"location":"user-guide/sdks/compatibility/","text":"SDK Compatibility \u00b6 SDK and Numaflow compatibility version matrix. Please refer to features matrix in SDK Features for the features supported by each SDK version. Numaflow ensures at least one version backward compatibility for SDKs, i.e., SDKs are expected to work with the current and previous version of Numaflow. To upgrade SDKs, first upgrade Numaflow to the latest version supported by the SDK, then upgrade the SDK. NOTE : If your Numaflow version (and your SDK) are quite old, to upgrade to the latest SDK, you may need to upgrade Numaflow (and SDK) multiple times to reach the latest version. (\u25cf - Released, \u25cb - Future Release) Python \u00b6 Python \\ Numaflow v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.0 v1.0.0 v0.10.0 \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf Golang \u00b6 Golang \\ Numaflow v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.0 v1.0.0 v0.10.0 \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf Java \u00b6 Java \\ Numaflow v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.0 v1.0.0 v0.10.0 \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf Rust \u00b6 Rust \\ Numaflow v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.0 v1.0.0 0.3.0 (next) \u25cb \u25cb 0.2.0 \u25cf \u25cf \u25cf 0.1.0 \u25cf","title":"Compatibility"},{"location":"user-guide/sdks/compatibility/#sdk-compatibility","text":"SDK and Numaflow compatibility version matrix. Please refer to features matrix in SDK Features for the features supported by each SDK version. Numaflow ensures at least one version backward compatibility for SDKs, i.e., SDKs are expected to work with the current and previous version of Numaflow. To upgrade SDKs, first upgrade Numaflow to the latest version supported by the SDK, then upgrade the SDK. NOTE : If your Numaflow version (and your SDK) are quite old, to upgrade to the latest SDK, you may need to upgrade Numaflow (and SDK) multiple times to reach the latest version. (\u25cf - Released, \u25cb - Future Release)","title":"SDK Compatibility"},{"location":"user-guide/sdks/compatibility/#python","text":"Python \\ Numaflow v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.0 v1.0.0 v0.10.0 \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf","title":"Python"},{"location":"user-guide/sdks/compatibility/#golang","text":"Golang \\ Numaflow v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.0 v1.0.0 v0.10.0 \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf","title":"Golang"},{"location":"user-guide/sdks/compatibility/#java","text":"Java \\ Numaflow v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.0 v1.0.0 v0.10.0 \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf","title":"Java"},{"location":"user-guide/sdks/compatibility/#rust","text":"Rust \\ Numaflow v1.6.0 v1.5.0 v1.4.0 v1.3.0 v1.2.0 v1.1.0 v1.0.0 0.3.0 (next) \u25cb \u25cb 0.2.0 \u25cf \u25cf \u25cf 0.1.0 \u25cf","title":"Rust"},{"location":"user-guide/sdks/features/","text":"SDK Features \u00b6 The features supported by SDKs. These features are about whether each SDK support writing User Defined Functions. Please refer to the SDK Compatibility for SDK and Numaflow compatibility. (\u25cf - Released, \u25cb - Future Release) Python \u00b6 Source Transformer Map Batch Map Streaming Map Reduce Reduce Streaming Reduce Sessions Accumulator Sink v0.10.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf Golang \u00b6 Source Transformer Map Batch Map Streaming Map Reduce Reduce Streaming Reduce Sessions Accumulator Sink v0.10.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf Java \u00b6 Source Transformer Map Batch Map Streaming Map Reduce Reduce Streaming Reduce Sessions Accumulator Sink v0.10.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf Rust \u00b6 Source Transformer Map Batch Map Streaming Map Reduce Reduce Streaming Reduce Sessions Accumulator Sink 0.3.0 (next) \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cb \u25cb \u25cf 0.2.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf 0.1.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf","title":"Features"},{"location":"user-guide/sdks/features/#sdk-features","text":"The features supported by SDKs. These features are about whether each SDK support writing User Defined Functions. Please refer to the SDK Compatibility for SDK and Numaflow compatibility. (\u25cf - Released, \u25cb - Future Release)","title":"SDK Features"},{"location":"user-guide/sdks/features/#python","text":"Source Transformer Map Batch Map Streaming Map Reduce Reduce Streaming Reduce Sessions Accumulator Sink v0.10.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf","title":"Python"},{"location":"user-guide/sdks/features/#golang","text":"Source Transformer Map Batch Map Streaming Map Reduce Reduce Streaming Reduce Sessions Accumulator Sink v0.10.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf","title":"Golang"},{"location":"user-guide/sdks/features/#java","text":"Source Transformer Map Batch Map Streaming Map Reduce Reduce Streaming Reduce Sessions Accumulator Sink v0.10.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.9.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf v0.8.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf","title":"Java"},{"location":"user-guide/sdks/features/#rust","text":"Source Transformer Map Batch Map Streaming Map Reduce Reduce Streaming Reduce Sessions Accumulator Sink 0.3.0 (next) \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cb \u25cb \u25cf 0.2.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf 0.1.0 \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf \u25cf","title":"Rust"},{"location":"user-guide/sdks/overview/","text":"Numaflow SDKs \u00b6 Numaflow is language-agnostic, letting you develop event-driven applications, stream processing, and serving pipelines in the language of your choice. From source to transformer to sink \u2014 each component in your pipeline can be written in any supported language, so your developers can focus on what they\u2019re comfortable with. Please refer to the SDK Compatibility and SDK Features for more details. Available SDKs: Java SDK Go SDK Python SDK Rust SDK","title":"Overview"},{"location":"user-guide/sdks/overview/#numaflow-sdks","text":"Numaflow is language-agnostic, letting you develop event-driven applications, stream processing, and serving pipelines in the language of your choice. From source to transformer to sink \u2014 each component in your pipeline can be written in any supported language, so your developers can focus on what they\u2019re comfortable with. Please refer to the SDK Compatibility and SDK Features for more details. Available SDKs: Java SDK Go SDK Python SDK Rust SDK","title":"Numaflow SDKs"},{"location":"user-guide/sinks/blackhole/","text":"Blackhole Sink \u00b6 A Blackhole sink is where the output is drained without writing to any sink, it is to emulate /dev/null . spec : vertices : - name : output sink : blackhole : {} NOTE: The previous vertex should ideally be not forwarding the message to make it more efficient to avoid network latency.","title":"Blackhole Sink"},{"location":"user-guide/sinks/blackhole/#blackhole-sink","text":"A Blackhole sink is where the output is drained without writing to any sink, it is to emulate /dev/null . spec : vertices : - name : output sink : blackhole : {} NOTE: The previous vertex should ideally be not forwarding the message to make it more efficient to avoid network latency.","title":"Blackhole Sink"},{"location":"user-guide/sinks/fallback/","text":"Fallback Sink \u00b6 A Fallback Sink functions as a Dead Letter Queue (DLQ) Sink. It can be configured to serve as a backup sink when the primary sink fails processing messages. The Use Case \u00b6 Fallback Sink is useful to prevent back pressures caused by failed messages in the primary sink. In a pipeline without fallback sinks, if a sink fails to process certain messages, the failed messages, by default, can get retried indefinitely, causing back pressures propagated all the way back to the source vertex. Eventually, the pipeline will be blocked, and no new messages will be processed. A fallback sink can be set up to prevent this from happening, by storing the failed messages in a separate sink. Caveats \u00b6 A fallback sink can only be configured when the primary sink is a user-defined sink. How to use \u00b6 To configure a fallback sink, changes need to be made on both the pipeline specification and the user-defined sink implementation. Step 1 - update the specification \u00b6 Add a fallback field to the sink configuration in the pipeline specification file. The following example uses the builtin kafka as a fallback sink. - name : out sink : udsink : container : image : my-sink:latest fallback : kafka : brokers : - my-broker1:19700 - my-broker2:19700 topic : my-topic A fallback sink can also be a user-defined sink. - name : out sink : udsink : container : image : my-sink:latest fallback : udsink : container : image : my-sink:latest Step 2 - update the user-defined sink implementation \u00b6 Code changes have to be made in the primary sink to generate either a failed response or a fallback response, based on the use case. a failed response gets processed following the retry strategy , and if the retry strategy is set to fallback , the message will be directed to the fallback sink after the retries are exhausted. a fallback response doesn't respect the sink retry strategy. It gets immediately directed to the fallback sink without getting retried. SDK methods to generate either a fallback or a failed response in a primary user-defined sink can be found here: Golang , Java , Python","title":"Fallback Sink"},{"location":"user-guide/sinks/fallback/#fallback-sink","text":"A Fallback Sink functions as a Dead Letter Queue (DLQ) Sink. It can be configured to serve as a backup sink when the primary sink fails processing messages.","title":"Fallback Sink"},{"location":"user-guide/sinks/fallback/#the-use-case","text":"Fallback Sink is useful to prevent back pressures caused by failed messages in the primary sink. In a pipeline without fallback sinks, if a sink fails to process certain messages, the failed messages, by default, can get retried indefinitely, causing back pressures propagated all the way back to the source vertex. Eventually, the pipeline will be blocked, and no new messages will be processed. A fallback sink can be set up to prevent this from happening, by storing the failed messages in a separate sink.","title":"The Use Case"},{"location":"user-guide/sinks/fallback/#caveats","text":"A fallback sink can only be configured when the primary sink is a user-defined sink.","title":"Caveats"},{"location":"user-guide/sinks/fallback/#how-to-use","text":"To configure a fallback sink, changes need to be made on both the pipeline specification and the user-defined sink implementation.","title":"How to use"},{"location":"user-guide/sinks/fallback/#step-1-update-the-specification","text":"Add a fallback field to the sink configuration in the pipeline specification file. The following example uses the builtin kafka as a fallback sink. - name : out sink : udsink : container : image : my-sink:latest fallback : kafka : brokers : - my-broker1:19700 - my-broker2:19700 topic : my-topic A fallback sink can also be a user-defined sink. - name : out sink : udsink : container : image : my-sink:latest fallback : udsink : container : image : my-sink:latest","title":"Step 1 - update the specification"},{"location":"user-guide/sinks/fallback/#step-2-update-the-user-defined-sink-implementation","text":"Code changes have to be made in the primary sink to generate either a failed response or a fallback response, based on the use case. a failed response gets processed following the retry strategy , and if the retry strategy is set to fallback , the message will be directed to the fallback sink after the retries are exhausted. a fallback response doesn't respect the sink retry strategy. It gets immediately directed to the fallback sink without getting retried. SDK methods to generate either a fallback or a failed response in a primary user-defined sink can be found here: Golang , Java , Python","title":"Step 2 - update the user-defined sink implementation"},{"location":"user-guide/sinks/kafka/","text":"Kafka Sink \u00b6 Two methods are available for integrating Kafka topics into your Numaflow pipeline: using a user-defined Kafka Sink or opting for the built-in Kafka Sink provided by Numaflow. Option 1: User-Defined Kafka Sink \u00b6 Developed and maintained by the Numaflow contributor community, the Kafka Sink provides a reliable and feature-complete solution for publishing messages to Kafka topics. Key Features: Customization: Offers complete control over Kafka Sink configurations to tailor to specific requirements. Kafka Java Client Utilization: Leverages the Kafka Java client for reliable message publishing to Kafka topics. Schema Management: Integrates seamlessly with the Confluent Schema Registry to support schema validation and manage schema evolution effectively. More details on how to use the Kafka Sink can be found here . Option 2: Built-in Kafka Sink \u00b6 A Kafka sink is used to forward the messages to a Kafka topic. Kafka sink supports configuration overrides. Kafka Headers \u00b6 We will insert keys into the Kafka header, but since keys is an array, we will add keys into the header in the following format. __keys_len will have the number of key in the header. if __keys_len == 0 , means no keys are present. __keys_%d will have the key , e.g., __key_0 will be the first key, and so forth. Example \u00b6 spec : vertices : - name : kafka-output sink : kafka : brokers : - my-broker1:19700 - my-broker2:19700 topic : my-topic tls : # Optional. insecureSkipVerify : # Optional, where to skip TLS verification. Default to false. caCertSecret : # Optional, a secret reference, which contains the CA Cert. name : my-ca-cert key : my-ca-cert-key certSecret : # Optional, pointing to a secret reference which contains the Cert. name : my-cert key : my-cert-key keySecret : # Optional, pointing to a secret reference which contains the Private Key. name : my-pk key : my-pk-key sasl : # Optional mechanism : GSSAPI # PLAIN, GSSAPI, OAUTHBEARER, SCRAM-SHA-256 or SCRAM-SHA-512 other mechanisms not supported gssapi : # Optional, for GSSAPI mechanism serviceName : my-service realm : my-realm # KRB5_USER_AUTH for auth using password # KRB5_KEYTAB_AUTH for auth using keytab authType : KRB5_KEYTAB_AUTH usernameSecret : # Pointing to a secret reference which contains the username name : gssapi-username key : gssapi-username-key # Pointing to a secret reference which contains the keytab (authType: KRB5_KEYTAB_AUTH) keytabSecret : name : gssapi-keytab key : gssapi-keytab-key # Pointing to a secret reference which contains the keytab (authType: KRB5_USER_AUTH) passwordSecret : name : gssapi-password key : gssapi-password-key kerberosConfigSecret : # Pointing to a secret reference which contains the kerberos config name : my-kerberos-config key : my-kerberos-config-key plain : # Optional, for PLAIN mechanism userSecret : # Pointing to a secret reference which contains the user name : plain-user key : plain-user-key passwordSecret : # Pointing to a secret reference which contains the password name : plain-password key : plain-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true scramsha256 : # Optional, for SCRAM-SHA-256 mechanism userSecret : # Pointing to a secret reference which contains the user name : scram-sha-256-user key : scram-sha-256-user-key passwordSecret : # Pointing to a secret reference which contains the password name : scram-sha-256-password key : scram-sha-256-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true scramsha512 : # Optional, for SCRAM-SHA-512 mechanism userSecret : # Pointing to a secret reference which contains the user name : scram-sha-512-user key : scram-sha-512-user-key passwordSecret : # Pointing to a secret reference which contains the password name : scram-sha-512-password key : scram-sha-512-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true oauth : #Optional, for OAUTHBEARER mechanism clientID : # Pointing to a secret reference which contains the client id name : kafka-oauth-client key : clientid clientSecret : # Pointing to a secret reference which contains the client secret name : kafka-oauth-client key : clientsecret tokenEndpoint : https://oauth-token.com/v1/token # Optional, a yaml format string which could apply more configuration for the sink. # The configuration hierarchy follows the Struct of sarama.Config at https://github.com/IBM/sarama/blob/main/config.go. config : | producer: compression: 2","title":"Kafka Sink"},{"location":"user-guide/sinks/kafka/#kafka-sink","text":"Two methods are available for integrating Kafka topics into your Numaflow pipeline: using a user-defined Kafka Sink or opting for the built-in Kafka Sink provided by Numaflow.","title":"Kafka Sink"},{"location":"user-guide/sinks/kafka/#option-1-user-defined-kafka-sink","text":"Developed and maintained by the Numaflow contributor community, the Kafka Sink provides a reliable and feature-complete solution for publishing messages to Kafka topics. Key Features: Customization: Offers complete control over Kafka Sink configurations to tailor to specific requirements. Kafka Java Client Utilization: Leverages the Kafka Java client for reliable message publishing to Kafka topics. Schema Management: Integrates seamlessly with the Confluent Schema Registry to support schema validation and manage schema evolution effectively. More details on how to use the Kafka Sink can be found here .","title":"Option 1: User-Defined Kafka Sink"},{"location":"user-guide/sinks/kafka/#option-2-built-in-kafka-sink","text":"A Kafka sink is used to forward the messages to a Kafka topic. Kafka sink supports configuration overrides.","title":"Option 2: Built-in Kafka Sink"},{"location":"user-guide/sinks/kafka/#kafka-headers","text":"We will insert keys into the Kafka header, but since keys is an array, we will add keys into the header in the following format. __keys_len will have the number of key in the header. if __keys_len == 0 , means no keys are present. __keys_%d will have the key , e.g., __key_0 will be the first key, and so forth.","title":"Kafka Headers"},{"location":"user-guide/sinks/kafka/#example","text":"spec : vertices : - name : kafka-output sink : kafka : brokers : - my-broker1:19700 - my-broker2:19700 topic : my-topic tls : # Optional. insecureSkipVerify : # Optional, where to skip TLS verification. Default to false. caCertSecret : # Optional, a secret reference, which contains the CA Cert. name : my-ca-cert key : my-ca-cert-key certSecret : # Optional, pointing to a secret reference which contains the Cert. name : my-cert key : my-cert-key keySecret : # Optional, pointing to a secret reference which contains the Private Key. name : my-pk key : my-pk-key sasl : # Optional mechanism : GSSAPI # PLAIN, GSSAPI, OAUTHBEARER, SCRAM-SHA-256 or SCRAM-SHA-512 other mechanisms not supported gssapi : # Optional, for GSSAPI mechanism serviceName : my-service realm : my-realm # KRB5_USER_AUTH for auth using password # KRB5_KEYTAB_AUTH for auth using keytab authType : KRB5_KEYTAB_AUTH usernameSecret : # Pointing to a secret reference which contains the username name : gssapi-username key : gssapi-username-key # Pointing to a secret reference which contains the keytab (authType: KRB5_KEYTAB_AUTH) keytabSecret : name : gssapi-keytab key : gssapi-keytab-key # Pointing to a secret reference which contains the keytab (authType: KRB5_USER_AUTH) passwordSecret : name : gssapi-password key : gssapi-password-key kerberosConfigSecret : # Pointing to a secret reference which contains the kerberos config name : my-kerberos-config key : my-kerberos-config-key plain : # Optional, for PLAIN mechanism userSecret : # Pointing to a secret reference which contains the user name : plain-user key : plain-user-key passwordSecret : # Pointing to a secret reference which contains the password name : plain-password key : plain-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true scramsha256 : # Optional, for SCRAM-SHA-256 mechanism userSecret : # Pointing to a secret reference which contains the user name : scram-sha-256-user key : scram-sha-256-user-key passwordSecret : # Pointing to a secret reference which contains the password name : scram-sha-256-password key : scram-sha-256-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true scramsha512 : # Optional, for SCRAM-SHA-512 mechanism userSecret : # Pointing to a secret reference which contains the user name : scram-sha-512-user key : scram-sha-512-user-key passwordSecret : # Pointing to a secret reference which contains the password name : scram-sha-512-password key : scram-sha-512-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true oauth : #Optional, for OAUTHBEARER mechanism clientID : # Pointing to a secret reference which contains the client id name : kafka-oauth-client key : clientid clientSecret : # Pointing to a secret reference which contains the client secret name : kafka-oauth-client key : clientsecret tokenEndpoint : https://oauth-token.com/v1/token # Optional, a yaml format string which could apply more configuration for the sink. # The configuration hierarchy follows the Struct of sarama.Config at https://github.com/IBM/sarama/blob/main/config.go. config : | producer: compression: 2","title":"Example"},{"location":"user-guide/sinks/log/","text":"Log Sink \u00b6 A Log sink is very useful for debugging, it prints all the received messages to stdout . spec : vertices : - name : output sink : log : {}","title":"Log Sink"},{"location":"user-guide/sinks/log/#log-sink","text":"A Log sink is very useful for debugging, it prints all the received messages to stdout . spec : vertices : - name : output sink : log : {}","title":"Log Sink"},{"location":"user-guide/sinks/overview/","text":"Sinks \u00b6 The Sink serves as the endpoint for processed data that has been outputted from the platform, which is then sent to an external system or application. The purpose of the Sink is to deliver the processed data to its ultimate destination, such as a database, data warehouse, visualization tool, or alerting system. It's the opposite of the Source vertex, which receives input data into the platform. Sink vertex may require transformation or formatting of data prior to sending it to the target system. Depending on the target system's needs, this transformation can be simple or complex. A pipeline can have many Sink vertices, unlike the Source vertex. Numaflow currently supports the following Sinks Kafka Log Black Hole User-defined Sink A user-defined sink is a custom Sink that a user can write using Numaflow SDK when the user needs to output the processed data to a system or using a certain transformation that is not supported by the platform's built-in sinks. As an example, once we have processed the input messages, we can use Elasticsearch as a user-defined sink to store the processed data and enable search and analysis on the data. Fallback Sink (DLQ) \u00b6 There is an explicit DLQ support for sinks using a concept called fallback sink . For the rest of vertices, if you need DLQ, please use conditional-forwarding . Sink cannot do conditional-forwarding since it is a terminal state and hence we have explicit fallback option.","title":"Overview"},{"location":"user-guide/sinks/overview/#sinks","text":"The Sink serves as the endpoint for processed data that has been outputted from the platform, which is then sent to an external system or application. The purpose of the Sink is to deliver the processed data to its ultimate destination, such as a database, data warehouse, visualization tool, or alerting system. It's the opposite of the Source vertex, which receives input data into the platform. Sink vertex may require transformation or formatting of data prior to sending it to the target system. Depending on the target system's needs, this transformation can be simple or complex. A pipeline can have many Sink vertices, unlike the Source vertex. Numaflow currently supports the following Sinks Kafka Log Black Hole User-defined Sink A user-defined sink is a custom Sink that a user can write using Numaflow SDK when the user needs to output the processed data to a system or using a certain transformation that is not supported by the platform's built-in sinks. As an example, once we have processed the input messages, we can use Elasticsearch as a user-defined sink to store the processed data and enable search and analysis on the data.","title":"Sinks"},{"location":"user-guide/sinks/overview/#fallback-sink-dlq","text":"There is an explicit DLQ support for sinks using a concept called fallback sink . For the rest of vertices, if you need DLQ, please use conditional-forwarding . Sink cannot do conditional-forwarding since it is a terminal state and hence we have explicit fallback option.","title":"Fallback Sink (DLQ)"},{"location":"user-guide/sinks/retry-strategy/","text":"Retry Strategy \u00b6 Overview \u00b6 The RetryStrategy is used to configure the behavior for a sink after encountering failures during a write operation. This structure allows the user to specify how Numaflow should respond to different fail-over scenarios for Sinks, ensuring that the writing can be resilient and handle unexpected issues efficiently. RetryStrategy ONLY gets applied to failed messages. To return a failed message, use the methods provided by the SDKs. ResponseFailure for Golang responseFailure for Java as_failure for Python Retry Strategy Configuration \u00b6 The retryStrategy section allows you to define custom retry behavior for sink operations. If no custom fields are defined, the Default values are applied. Example Configuration \u00b6 sink : retryStrategy : # Optional backoff : interval : 1s # Optional, a string with timestamp suffix steps : 3 # Optional, unsigned int, cannot be 0 factor : 1.5 # Optional, float type, >= 1 cap : 20s # Optional, a string with timestamp suffix jitter : 0.1 # Optional, float type, >=0 and <1 # Optional onFailure : 'fallback' BackOff Parameters \u00b6 The BackOff configuration defines the timing and limits for retries. Below are the available fields: interval : The time interval to wait before retry attempts. Type: String with a timestamp suffix. Default: 1ms . steps : The maximum number of retry attempts, including the initial attempt. Type: Unsigned integer, must be greater than 0. Default: Infinite. factor : A multiplier applied to the interval after each retry attempt. Type: Float, must be greater than or equal to 1. Default: 1.0 . cap : The maximum value for the interval, limiting exponential backoff growth. Type: String with a timestamp suffix. Default: indefinite (no upper limit). jitter : Adds randomness to the interval to avoid retry collisions. Type: Float, must be greater than or equal to 0 and less than 1. Default: 0 . OnFailure Actions \u00b6 The onFailure field specifies the action to take when retries are exhausted. Available options are: retry : Restart the retry logic. fallback : Route the remaining messages to a fallback sink . drop : Discard any unprocessed messages. Default: retry Sink Example with Retry Strategy \u00b6 sink : retryStrategy : backoff : interval : 500ms steps : 10 factor : 2.2 cap : 10s onFailure : 'fallback' udsink : container : image : my-sink-image fallback : udsink : container : image : my-fallback-sink Explanation \u00b6 Primary Sink Processing : The main sink container ( UDSink ) processes the data. If a batch write operation fails, the system will retry up to 10 times. Retry Behavior : The first retry happens after 500 milliseconds. Each subsequent retry interval increases by multiplying the previous interval by 2.2, up to a maximum interval of 10 seconds. Fallback Handling : If all retries are exhausted and the operation still fails, the data is routed to a fallback sink.","title":"Retry Strategy"},{"location":"user-guide/sinks/retry-strategy/#retry-strategy","text":"","title":"Retry Strategy"},{"location":"user-guide/sinks/retry-strategy/#overview","text":"The RetryStrategy is used to configure the behavior for a sink after encountering failures during a write operation. This structure allows the user to specify how Numaflow should respond to different fail-over scenarios for Sinks, ensuring that the writing can be resilient and handle unexpected issues efficiently. RetryStrategy ONLY gets applied to failed messages. To return a failed message, use the methods provided by the SDKs. ResponseFailure for Golang responseFailure for Java as_failure for Python","title":"Overview"},{"location":"user-guide/sinks/retry-strategy/#retry-strategy-configuration","text":"The retryStrategy section allows you to define custom retry behavior for sink operations. If no custom fields are defined, the Default values are applied.","title":"Retry Strategy Configuration"},{"location":"user-guide/sinks/retry-strategy/#example-configuration","text":"sink : retryStrategy : # Optional backoff : interval : 1s # Optional, a string with timestamp suffix steps : 3 # Optional, unsigned int, cannot be 0 factor : 1.5 # Optional, float type, >= 1 cap : 20s # Optional, a string with timestamp suffix jitter : 0.1 # Optional, float type, >=0 and <1 # Optional onFailure : 'fallback'","title":"Example Configuration"},{"location":"user-guide/sinks/retry-strategy/#backoff-parameters","text":"The BackOff configuration defines the timing and limits for retries. Below are the available fields: interval : The time interval to wait before retry attempts. Type: String with a timestamp suffix. Default: 1ms . steps : The maximum number of retry attempts, including the initial attempt. Type: Unsigned integer, must be greater than 0. Default: Infinite. factor : A multiplier applied to the interval after each retry attempt. Type: Float, must be greater than or equal to 1. Default: 1.0 . cap : The maximum value for the interval, limiting exponential backoff growth. Type: String with a timestamp suffix. Default: indefinite (no upper limit). jitter : Adds randomness to the interval to avoid retry collisions. Type: Float, must be greater than or equal to 0 and less than 1. Default: 0 .","title":"BackOff Parameters"},{"location":"user-guide/sinks/retry-strategy/#onfailure-actions","text":"The onFailure field specifies the action to take when retries are exhausted. Available options are: retry : Restart the retry logic. fallback : Route the remaining messages to a fallback sink . drop : Discard any unprocessed messages. Default: retry","title":"OnFailure Actions"},{"location":"user-guide/sinks/retry-strategy/#sink-example-with-retry-strategy","text":"sink : retryStrategy : backoff : interval : 500ms steps : 10 factor : 2.2 cap : 10s onFailure : 'fallback' udsink : container : image : my-sink-image fallback : udsink : container : image : my-fallback-sink","title":"Sink Example with Retry Strategy"},{"location":"user-guide/sinks/retry-strategy/#explanation","text":"Primary Sink Processing : The main sink container ( UDSink ) processes the data. If a batch write operation fails, the system will retry up to 10 times. Retry Behavior : The first retry happens after 500 milliseconds. Each subsequent retry interval increases by multiplying the previous interval by 2.2, up to a maximum interval of 10 seconds. Fallback Handling : If all retries are exhausted and the operation still fails, the data is routed to a fallback sink.","title":"Explanation"},{"location":"user-guide/sinks/user-defined-sinks/","text":"User-defined Sinks \u00b6 Numaflow provides builtin Sinks but there are many use cases where an user might want to write a custom Sink implementation, and User Defined Sink ( udsink ) can be used to write those custom Sinks. A pre-defined sink vertex runs single-container pods, while a user-defined sink runs two-container pods. Related Topics: - Sinks Overview - General information about sinks - Fallback Sink - Dead Letter Queue (DLQ) functionality - Retry Strategy - Configuring retry behavior for failed messages Build Your Own User-defined Sinks \u00b6 You can build your own user-defined sinks in multiple languages. Check the SDK examples section below for detailed implementation patterns and working examples. Message\u2013Response Contract \u00b6 When developing a user-defined sink in Numaflow, your sink container communicates with the sidecar via an ordered batches of messages. For each batch of messages received, your sink must return a list of responses that strictly adheres to the following contract: Core Rules: \u00b6 Response Count: For every batch of N input messages received, your sink must return exactly N responses. Response Matching: Each response must include the corresponding message ID ( datumID ) to identify which message it corresponds to. The order of responses does not need to match the order of incoming messages. Every input message must have exactly one matching response. Response Status: Each response must indicate how the message was handled by returning one of the following statuses: OK FAILURE FALLBACK SERVE Response Types \u00b6 OK ( ResponseOK ) \u00b6 Meaning: Message was successfully processed and written to the sink. Action: Message is acknowledged and removed from the pipeline. Use Case: Normal successful operations. FAILURE ( ResponseFailure ) \u00b6 Meaning: Permanent error occurred during processing. Action: Message will be dropped from the pipeline (no retry). Requirement: Must include a descriptive error message. Use Case: Invalid data, unsupported formats, or unrecoverable failures. Related: See Retry Strategy for configuring retry behavior. FALLBACK ( ResponseFallback ) \u00b6 Meaning: Message should be routed to the fallback sink (dead-letter queue). Action: Message is immediately sent to the configured fallback sink without retry. Requirements: A fallback sink must be configured in the pipeline spec. Fallback responses must not return further fallback or serve statuses. Use Case: Messages that cannot be processed by the primary sink but should be preserved. Related: See Fallback Sink for detailed configuration. SERVE ( ResponseServe ) \u00b6 Meaning: Message should be stored in the serving store for later retrieval. Action: Message is stored in the configured serving store (e.g., NATS or user-defined). Requirements: Serving store must be configured in the pipeline spec. Use Case: Storing results for serving pipelines or caching responses. Example \u00b6 If the input batch is: [ { \"id\" : \"d1\" , \"value\" : \"event 1\" }, { \"id\" : \"d2\" , \"value\" : \"event 2\" }, { \"id\" : \"d3\" , \"value\" : \"event 3\" } ] Then your response must be: [ { \"id\" : \"d1\" , \"status\" : \"OK\" }, { \"id\" : \"d2\" , \"status\" : \"FAILURE\" , \"err_msg\" : \"Invalid format\" }, { \"id\" : \"d3\" , \"status\" : \"FALLBACK\" } ] Important Assumptions & Constraints \u00b6 Message ID Uniqueness: \u00b6 Within a single sink iterator, no duplicate datumIDs exist. Message IDs are unique across the entire pipeline for correct tracking. Response Validation: \u00b6 Fallback sink responses must not return fallback or serve statuses. Error messages are required for FAILURE responses and optional for others. Retry Behavior: \u00b6 FAILURE responses follow the configured retry strategy if enabled. FALLBACK responses bypass retry and go directly to the fallback sink. OK responses are not retried. Batch Processing: \u00b6 Messages are processed in batches for efficiency. All messages in a batch must be responded to before the next batch. Incorrect response counts or missing IDs will cause stalling of processing. Idempotency: \u00b6 Since retries can occur for FAILURE statuses, your sink implementation must be idempotent to avoid duplicated side effects. SDK Methods \u00b6 Check the links below to see the examples for different languages. Golang: ResponseOK ( id string ) ResponseFailure ( id string , errMsg string ) ResponseFallback ( id string ) ResponseServe ( id string , serveResponse [] byte ) Java: Response . ok ( id ) Response . failure ( id , errMsg ) Response . fallback ( id ) Response . serve ( id , serveResponse ) Python: Response . ok ( id ) Response . failure ( id , err_msg ) Response . fallback ( id ) Response . serve ( id , serve_response ) Explore SDK Examples \u00b6 For more detailed examples and implementation patterns, check out the official SDK repositories: Golang SDK Examples Java SDK Examples Python SDK Examples These repositories contain complete working examples of user-defined sinks with various response types and error handling patterns. Reminder \u00b6 Always validate that the number of responses equals the number of incoming messages before returning your response to the sidecar. Incorrect response counts or missing IDs will stall the pipeline. Example User-Defined Sink Vertex Configuration \u00b6 spec : vertices : - name : output sink : udsink : container : image : my-sink:latest Available Environment Variables \u00b6 Some environment variables are available in the user-defined sink container: NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex. User-Defined Sinks Contributed by the Open Source Community \u00b6 If you're looking for examples and usages contributed by the open source community, head over to the numaproj-contrib repositories . These user-defined sinks, like AWS SQS and GCP Pub/Sub, provide valuable insights and guidance on how to use and write a user-defined sink.","title":"User-defined Sinks"},{"location":"user-guide/sinks/user-defined-sinks/#user-defined-sinks","text":"Numaflow provides builtin Sinks but there are many use cases where an user might want to write a custom Sink implementation, and User Defined Sink ( udsink ) can be used to write those custom Sinks. A pre-defined sink vertex runs single-container pods, while a user-defined sink runs two-container pods. Related Topics: - Sinks Overview - General information about sinks - Fallback Sink - Dead Letter Queue (DLQ) functionality - Retry Strategy - Configuring retry behavior for failed messages","title":"User-defined Sinks"},{"location":"user-guide/sinks/user-defined-sinks/#build-your-own-user-defined-sinks","text":"You can build your own user-defined sinks in multiple languages. Check the SDK examples section below for detailed implementation patterns and working examples.","title":"Build Your Own User-defined Sinks"},{"location":"user-guide/sinks/user-defined-sinks/#messageresponse-contract","text":"When developing a user-defined sink in Numaflow, your sink container communicates with the sidecar via an ordered batches of messages. For each batch of messages received, your sink must return a list of responses that strictly adheres to the following contract:","title":"Message\u2013Response Contract"},{"location":"user-guide/sinks/user-defined-sinks/#core-rules","text":"Response Count: For every batch of N input messages received, your sink must return exactly N responses. Response Matching: Each response must include the corresponding message ID ( datumID ) to identify which message it corresponds to. The order of responses does not need to match the order of incoming messages. Every input message must have exactly one matching response. Response Status: Each response must indicate how the message was handled by returning one of the following statuses: OK FAILURE FALLBACK SERVE","title":"Core Rules:"},{"location":"user-guide/sinks/user-defined-sinks/#response-types","text":"","title":"Response Types"},{"location":"user-guide/sinks/user-defined-sinks/#ok-responseok","text":"Meaning: Message was successfully processed and written to the sink. Action: Message is acknowledged and removed from the pipeline. Use Case: Normal successful operations.","title":"OK (ResponseOK)"},{"location":"user-guide/sinks/user-defined-sinks/#failure-responsefailure","text":"Meaning: Permanent error occurred during processing. Action: Message will be dropped from the pipeline (no retry). Requirement: Must include a descriptive error message. Use Case: Invalid data, unsupported formats, or unrecoverable failures. Related: See Retry Strategy for configuring retry behavior.","title":"FAILURE (ResponseFailure)"},{"location":"user-guide/sinks/user-defined-sinks/#fallback-responsefallback","text":"Meaning: Message should be routed to the fallback sink (dead-letter queue). Action: Message is immediately sent to the configured fallback sink without retry. Requirements: A fallback sink must be configured in the pipeline spec. Fallback responses must not return further fallback or serve statuses. Use Case: Messages that cannot be processed by the primary sink but should be preserved. Related: See Fallback Sink for detailed configuration.","title":"FALLBACK (ResponseFallback)"},{"location":"user-guide/sinks/user-defined-sinks/#serve-responseserve","text":"Meaning: Message should be stored in the serving store for later retrieval. Action: Message is stored in the configured serving store (e.g., NATS or user-defined). Requirements: Serving store must be configured in the pipeline spec. Use Case: Storing results for serving pipelines or caching responses.","title":"SERVE (ResponseServe)"},{"location":"user-guide/sinks/user-defined-sinks/#example","text":"If the input batch is: [ { \"id\" : \"d1\" , \"value\" : \"event 1\" }, { \"id\" : \"d2\" , \"value\" : \"event 2\" }, { \"id\" : \"d3\" , \"value\" : \"event 3\" } ] Then your response must be: [ { \"id\" : \"d1\" , \"status\" : \"OK\" }, { \"id\" : \"d2\" , \"status\" : \"FAILURE\" , \"err_msg\" : \"Invalid format\" }, { \"id\" : \"d3\" , \"status\" : \"FALLBACK\" } ]","title":"Example"},{"location":"user-guide/sinks/user-defined-sinks/#important-assumptions-constraints","text":"","title":"Important Assumptions &amp; Constraints"},{"location":"user-guide/sinks/user-defined-sinks/#message-id-uniqueness","text":"Within a single sink iterator, no duplicate datumIDs exist. Message IDs are unique across the entire pipeline for correct tracking.","title":"Message ID Uniqueness:"},{"location":"user-guide/sinks/user-defined-sinks/#response-validation","text":"Fallback sink responses must not return fallback or serve statuses. Error messages are required for FAILURE responses and optional for others.","title":"Response Validation:"},{"location":"user-guide/sinks/user-defined-sinks/#retry-behavior","text":"FAILURE responses follow the configured retry strategy if enabled. FALLBACK responses bypass retry and go directly to the fallback sink. OK responses are not retried.","title":"Retry Behavior:"},{"location":"user-guide/sinks/user-defined-sinks/#batch-processing","text":"Messages are processed in batches for efficiency. All messages in a batch must be responded to before the next batch. Incorrect response counts or missing IDs will cause stalling of processing.","title":"Batch Processing:"},{"location":"user-guide/sinks/user-defined-sinks/#idempotency","text":"Since retries can occur for FAILURE statuses, your sink implementation must be idempotent to avoid duplicated side effects.","title":"Idempotency:"},{"location":"user-guide/sinks/user-defined-sinks/#sdk-methods","text":"Check the links below to see the examples for different languages. Golang: ResponseOK ( id string ) ResponseFailure ( id string , errMsg string ) ResponseFallback ( id string ) ResponseServe ( id string , serveResponse [] byte ) Java: Response . ok ( id ) Response . failure ( id , errMsg ) Response . fallback ( id ) Response . serve ( id , serveResponse ) Python: Response . ok ( id ) Response . failure ( id , err_msg ) Response . fallback ( id ) Response . serve ( id , serve_response )","title":"SDK Methods"},{"location":"user-guide/sinks/user-defined-sinks/#explore-sdk-examples","text":"For more detailed examples and implementation patterns, check out the official SDK repositories: Golang SDK Examples Java SDK Examples Python SDK Examples These repositories contain complete working examples of user-defined sinks with various response types and error handling patterns.","title":"Explore SDK Examples"},{"location":"user-guide/sinks/user-defined-sinks/#reminder","text":"Always validate that the number of responses equals the number of incoming messages before returning your response to the sidecar. Incorrect response counts or missing IDs will stall the pipeline.","title":"Reminder"},{"location":"user-guide/sinks/user-defined-sinks/#example-user-defined-sink-vertex-configuration","text":"spec : vertices : - name : output sink : udsink : container : image : my-sink:latest","title":"Example User-Defined Sink Vertex Configuration"},{"location":"user-guide/sinks/user-defined-sinks/#available-environment-variables","text":"Some environment variables are available in the user-defined sink container: NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex.","title":"Available Environment Variables"},{"location":"user-guide/sinks/user-defined-sinks/#user-defined-sinks-contributed-by-the-open-source-community","text":"If you're looking for examples and usages contributed by the open source community, head over to the numaproj-contrib repositories . These user-defined sinks, like AWS SQS and GCP Pub/Sub, provide valuable insights and guidance on how to use and write a user-defined sink.","title":"User-Defined Sinks Contributed by the Open Source Community"},{"location":"user-guide/sources/generator/","text":"Generator Source \u00b6 Generator Source is mainly used for development purpose, where you want to have self-contained source to generate some messages. We mainly use generator for load testing and integration testing of Numaflow. The load generated is per replica. Generator Payload Structure \u00b6 The payload generated by the generator is a JSON object with the following fields: id - Timestamp in nano seconds appended with replica number value - Value provided in the generator configuration or timestamp in nano seconds if not provided padding - Random bytes to make the message size to the approximate desired size { \"data\" : { \"id\" : \"1673239888-0\" , \"value\" : 1673239888 , \"padding\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] }, \"createdts\" : 1673239888 } Example \u00b6 apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : in source : generator : # How many messages to generate in the duration. rpu : 100 duration : 1s # Optional, size of each generated message, defaults to 10. msgSize : 1024 - name : p1 udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always - name : out sink : log : {} edges : - from : in to : p1 - from : p1 to : out User Defined Data \u00b6 The default data created by the generator is not likely to be useful in testing user pipelines with specific business logic. To allow this to help with user testing, a user defined value can be provided which will be emitted for each of the generator. Example \u00b6 - name: in source: generator: # How many messages to generate in the duration. rpu: 100 duration: 1s # Base64 encoding of data to send. Can be example serialized packet to # run through user pipeline to exercise particular capability or path through pipeline valueBlob: \"InlvdXIgc3BlY2lmaWMgZGF0YSI=\" # Note: msgSize and value will be ignored if valueBlob is set","title":"Generator Source"},{"location":"user-guide/sources/generator/#generator-source","text":"Generator Source is mainly used for development purpose, where you want to have self-contained source to generate some messages. We mainly use generator for load testing and integration testing of Numaflow. The load generated is per replica.","title":"Generator Source"},{"location":"user-guide/sources/generator/#generator-payload-structure","text":"The payload generated by the generator is a JSON object with the following fields: id - Timestamp in nano seconds appended with replica number value - Value provided in the generator configuration or timestamp in nano seconds if not provided padding - Random bytes to make the message size to the approximate desired size { \"data\" : { \"id\" : \"1673239888-0\" , \"value\" : 1673239888 , \"padding\" : [ 1 , 2 , 3 , 4 , 5 , 6 , 7 , 8 ] }, \"createdts\" : 1673239888 }","title":"Generator Payload Structure"},{"location":"user-guide/sources/generator/#example","text":"apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : in source : generator : # How many messages to generate in the duration. rpu : 100 duration : 1s # Optional, size of each generated message, defaults to 10. msgSize : 1024 - name : p1 udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always - name : out sink : log : {} edges : - from : in to : p1 - from : p1 to : out","title":"Example"},{"location":"user-guide/sources/generator/#user-defined-data","text":"The default data created by the generator is not likely to be useful in testing user pipelines with specific business logic. To allow this to help with user testing, a user defined value can be provided which will be emitted for each of the generator.","title":"User Defined Data"},{"location":"user-guide/sources/generator/#example_1","text":"- name: in source: generator: # How many messages to generate in the duration. rpu: 100 duration: 1s # Base64 encoding of data to send. Can be example serialized packet to # run through user pipeline to exercise particular capability or path through pipeline valueBlob: \"InlvdXIgc3BlY2lmaWMgZGF0YSI=\" # Note: msgSize and value will be ignored if valueBlob is set","title":"Example"},{"location":"user-guide/sources/http/","text":"HTTP Source \u00b6 HTTP Source starts an HTTP service with TLS enabled to accept POST request in the Vertex Pod. It listens to port 8443, with request URI /vertices/{vertexName} . A Pipeline with HTTP Source: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : http-pipeline spec : vertices : - name : in source : http : {} - name : p1 udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always - name : out sink : log : {} edges : - from : in to : p1 - from : p1 to : out Sending Data \u00b6 Data could be sent to an HTTP source through: ClusterIP Service (within the cluster) Ingress or LoadBalancer Service (outside of the cluster) Port-forward (for testing) ClusterIP Service \u00b6 An HTTP Source Vertex can generate a ClusterIP Service if service: true is specified, the service name is in the format of {pipelineName}-{vertexName} , so the HTTP Source can be accessed through https://{pipelineName}-{vertexName}.{namespace}.svc:8443/vertices/{vertexName} within the cluster. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : http-pipeline spec : vertices : - name : in source : http : service : true LoadBalancer Service or Ingress \u00b6 To create a LoadBalander type Service, or a NodePort one for Ingress, you need to do it by you own. Just remember to use selector like following in the Service: numaflow.numaproj.io/pipeline-name : http-pipeline # pipeline name numaflow.numaproj.io/vertex-name : in # vertex name Port-forwarding \u00b6 To test an HTTP source, you can do it from your local through port-forwarding. kubectl port-forward pod ${ pod -name } 8443 curl -kq -X POST -d \"hello world\" https://localhost:8443/vertices/in x-numaflow-id \u00b6 When posting data to the HTTP Source, an optional HTTP header x-numaflow-id can be specified, which will be used to dedup. If it's not provided, the HTTP Source will generate a random UUID to do it. curl -kq -X POST -H \"x-numaflow-id: ${ id } \" -d \"hello world\" ${ http -source-url } x-numaflow-event-time \u00b6 By default, the time of the date coming to the HTTP source is used as the event time, it could be set by putting an HTTP header x-numaflow-event-time with value of the number of milliseconds elapsed since January 1, 1970 UTC. curl -kq -X POST -H \"x-numaflow-event-time: 1663006726000\" -d \"hello world\" ${ http -source-url } Auth \u00b6 A Bearer token can be configured to prevent the HTTP Source from being accessed by unexpected clients. To do so, a Kubernetes Secret needs to be created to store the token, and the valid clients also need to include the token in its HTTP request header. Firstly, create a k8s secret containing your token. echo -n 'tr3qhs321fjglwf1e2e67dfda4tr' > ./token.txt kubectl create secret generic http-source-token --from-file = my-token = ./token.txt Then add auth to the Source Vertex: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : http-pipeline spec : vertices : - name : in source : http : auth : token : name : http-source-token key : my-token When the clients post data to the Source Vertex, add Authorization: Bearer tr3qhs321fjglwf1e2e67dfda4tr to the header, for example: TOKEN = \"Bearer tr3qhs321fjglwf1e2e67dfda4tr\" # Post data from a Pod in the same namespace of the cluster curl -kq -X POST -H \"Authorization: $TOKEN \" -d \"hello world\" https://http-pipeline-in:8443/vertices/in Health Check \u00b6 The HTTP Source also has an endpoint /health created automatically, which is useful for LoadBalancer or Ingress configuration, where a health check endpoint is often required by the cloud provider.","title":"HTTP Source"},{"location":"user-guide/sources/http/#http-source","text":"HTTP Source starts an HTTP service with TLS enabled to accept POST request in the Vertex Pod. It listens to port 8443, with request URI /vertices/{vertexName} . A Pipeline with HTTP Source: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : http-pipeline spec : vertices : - name : in source : http : {} - name : p1 udf : container : image : quay.io/numaio/numaflow-go/map-cat:stable # A UDF which simply cats the message imagePullPolicy : Always - name : out sink : log : {} edges : - from : in to : p1 - from : p1 to : out","title":"HTTP Source"},{"location":"user-guide/sources/http/#sending-data","text":"Data could be sent to an HTTP source through: ClusterIP Service (within the cluster) Ingress or LoadBalancer Service (outside of the cluster) Port-forward (for testing)","title":"Sending Data"},{"location":"user-guide/sources/http/#clusterip-service","text":"An HTTP Source Vertex can generate a ClusterIP Service if service: true is specified, the service name is in the format of {pipelineName}-{vertexName} , so the HTTP Source can be accessed through https://{pipelineName}-{vertexName}.{namespace}.svc:8443/vertices/{vertexName} within the cluster. apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : http-pipeline spec : vertices : - name : in source : http : service : true","title":"ClusterIP Service"},{"location":"user-guide/sources/http/#loadbalancer-service-or-ingress","text":"To create a LoadBalander type Service, or a NodePort one for Ingress, you need to do it by you own. Just remember to use selector like following in the Service: numaflow.numaproj.io/pipeline-name : http-pipeline # pipeline name numaflow.numaproj.io/vertex-name : in # vertex name","title":"LoadBalancer Service or Ingress"},{"location":"user-guide/sources/http/#port-forwarding","text":"To test an HTTP source, you can do it from your local through port-forwarding. kubectl port-forward pod ${ pod -name } 8443 curl -kq -X POST -d \"hello world\" https://localhost:8443/vertices/in","title":"Port-forwarding"},{"location":"user-guide/sources/http/#x-numaflow-id","text":"When posting data to the HTTP Source, an optional HTTP header x-numaflow-id can be specified, which will be used to dedup. If it's not provided, the HTTP Source will generate a random UUID to do it. curl -kq -X POST -H \"x-numaflow-id: ${ id } \" -d \"hello world\" ${ http -source-url }","title":"x-numaflow-id"},{"location":"user-guide/sources/http/#x-numaflow-event-time","text":"By default, the time of the date coming to the HTTP source is used as the event time, it could be set by putting an HTTP header x-numaflow-event-time with value of the number of milliseconds elapsed since January 1, 1970 UTC. curl -kq -X POST -H \"x-numaflow-event-time: 1663006726000\" -d \"hello world\" ${ http -source-url }","title":"x-numaflow-event-time"},{"location":"user-guide/sources/http/#auth","text":"A Bearer token can be configured to prevent the HTTP Source from being accessed by unexpected clients. To do so, a Kubernetes Secret needs to be created to store the token, and the valid clients also need to include the token in its HTTP request header. Firstly, create a k8s secret containing your token. echo -n 'tr3qhs321fjglwf1e2e67dfda4tr' > ./token.txt kubectl create secret generic http-source-token --from-file = my-token = ./token.txt Then add auth to the Source Vertex: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : http-pipeline spec : vertices : - name : in source : http : auth : token : name : http-source-token key : my-token When the clients post data to the Source Vertex, add Authorization: Bearer tr3qhs321fjglwf1e2e67dfda4tr to the header, for example: TOKEN = \"Bearer tr3qhs321fjglwf1e2e67dfda4tr\" # Post data from a Pod in the same namespace of the cluster curl -kq -X POST -H \"Authorization: $TOKEN \" -d \"hello world\" https://http-pipeline-in:8443/vertices/in","title":"Auth"},{"location":"user-guide/sources/http/#health-check","text":"The HTTP Source also has an endpoint /health created automatically, which is useful for LoadBalancer or Ingress configuration, where a health check endpoint is often required by the cloud provider.","title":"Health Check"},{"location":"user-guide/sources/jetstream/","text":"Jetstream Source \u00b6 A Jetstream source is used to ingest the messages from a Jetstream stream . spec : vertices : - name : input source : jetstream : url : nats://demo.nats.io # Jetstream server url stream : my-stream # stream name consumer : my-consumer # Optional, Name of the consumer on the stream. deliver_policy : last_per_subject # Optional, The point in the stream from which to receive messages. Defaults to \"all\" filter_subjects : # Optional, A set of subjects that overlap with the subjects bound to the stream to filter delivery to subscribers - \"abc.A.*\" - \"abc.B.*\" tls : # Optional. insecureSkipVerify : # Optional, whether to skip TLS verification. Default to false. caCertSecret : # Optional, a secret reference, which contains the CA Cert. name : my-ca-cert key : my-ca-cert-key certSecret : # Optional, pointing to a secret reference which contains the Cert. name : my-cert key : my-cert-key keySecret : # Optional, pointing to a secret reference which contains the Private Key. name : my-pk key : my-pk-key auth : # Optional. basic : # Optional, pointing to the secret references which contain user name and password. user : name : my-secret key : my-user password : name : my-secret key : my-password The consumer field represents the name of the consumer of the stream. If not specified, a consumer with name format numaflow-<pipeline_name>-<vertex_name>-<stream_name> will used. Numaflow will attempt to create this consumer on the stream if it doesn't exist. The valid values for deliver_policy are: all new last last_per_subject by_start_sequence <sequence_id> eg. by_start_sequence 42 by_start_time <unix_epoch_time_milliseconds> eg. by_start_time 1753428483000 Auth \u00b6 The auth strategies supported in Jetstream source include basic (user and password), token and nkey , check the API for the details.","title":"Jetstream Source"},{"location":"user-guide/sources/jetstream/#jetstream-source","text":"A Jetstream source is used to ingest the messages from a Jetstream stream . spec : vertices : - name : input source : jetstream : url : nats://demo.nats.io # Jetstream server url stream : my-stream # stream name consumer : my-consumer # Optional, Name of the consumer on the stream. deliver_policy : last_per_subject # Optional, The point in the stream from which to receive messages. Defaults to \"all\" filter_subjects : # Optional, A set of subjects that overlap with the subjects bound to the stream to filter delivery to subscribers - \"abc.A.*\" - \"abc.B.*\" tls : # Optional. insecureSkipVerify : # Optional, whether to skip TLS verification. Default to false. caCertSecret : # Optional, a secret reference, which contains the CA Cert. name : my-ca-cert key : my-ca-cert-key certSecret : # Optional, pointing to a secret reference which contains the Cert. name : my-cert key : my-cert-key keySecret : # Optional, pointing to a secret reference which contains the Private Key. name : my-pk key : my-pk-key auth : # Optional. basic : # Optional, pointing to the secret references which contain user name and password. user : name : my-secret key : my-user password : name : my-secret key : my-password The consumer field represents the name of the consumer of the stream. If not specified, a consumer with name format numaflow-<pipeline_name>-<vertex_name>-<stream_name> will used. Numaflow will attempt to create this consumer on the stream if it doesn't exist. The valid values for deliver_policy are: all new last last_per_subject by_start_sequence <sequence_id> eg. by_start_sequence 42 by_start_time <unix_epoch_time_milliseconds> eg. by_start_time 1753428483000","title":"Jetstream Source"},{"location":"user-guide/sources/jetstream/#auth","text":"The auth strategies supported in Jetstream source include basic (user and password), token and nkey , check the API for the details.","title":"Auth"},{"location":"user-guide/sources/kafka/","text":"Kafka Source \u00b6 Two methods are available for integrating Kafka topics into your Numaflow pipeline: using a user-defined Kafka Source or opting for the built-in Kafka Source provided by Numaflow. Option 1: User-Defined Kafka Source \u00b6 Developed and maintained by the Numaflow contributor community, the Kafka Source offers a robust and feature-complete solution for integrating Kafka as a data source into your Numaflow pipeline. Key Features: Flexibility: Allows full customization of Kafka Source configurations to suit specific needs. Kafka Java Client Utilization: Leverages the Kafka Java client for robust message consumption from Kafka topics. Schema Management: Integrates seamlessly with the Confluent Schema Registry to support schema validation and manage schema evolution effectively. More details on how to use the Kafka Source can be found here . Option 2: Built-in Kafka Source \u00b6 Numaflow provides a built-in Kafka source to ingest messages from a Kafka topic. The source uses consumer-groups to manage offsets. spec : vertices : - name : input source : kafka : brokers : - my-broker1:19700 - my-broker2:19700 topic : my-topic consumerGroup : my-consumer-group config : | # Optional. consumer: offsets: initial: -2 # -2 for sarama.OffsetOldest, -1 for sarama.OffsetNewest. Default to sarama.OffsetNewest. tls : # Optional. insecureSkipVerify : # Optional, whether to skip TLS verification. Default to false. caCertSecret : # Optional, a secret reference, which contains the CA Cert. name : my-ca-cert key : my-ca-cert-key certSecret : # Optional, pointing to a secret reference which contains the Cert. name : my-cert key : my-cert-key keySecret : # Optional, pointing to a secret reference which contains the Private Key. name : my-pk key : my-pk-key sasl : # Optional mechanism : GSSAPI # PLAIN, GSSAPI, SCRAM-SHA-256 or SCRAM-SHA-512, other mechanisms not supported gssapi : # Optional, for GSSAPI mechanism serviceName : my-service realm : my-realm # KRB5_USER_AUTH for auth using password # KRB5_KEYTAB_AUTH for auth using keytab authType : KRB5_KEYTAB_AUTH usernameSecret : # Pointing to a secret reference which contains the username name : gssapi-username key : gssapi-username-key # Pointing to a secret reference which contains the keytab (authType: KRB5_KEYTAB_AUTH) keytabSecret : name : gssapi-keytab key : gssapi-keytab-key # Pointing to a secret reference which contains the keytab (authType: KRB5_USER_AUTH) passwordSecret : name : gssapi-password key : gssapi-password-key kerberosConfigSecret : # Pointing to a secret reference which contains the kerberos config name : my-kerberos-config key : my-kerberos-config-key plain : # Optional, for PLAIN mechanism userSecret : # Pointing to a secret reference which contains the user name : plain-user key : plain-user-key passwordSecret : # Pointing to a secret reference which contains the password name : plain-password key : plain-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true scramsha256 : # Optional, for SCRAM-SHA-256 mechanism userSecret : # Pointing to a secret reference which contains the user name : scram-sha-256-user key : scram-sha-256-user-key passwordSecret : # Pointing to a secret reference which contains the password name : scram-sha-256-password key : scram-sha-256-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true scramsha512 : # Optional, for SCRAM-SHA-512 mechanism userSecret : # Pointing to a secret reference which contains the user name : scram-sha-512-user key : scram-sha-512-user-key passwordSecret : # Pointing to a secret reference which contains the password name : scram-sha-512-password key : scram-sha-512-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true oauth : #Optional, for OAUTHBEARER mechanism clientID : # Pointing to a secret reference which contains the client id name : kafka-oauth-client key : clientid clientSecret : # Pointing to a secret reference which contains the client secret name : kafka-oauth-client key : clientsecret tokenEndpoint : https://oauth-token.com/v1/token FAQ \u00b6 How to start the Kafka Source from a specific offset based on datetime? \u00b6 In order to start the Kafka Source from a specific offset based on datetime, we need to reset the offset before we start the pipeline. For example, we have a topic quickstart-events with 3 partitions and a consumer group console-consumer-94457 . This example uses Kafka 3.6.1 and localhost. \u279c kafka_2.13-3.6.1 bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic quickstart-events Topic: quickstart-events TopicId: WqIN6j7hTQqGZUQWdF7AdA PartitionCount: 3 ReplicationFactor: 1 Configs: Topic: quickstart-events Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: quickstart-events Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Topic: quickstart-events Partition: 2 Leader: 0 Replicas: 0 Isr: 0 \u279c kafka_2.13-3.6.1 bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list --all-groups console-consumer-94457 We have already consumed all the available messages in the topic quickstart-events , but we want to go back to some datetime and re-consume the data from that datetime. \u279c kafka_2.13-3.6.1 bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group console-consumer-94457 GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID console-consumer-94457 quickstart-events 0 56 56 0 - - - console-consumer-94457 quickstart-events 1 38 38 0 - - - console-consumer-94457 quickstart-events 2 4 4 0 - - - To achieve that, before the pipeline start, we need to first stop the consumers in the consumer group console-consumer-94457 because offsets can only be reset if the group console-consumer-94457 is inactive. Then, reset the offsets using the desired date and time. The example command below uses UTC time. \u279c kafka_2.13-3.6.1 bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --execute --reset-offsets --group console-consumer-94457 --topic quickstart-events --to-datetime 2024 -01-19T19:26:00.000 GROUP TOPIC PARTITION NEW-OFFSET console-consumer-94457 quickstart-events 0 54 console-consumer-94457 quickstart-events 1 26 console-consumer-94457 quickstart-events 2 0 Now, we can start the pipeline, and the Kafka source will start consuming the topic quickstart-events with consumer group console-consumer-94457 from the NEW-OFFSET . You may need to create a property file which contains the connectivity details and use it to connect to the clusters. Below are two example config.properties files: SASL/PLAIN and TSL . ssl.endpoint.identification.algorithm=https sasl.mechanism=PLAIN request.timeout.ms=20000 bootstrap.servers=<BOOSTRAP_BROKER_LIST> retry.backoff.ms=500 sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\ username=\"<CLUSTER_API_KEY>\" \\ password=\"<CLUSTER_API_SECRET>\"; security.protocol=SASL_SSL request.timeout.ms=20000 bootstrap.servers=<BOOSTRAP_BROKER_LIST> security.protocol=SSL ssl.enabled.protocols=TLSv1.2 ssl.truststore.location=<JKS_FILE_PATH> ssl.truststore.password=<PASSWORD> Run the command with the --command-config option. bin/kafka-consumer-groups.sh --bootstrap-server <BOOTSTRAP_BROKER_LIST> --command-config config.properties --execute --reset-offsets --group <GROUP_NAME> --topic <TOPIC_NAME> --to-datetime <DATETIME_STRING> Reference: - How to Use Kafka Tools With Confluent Cloud - Apache Kafka Security","title":"Kafka Source"},{"location":"user-guide/sources/kafka/#kafka-source","text":"Two methods are available for integrating Kafka topics into your Numaflow pipeline: using a user-defined Kafka Source or opting for the built-in Kafka Source provided by Numaflow.","title":"Kafka Source"},{"location":"user-guide/sources/kafka/#option-1-user-defined-kafka-source","text":"Developed and maintained by the Numaflow contributor community, the Kafka Source offers a robust and feature-complete solution for integrating Kafka as a data source into your Numaflow pipeline. Key Features: Flexibility: Allows full customization of Kafka Source configurations to suit specific needs. Kafka Java Client Utilization: Leverages the Kafka Java client for robust message consumption from Kafka topics. Schema Management: Integrates seamlessly with the Confluent Schema Registry to support schema validation and manage schema evolution effectively. More details on how to use the Kafka Source can be found here .","title":"Option 1: User-Defined Kafka Source"},{"location":"user-guide/sources/kafka/#option-2-built-in-kafka-source","text":"Numaflow provides a built-in Kafka source to ingest messages from a Kafka topic. The source uses consumer-groups to manage offsets. spec : vertices : - name : input source : kafka : brokers : - my-broker1:19700 - my-broker2:19700 topic : my-topic consumerGroup : my-consumer-group config : | # Optional. consumer: offsets: initial: -2 # -2 for sarama.OffsetOldest, -1 for sarama.OffsetNewest. Default to sarama.OffsetNewest. tls : # Optional. insecureSkipVerify : # Optional, whether to skip TLS verification. Default to false. caCertSecret : # Optional, a secret reference, which contains the CA Cert. name : my-ca-cert key : my-ca-cert-key certSecret : # Optional, pointing to a secret reference which contains the Cert. name : my-cert key : my-cert-key keySecret : # Optional, pointing to a secret reference which contains the Private Key. name : my-pk key : my-pk-key sasl : # Optional mechanism : GSSAPI # PLAIN, GSSAPI, SCRAM-SHA-256 or SCRAM-SHA-512, other mechanisms not supported gssapi : # Optional, for GSSAPI mechanism serviceName : my-service realm : my-realm # KRB5_USER_AUTH for auth using password # KRB5_KEYTAB_AUTH for auth using keytab authType : KRB5_KEYTAB_AUTH usernameSecret : # Pointing to a secret reference which contains the username name : gssapi-username key : gssapi-username-key # Pointing to a secret reference which contains the keytab (authType: KRB5_KEYTAB_AUTH) keytabSecret : name : gssapi-keytab key : gssapi-keytab-key # Pointing to a secret reference which contains the keytab (authType: KRB5_USER_AUTH) passwordSecret : name : gssapi-password key : gssapi-password-key kerberosConfigSecret : # Pointing to a secret reference which contains the kerberos config name : my-kerberos-config key : my-kerberos-config-key plain : # Optional, for PLAIN mechanism userSecret : # Pointing to a secret reference which contains the user name : plain-user key : plain-user-key passwordSecret : # Pointing to a secret reference which contains the password name : plain-password key : plain-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true scramsha256 : # Optional, for SCRAM-SHA-256 mechanism userSecret : # Pointing to a secret reference which contains the user name : scram-sha-256-user key : scram-sha-256-user-key passwordSecret : # Pointing to a secret reference which contains the password name : scram-sha-256-password key : scram-sha-256-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true scramsha512 : # Optional, for SCRAM-SHA-512 mechanism userSecret : # Pointing to a secret reference which contains the user name : scram-sha-512-user key : scram-sha-512-user-key passwordSecret : # Pointing to a secret reference which contains the password name : scram-sha-512-password key : scram-sha-512-password-key # Send the Kafka SASL handshake first if enabled (defaults to true) # Set this to false if using a non-Kafka SASL proxy handshake : true oauth : #Optional, for OAUTHBEARER mechanism clientID : # Pointing to a secret reference which contains the client id name : kafka-oauth-client key : clientid clientSecret : # Pointing to a secret reference which contains the client secret name : kafka-oauth-client key : clientsecret tokenEndpoint : https://oauth-token.com/v1/token","title":"Option 2: Built-in Kafka Source"},{"location":"user-guide/sources/kafka/#faq","text":"","title":"FAQ"},{"location":"user-guide/sources/kafka/#how-to-start-the-kafka-source-from-a-specific-offset-based-on-datetime","text":"In order to start the Kafka Source from a specific offset based on datetime, we need to reset the offset before we start the pipeline. For example, we have a topic quickstart-events with 3 partitions and a consumer group console-consumer-94457 . This example uses Kafka 3.6.1 and localhost. \u279c kafka_2.13-3.6.1 bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic quickstart-events Topic: quickstart-events TopicId: WqIN6j7hTQqGZUQWdF7AdA PartitionCount: 3 ReplicationFactor: 1 Configs: Topic: quickstart-events Partition: 0 Leader: 0 Replicas: 0 Isr: 0 Topic: quickstart-events Partition: 1 Leader: 0 Replicas: 0 Isr: 0 Topic: quickstart-events Partition: 2 Leader: 0 Replicas: 0 Isr: 0 \u279c kafka_2.13-3.6.1 bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list --all-groups console-consumer-94457 We have already consumed all the available messages in the topic quickstart-events , but we want to go back to some datetime and re-consume the data from that datetime. \u279c kafka_2.13-3.6.1 bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group console-consumer-94457 GROUP TOPIC PARTITION CURRENT-OFFSET LOG-END-OFFSET LAG CONSUMER-ID HOST CLIENT-ID console-consumer-94457 quickstart-events 0 56 56 0 - - - console-consumer-94457 quickstart-events 1 38 38 0 - - - console-consumer-94457 quickstart-events 2 4 4 0 - - - To achieve that, before the pipeline start, we need to first stop the consumers in the consumer group console-consumer-94457 because offsets can only be reset if the group console-consumer-94457 is inactive. Then, reset the offsets using the desired date and time. The example command below uses UTC time. \u279c kafka_2.13-3.6.1 bin/kafka-consumer-groups.sh --bootstrap-server localhost:9092 --execute --reset-offsets --group console-consumer-94457 --topic quickstart-events --to-datetime 2024 -01-19T19:26:00.000 GROUP TOPIC PARTITION NEW-OFFSET console-consumer-94457 quickstart-events 0 54 console-consumer-94457 quickstart-events 1 26 console-consumer-94457 quickstart-events 2 0 Now, we can start the pipeline, and the Kafka source will start consuming the topic quickstart-events with consumer group console-consumer-94457 from the NEW-OFFSET . You may need to create a property file which contains the connectivity details and use it to connect to the clusters. Below are two example config.properties files: SASL/PLAIN and TSL . ssl.endpoint.identification.algorithm=https sasl.mechanism=PLAIN request.timeout.ms=20000 bootstrap.servers=<BOOSTRAP_BROKER_LIST> retry.backoff.ms=500 sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required \\ username=\"<CLUSTER_API_KEY>\" \\ password=\"<CLUSTER_API_SECRET>\"; security.protocol=SASL_SSL request.timeout.ms=20000 bootstrap.servers=<BOOSTRAP_BROKER_LIST> security.protocol=SSL ssl.enabled.protocols=TLSv1.2 ssl.truststore.location=<JKS_FILE_PATH> ssl.truststore.password=<PASSWORD> Run the command with the --command-config option. bin/kafka-consumer-groups.sh --bootstrap-server <BOOTSTRAP_BROKER_LIST> --command-config config.properties --execute --reset-offsets --group <GROUP_NAME> --topic <TOPIC_NAME> --to-datetime <DATETIME_STRING> Reference: - How to Use Kafka Tools With Confluent Cloud - Apache Kafka Security","title":"How to start the Kafka Source from a specific offset based on datetime?"},{"location":"user-guide/sources/nats/","text":"Nats Source \u00b6 A Nats source is used to ingest the messages from a nats subject. spec : vertices : - name : input source : nats : url : nats://demo.nats.io # Multiple urls separated by comma. subject : my-subject queue : my-queue # Queue subscription, see https://docs.nats.io/using-nats/developer/receiving/queues tls : # Optional. insecureSkipVerify : # Optional, whether to skip TLS verification. Default to false. caCertSecret : # Optional, a secret reference, which contains the CA Cert. name : my-ca-cert key : my-ca-cert-key certSecret : # Optional, pointing to a secret reference which contains the Cert. name : my-cert key : my-cert-key keySecret : # Optional, pointing to a secret reference which contains the Private Key. name : my-pk key : my-pk-key auth : # Optional. basic : # Optional, pointing to the secret references which contain user name and password. user : name : my-secret key : my-user password : name : my-secret key : my-password Auth \u00b6 The auth strategies supported in nats source include basic (user and password), token and nkey , check the API for the details.","title":"Nats Source"},{"location":"user-guide/sources/nats/#nats-source","text":"A Nats source is used to ingest the messages from a nats subject. spec : vertices : - name : input source : nats : url : nats://demo.nats.io # Multiple urls separated by comma. subject : my-subject queue : my-queue # Queue subscription, see https://docs.nats.io/using-nats/developer/receiving/queues tls : # Optional. insecureSkipVerify : # Optional, whether to skip TLS verification. Default to false. caCertSecret : # Optional, a secret reference, which contains the CA Cert. name : my-ca-cert key : my-ca-cert-key certSecret : # Optional, pointing to a secret reference which contains the Cert. name : my-cert key : my-cert-key keySecret : # Optional, pointing to a secret reference which contains the Private Key. name : my-pk key : my-pk-key auth : # Optional. basic : # Optional, pointing to the secret references which contain user name and password. user : name : my-secret key : my-user password : name : my-secret key : my-password","title":"Nats Source"},{"location":"user-guide/sources/nats/#auth","text":"The auth strategies supported in nats source include basic (user and password), token and nkey , check the API for the details.","title":"Auth"},{"location":"user-guide/sources/overview/","text":"Sources \u00b6 Source vertex is responsible for reliable reading data from an unbounded source into Numaflow. Source vertex may require transformation or formatting of data prior to sending it to the output buffers. Source Vertex also does Watermark tracking and late data detection. In Numaflow, we currently support the following sources Kafka HTTP Ticker Nats Jetstream User-defined Source A user-defined source is a custom source that a user can write using Numaflow SDK when the user needs to read data from a system that is not supported by the platform's built-in sources. User-defined source also supports custom acknowledge management for exactly-once reading.","title":"Overview"},{"location":"user-guide/sources/overview/#sources","text":"Source vertex is responsible for reliable reading data from an unbounded source into Numaflow. Source vertex may require transformation or formatting of data prior to sending it to the output buffers. Source Vertex also does Watermark tracking and late data detection. In Numaflow, we currently support the following sources Kafka HTTP Ticker Nats Jetstream User-defined Source A user-defined source is a custom source that a user can write using Numaflow SDK when the user needs to read data from a system that is not supported by the platform's built-in sources. User-defined source also supports custom acknowledge management for exactly-once reading.","title":"Sources"},{"location":"user-guide/sources/pulsar/","text":"Pulsar Source \u00b6 NOTE: 1.5 Feature, not available Numaflow version < 1.5 \u00b6 A Pulsar source is used to ingest the messages from a Pulsar topic. apiVersion : v1 kind : Secret metadata : name : pulsar type : Opaque data : token : ZXlKaGJHY2lPaUpJVXpJMU5pSjkuZXlKemRXSWlPaUowWlhOMExYVnpaWElpZlEuZkRTWFFOcEdBWUN4anN1QlZzSDRTM2VLOVlZdHpwejhfdkFZcUxwVHAybwo= --- apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : in source : pulsar : serverAddr : \"pulsar+ssl://borker.example.com:6651\" consumerName : my_consumer topic : my_topic subscriptionName : my_subscription auth : # Optional token : # Optional, pointing to a secret reference which contains the JWT Token. name : pulsar key : token We have only tested the 4.0.x LTS version of Pulsar. Currently, the implementation only supports JWT token based authentication. If the auth field is not specified, Numaflow will connect to the Pulsar servers without authentication. More authentication mechanisms and the ability to customize Pulsar consumer will be added in the future.","title":"Pulsar Source"},{"location":"user-guide/sources/pulsar/#pulsar-source","text":"","title":"Pulsar Source"},{"location":"user-guide/sources/pulsar/#note-15-feature-not-available-numaflow-version-15","text":"A Pulsar source is used to ingest the messages from a Pulsar topic. apiVersion : v1 kind : Secret metadata : name : pulsar type : Opaque data : token : ZXlKaGJHY2lPaUpJVXpJMU5pSjkuZXlKemRXSWlPaUowWlhOMExYVnpaWElpZlEuZkRTWFFOcEdBWUN4anN1QlZzSDRTM2VLOVlZdHpwejhfdkFZcUxwVHAybwo= --- apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-pipeline spec : vertices : - name : in source : pulsar : serverAddr : \"pulsar+ssl://borker.example.com:6651\" consumerName : my_consumer topic : my_topic subscriptionName : my_subscription auth : # Optional token : # Optional, pointing to a secret reference which contains the JWT Token. name : pulsar key : token We have only tested the 4.0.x LTS version of Pulsar. Currently, the implementation only supports JWT token based authentication. If the auth field is not specified, Numaflow will connect to the Pulsar servers without authentication. More authentication mechanisms and the ability to customize Pulsar consumer will be added in the future.","title":"NOTE: 1.5 Feature, not available Numaflow version &lt; 1.5"},{"location":"user-guide/sources/sqs/","text":"Setting up Numaflow MonoVertex with SQS Source \u00b6 This guide explains how to set up a Numaflow MonoVertex that reads from an AWS SQS queue. Configuring Credentials to access AWS \u00b6 There are couple of ways you could achieve this. Creating AWS Credentials Secret \u00b6 First, we need to create a Kubernetes secret to store AWS credentials securely and encode AWS Credentials. # Encode your AWS credentials (replace with your actual credentials) ACCESS_KEY_ID = $( echo -n \"your-aws-access-key-id\" | base64 ) SECRET_ACCESS_KEY = $( echo -n \"your-aws-secret-access-key\" | base64 ) kubectl create secret generic aws-secret \\ --from-literal access-key-id = ${ ACCESS_KEY_ID } \\ --from-literal secret-access-key = ${ SECRET_ACCESS_KEY } IAM ROLE \u00b6 Use an IAM role with the necessary permissions to access the SQS queue. Ensure the IAM role of the pod has access to the SQS queue, and the SQS queue policy allows the IAM role to perform actions on the queue. Attach the appropriate service account with the IAM role to the pod. For more details, refer to the AWS documentation: https://docs.aws.amazon.com/eks/latest/userguide/pod-id-how-it-works.html Create the numaflow pipeline \u00b6 Create a file named sqs-pl.yaml with either of the following content. MonoVertex Specification \u00b6 apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : sqs-reader spec : source : sqs : queueName : \"your-queue-name\" # Required: Name of your SQS queue awsRegion : \"your-aws-region\" # Required: AWS region where queue is located queueOwnerAWSAccountID : \"123456789012\" # Required: AWS account ID of the queue owner # Optional configurations maxNumberOfMessages : 10 # Max messages per poll (1-10) visibilityTimeout : 30 # Visibility timeout in seconds waitTimeSeconds : 20 # Long polling wait time attributeNames : # SQS attributes to retrieve - All messageAttributeNames : # Message attributes to retrieve - All sink : log : {} # Simple log sink for testing limits : readBatchSize : 10 bufferSize : 100 scale : min : 1 max : 5 Pipeline Specification \u00b6 apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : sqs-source-pl spec : vertices : - name : in scale : min : 1 source : sqs : queueName : \"queue-name\" awsRegion : \"us-west-2\" queueOwnerAWSAccountID : \"123456789012\" # Required: AWS account ID of the queue owner # Optional configurations maxNumberOfMessages : 10 # Max messages per poll (1-10) visibilityTimeout : 30 # Visibility timeout in seconds waitTimeSeconds : 20 # Long polling wait time attributeNames : # SQS attributes to retrieve - All messageAttributeNames : # Message attributes to retrieve - All - name : out scale : min : 1 sink : log : {} edges : - from : in to : out Apply the Configuration \u00b6 Apply the pipeline specification: kubectl apply -f sqs-pl.yaml Verify the Setup \u00b6 Check that the MonoVertex is running: kubectl get monovertex sqs-reader kubectl get pods -l numaflow.numaproj.io/vertex-name = sqs-reader Miscellaneous \u00b6 Troubleshooting \u00b6 Check the pods logs for any errors: kubectl logs -l numaflow.numaproj.io/vertex-name = sqs-reader Verify the secret exists: kubectl get secret aws-secret Ensure the SQS queue exists and is accessible with the provided credentials","title":"SQS Source"},{"location":"user-guide/sources/sqs/#setting-up-numaflow-monovertex-with-sqs-source","text":"This guide explains how to set up a Numaflow MonoVertex that reads from an AWS SQS queue.","title":"Setting up Numaflow MonoVertex with SQS Source"},{"location":"user-guide/sources/sqs/#configuring-credentials-to-access-aws","text":"There are couple of ways you could achieve this.","title":"Configuring Credentials to access AWS"},{"location":"user-guide/sources/sqs/#creating-aws-credentials-secret","text":"First, we need to create a Kubernetes secret to store AWS credentials securely and encode AWS Credentials. # Encode your AWS credentials (replace with your actual credentials) ACCESS_KEY_ID = $( echo -n \"your-aws-access-key-id\" | base64 ) SECRET_ACCESS_KEY = $( echo -n \"your-aws-secret-access-key\" | base64 ) kubectl create secret generic aws-secret \\ --from-literal access-key-id = ${ ACCESS_KEY_ID } \\ --from-literal secret-access-key = ${ SECRET_ACCESS_KEY }","title":"Creating AWS Credentials Secret"},{"location":"user-guide/sources/sqs/#iam-role","text":"Use an IAM role with the necessary permissions to access the SQS queue. Ensure the IAM role of the pod has access to the SQS queue, and the SQS queue policy allows the IAM role to perform actions on the queue. Attach the appropriate service account with the IAM role to the pod. For more details, refer to the AWS documentation: https://docs.aws.amazon.com/eks/latest/userguide/pod-id-how-it-works.html","title":"IAM ROLE"},{"location":"user-guide/sources/sqs/#create-the-numaflow-pipeline","text":"Create a file named sqs-pl.yaml with either of the following content.","title":"Create the numaflow pipeline"},{"location":"user-guide/sources/sqs/#monovertex-specification","text":"apiVersion : numaflow.numaproj.io/v1alpha1 kind : MonoVertex metadata : name : sqs-reader spec : source : sqs : queueName : \"your-queue-name\" # Required: Name of your SQS queue awsRegion : \"your-aws-region\" # Required: AWS region where queue is located queueOwnerAWSAccountID : \"123456789012\" # Required: AWS account ID of the queue owner # Optional configurations maxNumberOfMessages : 10 # Max messages per poll (1-10) visibilityTimeout : 30 # Visibility timeout in seconds waitTimeSeconds : 20 # Long polling wait time attributeNames : # SQS attributes to retrieve - All messageAttributeNames : # Message attributes to retrieve - All sink : log : {} # Simple log sink for testing limits : readBatchSize : 10 bufferSize : 100 scale : min : 1 max : 5","title":"MonoVertex Specification"},{"location":"user-guide/sources/sqs/#pipeline-specification","text":"apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : sqs-source-pl spec : vertices : - name : in scale : min : 1 source : sqs : queueName : \"queue-name\" awsRegion : \"us-west-2\" queueOwnerAWSAccountID : \"123456789012\" # Required: AWS account ID of the queue owner # Optional configurations maxNumberOfMessages : 10 # Max messages per poll (1-10) visibilityTimeout : 30 # Visibility timeout in seconds waitTimeSeconds : 20 # Long polling wait time attributeNames : # SQS attributes to retrieve - All messageAttributeNames : # Message attributes to retrieve - All - name : out scale : min : 1 sink : log : {} edges : - from : in to : out","title":"Pipeline Specification"},{"location":"user-guide/sources/sqs/#apply-the-configuration","text":"Apply the pipeline specification: kubectl apply -f sqs-pl.yaml","title":"Apply the Configuration"},{"location":"user-guide/sources/sqs/#verify-the-setup","text":"Check that the MonoVertex is running: kubectl get monovertex sqs-reader kubectl get pods -l numaflow.numaproj.io/vertex-name = sqs-reader","title":"Verify the Setup"},{"location":"user-guide/sources/sqs/#miscellaneous","text":"","title":"Miscellaneous"},{"location":"user-guide/sources/sqs/#troubleshooting","text":"Check the pods logs for any errors: kubectl logs -l numaflow.numaproj.io/vertex-name = sqs-reader Verify the secret exists: kubectl get secret aws-secret Ensure the SQS queue exists and is accessible with the provided credentials","title":"Troubleshooting"},{"location":"user-guide/sources/user-defined-sources/","text":"User-defined Sources \u00b6 A Pipeline may have multiple Sources, those sources could either be a pre-defined source such as kafka , http , etc., or a user-defined source . With no source data transformer, A pre-defined source vertex runs single-container pods; a user-defined source runs two-container pods. Build Your Own User-defined Sources \u00b6 You can build your own user-defined sources in multiple languages. Check the links below to see the examples for different languages. Golang Java Python After building a docker image for the written user-defined source, specify the image as below in the vertex spec. spec : vertices : - name : input source : udsource : container : image : my-source:latest Available Environment Variables \u00b6 Some environment variables are available in the user-defined source container: NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex. User-defined sources contributed from the open source community \u00b6 If you're looking for examples and usages contributed by the open source community, head over to the numaproj-contrib repositories . These user-defined sources like AWS SQS, GCP Pub/Sub, provide valuable insights and guidance on how to use and write a user-defined source.","title":"User-defined Sources"},{"location":"user-guide/sources/user-defined-sources/#user-defined-sources","text":"A Pipeline may have multiple Sources, those sources could either be a pre-defined source such as kafka , http , etc., or a user-defined source . With no source data transformer, A pre-defined source vertex runs single-container pods; a user-defined source runs two-container pods.","title":"User-defined Sources"},{"location":"user-guide/sources/user-defined-sources/#build-your-own-user-defined-sources","text":"You can build your own user-defined sources in multiple languages. Check the links below to see the examples for different languages. Golang Java Python After building a docker image for the written user-defined source, specify the image as below in the vertex spec. spec : vertices : - name : input source : udsource : container : image : my-source:latest","title":"Build Your Own User-defined Sources"},{"location":"user-guide/sources/user-defined-sources/#available-environment-variables","text":"Some environment variables are available in the user-defined source container: NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex.","title":"Available Environment Variables"},{"location":"user-guide/sources/user-defined-sources/#user-defined-sources-contributed-from-the-open-source-community","text":"If you're looking for examples and usages contributed by the open source community, head over to the numaproj-contrib repositories . These user-defined sources like AWS SQS, GCP Pub/Sub, provide valuable insights and guidance on how to use and write a user-defined source.","title":"User-defined sources contributed from the open source community"},{"location":"user-guide/sources/transformer/overview/","text":"Source Data Transformer \u00b6 The Source Data Transformer is a feature that allows users to execute custom code to transform their data at source. This functionality offers two primary advantages to users: Event Time Assignment - It enables users to extract the event time from the message payload, providing a more precise and accurate event time than the default mechanisms like LOG_APPEND_TIME of Kafka for Kafka source, custom HTTP header for HTTP source, and others. Early data processing - It pre-processes the data, or filters out unwanted data at source vertex, saving the cost of creating another UDF vertex and an inter-step buffer. Source Data Transformer runs as a sidecar container in a Source Vertex Pod. Data processing in the transformer is supposed to be idempotent. The communication between the main container (platform code) and the sidecar container (user code) is through gRPC over Unix Domain Socket. Build Your Own Transformer \u00b6 You can build your own transformer in multiple languages. A user-defined transformer could be as simple as the example below in Golang. In the example, the transformer extracts event times from timestamp of the JSON payload and assigns them to messages as new event times. It also filters out unwanted messages based on filterOut of the payload. package main import ( \"context\" \"encoding/json\" \"log\" \"time\" \"github.com/numaproj/numaflow-go/pkg/sourcetransformer\" ) func transform ( _ context . Context , keys [] string , data sourcetransformer . Datum ) sourcetransformer . Messages { /* Input messages are in JSON format. Sample: {\"timestamp\": \"1673239888\", \"filterOut\": \"true\"}. Field \"timestamp\" shows the real event time of the message, in the format of epoch. Field \"filterOut\" indicates whether the message should be filtered out, in the format of boolean. */ var jsonObject map [ string ] interface {} json . Unmarshal ( data . Value (), & jsonObject ) // event time assignment eventTime := data . EventTime () // if timestamp field exists, extract event time from payload. if ts , ok := jsonObject [ \"timestamp\" ]; ok { eventTime = time . Unix ( int64 ( ts .( float64 )), 0 ) } // data filtering var filterOut bool if f , ok := jsonObject [ \"filterOut\" ]; ok { filterOut = f .( bool ) } if filterOut { return sourcetransformer . MessagesBuilder (). Append ( sourcetransformer . MessageToDrop ( eventTime )) } else { return sourcetransformer . MessagesBuilder (). Append ( sourcetransformer . NewMessage ( data . Value (), eventTime ). WithKeys ( keys )) } } func main () { err := sourcetransformer . NewServer ( sourcetransformer . SourceTransformFunc ( transform )). Start ( context . Background ()) if err != nil { log . Panic ( \"Failed to start source transform server: \" , err ) } } Check the links below to see another transformer example in various programming languages, where we apply conditional forwarding based on the input event time. Python Golang Java After building a docker image for the written transformer, specify the image as below in the source vertex spec. spec : vertices : - name : my-vertex source : http : {} transformer : container : image : my-python-transformer-example:latest Available Environment Variables \u00b6 Some environment variables are available in the source transformer container, they might be useful in you own source data transformer implementation. NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex. Configuration \u00b6 Configuration data can be provided to the transformer container at runtime multiple ways. environment variables args command volumes init containers","title":"Overview"},{"location":"user-guide/sources/transformer/overview/#source-data-transformer","text":"The Source Data Transformer is a feature that allows users to execute custom code to transform their data at source. This functionality offers two primary advantages to users: Event Time Assignment - It enables users to extract the event time from the message payload, providing a more precise and accurate event time than the default mechanisms like LOG_APPEND_TIME of Kafka for Kafka source, custom HTTP header for HTTP source, and others. Early data processing - It pre-processes the data, or filters out unwanted data at source vertex, saving the cost of creating another UDF vertex and an inter-step buffer. Source Data Transformer runs as a sidecar container in a Source Vertex Pod. Data processing in the transformer is supposed to be idempotent. The communication between the main container (platform code) and the sidecar container (user code) is through gRPC over Unix Domain Socket.","title":"Source Data Transformer"},{"location":"user-guide/sources/transformer/overview/#build-your-own-transformer","text":"You can build your own transformer in multiple languages. A user-defined transformer could be as simple as the example below in Golang. In the example, the transformer extracts event times from timestamp of the JSON payload and assigns them to messages as new event times. It also filters out unwanted messages based on filterOut of the payload. package main import ( \"context\" \"encoding/json\" \"log\" \"time\" \"github.com/numaproj/numaflow-go/pkg/sourcetransformer\" ) func transform ( _ context . Context , keys [] string , data sourcetransformer . Datum ) sourcetransformer . Messages { /* Input messages are in JSON format. Sample: {\"timestamp\": \"1673239888\", \"filterOut\": \"true\"}. Field \"timestamp\" shows the real event time of the message, in the format of epoch. Field \"filterOut\" indicates whether the message should be filtered out, in the format of boolean. */ var jsonObject map [ string ] interface {} json . Unmarshal ( data . Value (), & jsonObject ) // event time assignment eventTime := data . EventTime () // if timestamp field exists, extract event time from payload. if ts , ok := jsonObject [ \"timestamp\" ]; ok { eventTime = time . Unix ( int64 ( ts .( float64 )), 0 ) } // data filtering var filterOut bool if f , ok := jsonObject [ \"filterOut\" ]; ok { filterOut = f .( bool ) } if filterOut { return sourcetransformer . MessagesBuilder (). Append ( sourcetransformer . MessageToDrop ( eventTime )) } else { return sourcetransformer . MessagesBuilder (). Append ( sourcetransformer . NewMessage ( data . Value (), eventTime ). WithKeys ( keys )) } } func main () { err := sourcetransformer . NewServer ( sourcetransformer . SourceTransformFunc ( transform )). Start ( context . Background ()) if err != nil { log . Panic ( \"Failed to start source transform server: \" , err ) } } Check the links below to see another transformer example in various programming languages, where we apply conditional forwarding based on the input event time. Python Golang Java After building a docker image for the written transformer, specify the image as below in the source vertex spec. spec : vertices : - name : my-vertex source : http : {} transformer : container : image : my-python-transformer-example:latest","title":"Build Your Own Transformer"},{"location":"user-guide/sources/transformer/overview/#available-environment-variables","text":"Some environment variables are available in the source transformer container, they might be useful in you own source data transformer implementation. NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex.","title":"Available Environment Variables"},{"location":"user-guide/sources/transformer/overview/#configuration","text":"Configuration data can be provided to the transformer container at runtime multiple ways. environment variables args command volumes init containers","title":"Configuration"},{"location":"user-guide/sources/transformer/builtin-transformers/","text":"Built-in Functions [Deprecated] \u00b6 \u26a0\ufe0f Note: Builtins for transformer are deprecated and will be removed with 1.6 release. Numaflow provides some built-in source data transformers that can be used directly. Filter A filter built-in transformer filters the message based on expression. payload keyword represents message object. see documentation for filter expression here spec : vertices : - name : in source : http : {} transformer : builtin : name : filter kwargs : expression : int(json(payload).id) < 100 Event Time Extractor A eventTimeExtractor built-in transformer extracts event time from the payload of the message, based on expression and user-specified format. payload keyword represents message object. see documentation for event time extractor expression here . If you want to handle event times in epoch format, you can find helpful resource here . spec : vertices : - name : in source : http : {} transformer : builtin : name : eventTimeExtractor kwargs : expression : json(payload).item[0].time format : 2006-01-02T15:04:05Z07:00 Time Extraction Filter A timeExtractionFilter implements both the eventTimeExtractor and filter built-in functions. It evaluates a message on a pipeline and if valid, extracts event time from the payload of the messsage. spec : vertices : - name : in source : http : {} transformer : builtin : name : timeExtractionFilter kwargs : filterExpr : int(json(payload).id) < 100 eventTimeExpr : json(payload).item[1].time eventTimeFormat : 2006-01-02T15:04:05Z07:00","title":"Overview"},{"location":"user-guide/sources/transformer/builtin-transformers/#built-in-functions-deprecated","text":"\u26a0\ufe0f Note: Builtins for transformer are deprecated and will be removed with 1.6 release. Numaflow provides some built-in source data transformers that can be used directly. Filter A filter built-in transformer filters the message based on expression. payload keyword represents message object. see documentation for filter expression here spec : vertices : - name : in source : http : {} transformer : builtin : name : filter kwargs : expression : int(json(payload).id) < 100 Event Time Extractor A eventTimeExtractor built-in transformer extracts event time from the payload of the message, based on expression and user-specified format. payload keyword represents message object. see documentation for event time extractor expression here . If you want to handle event times in epoch format, you can find helpful resource here . spec : vertices : - name : in source : http : {} transformer : builtin : name : eventTimeExtractor kwargs : expression : json(payload).item[0].time format : 2006-01-02T15:04:05Z07:00 Time Extraction Filter A timeExtractionFilter implements both the eventTimeExtractor and filter built-in functions. It evaluates a message on a pipeline and if valid, extracts event time from the payload of the messsage. spec : vertices : - name : in source : http : {} transformer : builtin : name : timeExtractionFilter kwargs : filterExpr : int(json(payload).id) < 100 eventTimeExpr : json(payload).item[1].time eventTimeFormat : 2006-01-02T15:04:05Z07:00","title":"Built-in Functions [Deprecated]"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/","text":"Event Time Extractor [Deprecated] \u00b6 \u26a0\ufe0f Note: Builtins for transformer are deprecated. An example in Go for event time extractor can be found here. A eventTimeExtractor built-in transformer extracts event time from the payload of the message, based on a user-provided expression and an optional format specification. expression is used to compile the payload to a string representation of the event time. format is used to convert the event time in string format to a time.Time object. Expression (required) \u00b6 Event Time Extractor expression is implemented with expr and sprig libraries. Data conversion functions \u00b6 These function can be accessed directly in expression. payload keyword represents the message object. It will be the root element to represent the message object in expression. json - Convert payload in JSON object. e.g: json(payload) int - Convert element/payload into int value. e.g: int(json(payload).id) string - Convert element/payload into string value. e.g: string(json(payload).amount) Sprig functions \u00b6 Sprig library has 70+ functions. sprig prefix need to be added to access the sprig functions. sprig functions E.g: sprig.trim(string(json(payload).timestamp)) # Remove spaces from either side of the value of field timestamp Format (optional) \u00b6 Depending on whether a format is specified, Event Time Extractor uses different approaches to convert the event time string to a time.Time object. When specified \u00b6 When format is specified, the native time.Parse(layout, value string) library is used to make the conversion. In this process, the format parameter is passed as the layout input to the time.Parse() function, while the event time string is passed as the value parameter. When not specified \u00b6 When format is not specified, the extractor uses dateparse to parse the event time string without knowing the format in advance. How to specify format \u00b6 Please refer to golang format library . Error Scenarios \u00b6 When encountering parsing errors, event time extractor skips the extraction and passes on the message without modifying the original input message event time. Errors can occur for a variety of reasons, including: format is specified but the event time string can't parse to the specified format. format is not specified but dataparse can't convert the event time string to a time.Time object. Ambiguous event time strings \u00b6 Event time strings can be ambiguous when it comes to date format, such as MM/DD/YYYY versus DD/MM/YYYY. When using such format, you're required to explicitly specify format , to avoid confusion. If no format is provided, event time extractor treats ambiguous event time strings as an error scenario. Epoch format \u00b6 If the event time string in your message payload is in epoch format, you can skip specifying a format . You can rely on dateparse to recognize a wide range of epoch timestamp formats, including Unix seconds, milliseconds, microseconds, and nanoseconds. Event Time Extractor Spec \u00b6 spec : vertices : - name : in source : http : {} transformer : builtin : name : eventTimeExtractor kwargs : expression : sprig.trim(string(json(payload).timestamp)) format : 2006-01-02T15:04:05Z07:00","title":"Event Time Extractor"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#event-time-extractor-deprecated","text":"\u26a0\ufe0f Note: Builtins for transformer are deprecated. An example in Go for event time extractor can be found here. A eventTimeExtractor built-in transformer extracts event time from the payload of the message, based on a user-provided expression and an optional format specification. expression is used to compile the payload to a string representation of the event time. format is used to convert the event time in string format to a time.Time object.","title":"Event Time Extractor [Deprecated]"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#expression-required","text":"Event Time Extractor expression is implemented with expr and sprig libraries.","title":"Expression (required)"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#data-conversion-functions","text":"These function can be accessed directly in expression. payload keyword represents the message object. It will be the root element to represent the message object in expression. json - Convert payload in JSON object. e.g: json(payload) int - Convert element/payload into int value. e.g: int(json(payload).id) string - Convert element/payload into string value. e.g: string(json(payload).amount)","title":"Data conversion functions"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#sprig-functions","text":"Sprig library has 70+ functions. sprig prefix need to be added to access the sprig functions. sprig functions E.g: sprig.trim(string(json(payload).timestamp)) # Remove spaces from either side of the value of field timestamp","title":"Sprig functions"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#format-optional","text":"Depending on whether a format is specified, Event Time Extractor uses different approaches to convert the event time string to a time.Time object.","title":"Format (optional)"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#when-specified","text":"When format is specified, the native time.Parse(layout, value string) library is used to make the conversion. In this process, the format parameter is passed as the layout input to the time.Parse() function, while the event time string is passed as the value parameter.","title":"When specified"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#when-not-specified","text":"When format is not specified, the extractor uses dateparse to parse the event time string without knowing the format in advance.","title":"When not specified"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#how-to-specify-format","text":"Please refer to golang format library .","title":"How to specify format"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#error-scenarios","text":"When encountering parsing errors, event time extractor skips the extraction and passes on the message without modifying the original input message event time. Errors can occur for a variety of reasons, including: format is specified but the event time string can't parse to the specified format. format is not specified but dataparse can't convert the event time string to a time.Time object.","title":"Error Scenarios"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#ambiguous-event-time-strings","text":"Event time strings can be ambiguous when it comes to date format, such as MM/DD/YYYY versus DD/MM/YYYY. When using such format, you're required to explicitly specify format , to avoid confusion. If no format is provided, event time extractor treats ambiguous event time strings as an error scenario.","title":"Ambiguous event time strings"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#epoch-format","text":"If the event time string in your message payload is in epoch format, you can skip specifying a format . You can rely on dateparse to recognize a wide range of epoch timestamp formats, including Unix seconds, milliseconds, microseconds, and nanoseconds.","title":"Epoch format"},{"location":"user-guide/sources/transformer/builtin-transformers/event-time-extractor/#event-time-extractor-spec","text":"spec : vertices : - name : in source : http : {} transformer : builtin : name : eventTimeExtractor kwargs : expression : sprig.trim(string(json(payload).timestamp)) format : 2006-01-02T15:04:05Z07:00","title":"Event Time Extractor Spec"},{"location":"user-guide/sources/transformer/builtin-transformers/filter/","text":"Filter [Deprecated] \u00b6 \u26a0\ufe0f Note: Builtins for transformer are deprecated. An example in Go for filter can be found here. A filter is a special-purpose built-in function. It is used to evaluate on each message in a pipeline and is often used to filter the number of messages that are passed to next vertices. Filter function supports comprehensive expression language which extends flexibility write complex expressions. payload will be root element to represent the message object in expression. Expression \u00b6 Filter expression implemented with expr and sprig libraries. Data conversion functions \u00b6 These function can be accessed directly in expression. json - Convert payload in JSON object. e.g: json(payload) int - Convert element/payload into int value. e.g: int(json(payload).id) string - Convert element/payload into string value. e.g: string(json(payload).amount) Sprig functions \u00b6 Sprig library has 70+ functions. sprig prefix need to be added to access the sprig functions. sprig functions E.g: sprig.contains('James', json(payload).name) # James is contained in the value of name . int(json(sprig.b64dec(payload)).id) < 100 Filter Spec \u00b6 spec : vertices : - name : in source : http : {} transformer : builtin : name : filter kwargs : expression : int(json(payload).id) < 100","title":"Filter"},{"location":"user-guide/sources/transformer/builtin-transformers/filter/#filter-deprecated","text":"\u26a0\ufe0f Note: Builtins for transformer are deprecated. An example in Go for filter can be found here. A filter is a special-purpose built-in function. It is used to evaluate on each message in a pipeline and is often used to filter the number of messages that are passed to next vertices. Filter function supports comprehensive expression language which extends flexibility write complex expressions. payload will be root element to represent the message object in expression.","title":"Filter [Deprecated]"},{"location":"user-guide/sources/transformer/builtin-transformers/filter/#expression","text":"Filter expression implemented with expr and sprig libraries.","title":"Expression"},{"location":"user-guide/sources/transformer/builtin-transformers/filter/#data-conversion-functions","text":"These function can be accessed directly in expression. json - Convert payload in JSON object. e.g: json(payload) int - Convert element/payload into int value. e.g: int(json(payload).id) string - Convert element/payload into string value. e.g: string(json(payload).amount)","title":"Data conversion functions"},{"location":"user-guide/sources/transformer/builtin-transformers/filter/#sprig-functions","text":"Sprig library has 70+ functions. sprig prefix need to be added to access the sprig functions. sprig functions E.g: sprig.contains('James', json(payload).name) # James is contained in the value of name . int(json(sprig.b64dec(payload)).id) < 100","title":"Sprig functions"},{"location":"user-guide/sources/transformer/builtin-transformers/filter/#filter-spec","text":"spec : vertices : - name : in source : http : {} transformer : builtin : name : filter kwargs : expression : int(json(payload).id) < 100","title":"Filter Spec"},{"location":"user-guide/sources/transformer/builtin-transformers/time-extraction-filter/","text":"Time Extraction Filter [Deprecated] \u00b6 \u26a0\ufe0f Note: Builtins for transformer are deprecated. An example in Go for time extraction filter can be found here. A timeExtractionFilter built-in transformer implements both the eventTimeExtractor and filter built-in functions. It evaluates a message on a pipeline and if valid, extracts event time from the payload of the messsage. filterExpr is used to evaluate and drop invalid messages. eventTimeExpr is used to compile the payload to a string representation of the event time. format is used to convert the event time in string format to a time.Time object. Expression (required) \u00b6 The expressions for the filter and event time extractor are implemented with expr and sprig libraries. Data conversion functions \u00b6 These function can be accessed directly in expression. payload keyword represents the message object. It will be the root element to represent the message object in expression. json - Convert payload in JSON object. e.g: json(payload) int - Convert element/payload into int value. e.g: int(json(payload).id) string - Convert element/payload into string value. e.g: string(json(payload).amount) Sprig functions \u00b6 Sprig library has 70+ functions. sprig prefix need to be added to access the sprig functions. sprig functions E.g: sprig.trim(string(json(payload).timestamp)) # Remove spaces from either side of the value of field timestamp Format (optional) \u00b6 Depending on whether a format is specified, the Event Time Extractor uses different approaches to convert the event time string to a time.Time object. Time Extraction Filter Spec \u00b6 spec : vertices : - name : in source : http : {} transformer : builtin : name : timeExtractionFilter kwargs : filterExpr : int(json(payload).id) < 100 eventTimeExpr : json(payload).item[1].time eventTimeFormat : 2006-01-02T15:04:05Z07:00","title":"Event Time Extraction Filter"},{"location":"user-guide/sources/transformer/builtin-transformers/time-extraction-filter/#time-extraction-filter-deprecated","text":"\u26a0\ufe0f Note: Builtins for transformer are deprecated. An example in Go for time extraction filter can be found here. A timeExtractionFilter built-in transformer implements both the eventTimeExtractor and filter built-in functions. It evaluates a message on a pipeline and if valid, extracts event time from the payload of the messsage. filterExpr is used to evaluate and drop invalid messages. eventTimeExpr is used to compile the payload to a string representation of the event time. format is used to convert the event time in string format to a time.Time object.","title":"Time Extraction Filter [Deprecated]"},{"location":"user-guide/sources/transformer/builtin-transformers/time-extraction-filter/#expression-required","text":"The expressions for the filter and event time extractor are implemented with expr and sprig libraries.","title":"Expression (required)"},{"location":"user-guide/sources/transformer/builtin-transformers/time-extraction-filter/#data-conversion-functions","text":"These function can be accessed directly in expression. payload keyword represents the message object. It will be the root element to represent the message object in expression. json - Convert payload in JSON object. e.g: json(payload) int - Convert element/payload into int value. e.g: int(json(payload).id) string - Convert element/payload into string value. e.g: string(json(payload).amount)","title":"Data conversion functions"},{"location":"user-guide/sources/transformer/builtin-transformers/time-extraction-filter/#sprig-functions","text":"Sprig library has 70+ functions. sprig prefix need to be added to access the sprig functions. sprig functions E.g: sprig.trim(string(json(payload).timestamp)) # Remove spaces from either side of the value of field timestamp","title":"Sprig functions"},{"location":"user-guide/sources/transformer/builtin-transformers/time-extraction-filter/#format-optional","text":"Depending on whether a format is specified, the Event Time Extractor uses different approaches to convert the event time string to a time.Time object.","title":"Format (optional)"},{"location":"user-guide/sources/transformer/builtin-transformers/time-extraction-filter/#time-extraction-filter-spec","text":"spec : vertices : - name : in source : http : {} transformer : builtin : name : timeExtractionFilter kwargs : filterExpr : int(json(payload).id) < 100 eventTimeExpr : json(payload).item[1].time eventTimeFormat : 2006-01-02T15:04:05Z07:00","title":"Time Extraction Filter Spec"},{"location":"user-guide/use-cases/monitoring-and-observability/","text":"Monitoring and Observability \u00b6 Docs \u00b6 How Intuit platform engineers use Numaflow to compute golden signals . Videos \u00b6 Numaflow as the stream-processing solution in Intuit\u2019s Customer Centric Observability Journey Using AIOps Using Numaflow for fast incident detection: Argo CD Observability with AIOps - Detect Incident Fast Implementing anomaly detection with Numaflow: Cluster Golden Signals to Avoid Alert Fatigue at Scale Appendix: What is Monitoring and Observability? \u00b6 Monitoring and observability are two critical concepts in software engineering that help developers ensure the health and performance of their applications. Monitoring refers to the process of collecting and analyzing data about an application's performance. This data can include metrics such as CPU usage, memory usage, network traffic, and response times. Monitoring tools allow developers to track these metrics over time and set alerts when certain thresholds are exceeded. This enables them to quickly identify and respond to issues before they become critical. Observability, on the other hand, is a more holistic approach to monitoring that focuses on understanding the internal workings of an application. Observability tools provide developers with deep insights into the behavior of their applications, allowing them to understand how different components interact with each other and how changes in one area can affect the overall system. This includes collecting data on things like logs, traces, and events, which can be used to reconstruct the state of the system at any given point in time. Together, monitoring and observability provide developers with a comprehensive view of their applications' performance, enabling them to quickly identify and respond to issues as they arise. By leveraging these tools, software engineers can ensure that their applications are running smoothly and efficiently, delivering the best possible experience to their users.","title":"Monitoring and Observability"},{"location":"user-guide/use-cases/monitoring-and-observability/#monitoring-and-observability","text":"","title":"Monitoring and Observability"},{"location":"user-guide/use-cases/monitoring-and-observability/#docs","text":"How Intuit platform engineers use Numaflow to compute golden signals .","title":"Docs"},{"location":"user-guide/use-cases/monitoring-and-observability/#videos","text":"Numaflow as the stream-processing solution in Intuit\u2019s Customer Centric Observability Journey Using AIOps Using Numaflow for fast incident detection: Argo CD Observability with AIOps - Detect Incident Fast Implementing anomaly detection with Numaflow: Cluster Golden Signals to Avoid Alert Fatigue at Scale","title":"Videos"},{"location":"user-guide/use-cases/monitoring-and-observability/#appendix-what-is-monitoring-and-observability","text":"Monitoring and observability are two critical concepts in software engineering that help developers ensure the health and performance of their applications. Monitoring refers to the process of collecting and analyzing data about an application's performance. This data can include metrics such as CPU usage, memory usage, network traffic, and response times. Monitoring tools allow developers to track these metrics over time and set alerts when certain thresholds are exceeded. This enables them to quickly identify and respond to issues before they become critical. Observability, on the other hand, is a more holistic approach to monitoring that focuses on understanding the internal workings of an application. Observability tools provide developers with deep insights into the behavior of their applications, allowing them to understand how different components interact with each other and how changes in one area can affect the overall system. This includes collecting data on things like logs, traces, and events, which can be used to reconstruct the state of the system at any given point in time. Together, monitoring and observability provide developers with a comprehensive view of their applications' performance, enabling them to quickly identify and respond to issues as they arise. By leveraging these tools, software engineers can ensure that their applications are running smoothly and efficiently, delivering the best possible experience to their users.","title":"Appendix: What is Monitoring and Observability?"},{"location":"user-guide/use-cases/overview/","text":"Overview \u00b6 Numaflow allows developers without any special knowledge of data/stream processing to easily create massively parallel data/stream processing jobs using a programming language of their choice, with just basic knowledge of Kubernetes. In this section, you'll find sample use cases for Numaflow and learn how to leverage its features for your stream processing tasks. Real-time data analytics applications. Event-driven applications: anomaly detection and monitoring . Streaming applications: data instrumentation and movement. Any workflows running in a streaming manner. Numaflow is still a relatively new tool, and there are likely many other use cases that we haven't yet explored. We're committed to keeping this page up-to-date with the latest use cases and best practices for using Numaflow. We welcome contributions from the community and encourage you to share your own use cases and experiences with us. As we continue to develop and improve Numaflow, we look forward to seeing the cool things you build with it!","title":"Overview"},{"location":"user-guide/use-cases/overview/#overview","text":"Numaflow allows developers without any special knowledge of data/stream processing to easily create massively parallel data/stream processing jobs using a programming language of their choice, with just basic knowledge of Kubernetes. In this section, you'll find sample use cases for Numaflow and learn how to leverage its features for your stream processing tasks. Real-time data analytics applications. Event-driven applications: anomaly detection and monitoring . Streaming applications: data instrumentation and movement. Any workflows running in a streaming manner. Numaflow is still a relatively new tool, and there are likely many other use cases that we haven't yet explored. We're committed to keeping this page up-to-date with the latest use cases and best practices for using Numaflow. We welcome contributions from the community and encourage you to share your own use cases and experiences with us. As we continue to develop and improve Numaflow, we look forward to seeing the cool things you build with it!","title":"Overview"},{"location":"user-guide/user-defined-functions/user-defined-functions/","text":"A Pipeline consists of multiple vertices, Source , Sink and UDF(user-defined functions) . User-defined functions (UDF) is the vertex where users can run custom code to transform the data. Data processing in the UDF is supposed to be idempotent. UDF runs as a sidecar container in a Vertex Pod, processes the received data. The communication between the main container (platform code) and the sidecar container (user code) is through gRPC over Unix Domain Socket. There are two kinds of processing users can run Map Reduce","title":"Overview"},{"location":"user-guide/user-defined-functions/map/examples/","text":"Map Examples \u00b6 Please read map to get the best out of these examples. Prerequisites \u00b6 Inter-Step Buffer Service (ISB Service) \u00b6 What is ISB Service? \u00b6 An Inter-Step Buffer Service is described by a Custom Resource , which is used to pass data between vertices of a numaflow pipeline. Please refer to the doc Inter-Step Buffer Service for more information on ISB. How to install the ISB Service \u00b6 kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/stable/examples/0-isbsvc-jetstream.yaml The expected output of the above command is shown below: $ kubectl get isbsvc NAME TYPE PHASE MESSAGE AGE default jetstream Running 3d19h # Wait for pods to be ready $ kubectl get pods NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3 /3 Running 0 19s isbsvc-default-js-1 3 /3 Running 0 19s isbsvc-default-js-2 3 /3 Running 0 19s NOTE The Source used in the examples is an HTTP source producing messages with values 5 and 10 with event time starting from 60000. Please refer to the doc http source on how to use an HTTP source. An example will be as follows, curl -kq -X POST -H \"x-numaflow-event-time: 60000\" -d \"5\" ${ http -source-url } curl -kq -X POST -H \"x-numaflow-event-time: 60000\" -d \"10\" ${ http -source-url } Creating a Simple Map Pipeline \u00b6 Now we will walk you through creating a map pipeline. In our example, this is called the even-odd pipeline, illustrated by the following diagram: There are five vertices in this example of a map pipeline. An HTTP source vertex which serves an HTTP endpoint to receive numbers as source data, a UDF vertex to tag the ingested numbers with the key even or odd , three Log sinks, one to print the even numbers, one to print the odd numbers, and the other one to print both the even and odd numbers. Run the following command to create the even-odd pipeline. kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/2-even-odd-pipeline.yaml You may opt to view the list of pipelines you've created so far by running kubectl get pipeline . Otherwise, proceed to inspect the status of the pipeline, using kubectl get pods . # Wait for pods to be ready kubectl get pods NAME READY STATUS RESTARTS AGE even-odd-daemon-64d65c945d-vjs9f 1 /1 Running 0 5m3s even-odd-even-or-odd-0-pr4ze 2 /2 Running 0 30s even-odd-even-sink-0-unffo 1 /1 Running 0 22s even-odd-in-0-a7iyd 1 /1 Running 0 5m3s even-odd-number-sink-0-zmg2p 1 /1 Running 0 7s even-odd-odd-sink-0-2736r 1 /1 Running 0 15s isbsvc-default-js-0 3 /3 Running 0 10m isbsvc-default-js-1 3 /3 Running 0 10m isbsvc-default-js-2 3 /3 Running 0 10m Next, port-forward the HTTP endpoint, and make a POST request using curl . Remember to replace xxxxx with the appropriate pod names both here and in the next step. kubectl port-forward even-odd-in-0-xxxx 8444 :8443 # Post data to the HTTP endpoint curl -kq -X POST -d \"101\" https://localhost:8444/vertices/in curl -kq -X POST -d \"102\" https://localhost:8444/vertices/in curl -kq -X POST -d \"103\" https://localhost:8444/vertices/in curl -kq -X POST -d \"104\" https://localhost:8444/vertices/in Now you can watch the log for the even and odd vertices by running the commands below. # Watch the log for the even vertex kubectl logs -f even-odd-even-sink-0-xxxxx 2022 /09/07 22 :29:40 ( even-sink ) 102 2022 /09/07 22 :29:40 ( even-sink ) 104 # Watch the log for the odd vertex kubectl logs -f even-odd-odd-sink-0-xxxxx 2022 /09/07 22 :30:19 ( odd-sink ) 101 2022 /09/07 22 :30:19 ( odd-sink ) 103 View the UI for a pipeline at https://localhost:8443/. The source code of the even-odd user-defined function can be found here . You also can replace the Log Sink with some other sinks like Kafka to forward the data to Kafka topics. The pipeline can be deleted by kubectl delete -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/2-even-odd-pipeline.yaml","title":"Examples"},{"location":"user-guide/user-defined-functions/map/examples/#map-examples","text":"Please read map to get the best out of these examples.","title":"Map Examples"},{"location":"user-guide/user-defined-functions/map/examples/#prerequisites","text":"","title":"Prerequisites"},{"location":"user-guide/user-defined-functions/map/examples/#inter-step-buffer-service-isb-service","text":"","title":"Inter-Step Buffer Service (ISB Service)"},{"location":"user-guide/user-defined-functions/map/examples/#what-is-isb-service","text":"An Inter-Step Buffer Service is described by a Custom Resource , which is used to pass data between vertices of a numaflow pipeline. Please refer to the doc Inter-Step Buffer Service for more information on ISB.","title":"What is ISB Service?"},{"location":"user-guide/user-defined-functions/map/examples/#how-to-install-the-isb-service","text":"kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/stable/examples/0-isbsvc-jetstream.yaml The expected output of the above command is shown below: $ kubectl get isbsvc NAME TYPE PHASE MESSAGE AGE default jetstream Running 3d19h # Wait for pods to be ready $ kubectl get pods NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3 /3 Running 0 19s isbsvc-default-js-1 3 /3 Running 0 19s isbsvc-default-js-2 3 /3 Running 0 19s NOTE The Source used in the examples is an HTTP source producing messages with values 5 and 10 with event time starting from 60000. Please refer to the doc http source on how to use an HTTP source. An example will be as follows, curl -kq -X POST -H \"x-numaflow-event-time: 60000\" -d \"5\" ${ http -source-url } curl -kq -X POST -H \"x-numaflow-event-time: 60000\" -d \"10\" ${ http -source-url }","title":"How to install the ISB Service"},{"location":"user-guide/user-defined-functions/map/examples/#creating-a-simple-map-pipeline","text":"Now we will walk you through creating a map pipeline. In our example, this is called the even-odd pipeline, illustrated by the following diagram: There are five vertices in this example of a map pipeline. An HTTP source vertex which serves an HTTP endpoint to receive numbers as source data, a UDF vertex to tag the ingested numbers with the key even or odd , three Log sinks, one to print the even numbers, one to print the odd numbers, and the other one to print both the even and odd numbers. Run the following command to create the even-odd pipeline. kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/2-even-odd-pipeline.yaml You may opt to view the list of pipelines you've created so far by running kubectl get pipeline . Otherwise, proceed to inspect the status of the pipeline, using kubectl get pods . # Wait for pods to be ready kubectl get pods NAME READY STATUS RESTARTS AGE even-odd-daemon-64d65c945d-vjs9f 1 /1 Running 0 5m3s even-odd-even-or-odd-0-pr4ze 2 /2 Running 0 30s even-odd-even-sink-0-unffo 1 /1 Running 0 22s even-odd-in-0-a7iyd 1 /1 Running 0 5m3s even-odd-number-sink-0-zmg2p 1 /1 Running 0 7s even-odd-odd-sink-0-2736r 1 /1 Running 0 15s isbsvc-default-js-0 3 /3 Running 0 10m isbsvc-default-js-1 3 /3 Running 0 10m isbsvc-default-js-2 3 /3 Running 0 10m Next, port-forward the HTTP endpoint, and make a POST request using curl . Remember to replace xxxxx with the appropriate pod names both here and in the next step. kubectl port-forward even-odd-in-0-xxxx 8444 :8443 # Post data to the HTTP endpoint curl -kq -X POST -d \"101\" https://localhost:8444/vertices/in curl -kq -X POST -d \"102\" https://localhost:8444/vertices/in curl -kq -X POST -d \"103\" https://localhost:8444/vertices/in curl -kq -X POST -d \"104\" https://localhost:8444/vertices/in Now you can watch the log for the even and odd vertices by running the commands below. # Watch the log for the even vertex kubectl logs -f even-odd-even-sink-0-xxxxx 2022 /09/07 22 :29:40 ( even-sink ) 102 2022 /09/07 22 :29:40 ( even-sink ) 104 # Watch the log for the odd vertex kubectl logs -f even-odd-odd-sink-0-xxxxx 2022 /09/07 22 :30:19 ( odd-sink ) 101 2022 /09/07 22 :30:19 ( odd-sink ) 103 View the UI for a pipeline at https://localhost:8443/. The source code of the even-odd user-defined function can be found here . You also can replace the Log Sink with some other sinks like Kafka to forward the data to Kafka topics. The pipeline can be deleted by kubectl delete -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/2-even-odd-pipeline.yaml","title":"Creating a Simple Map Pipeline"},{"location":"user-guide/user-defined-functions/map/map/","text":"Map UDF \u00b6 Map in a Map vertex takes an input and returns 0, 1, or more outputs (also known as flat-map operation). Map is an element wise operator. Build Your Own UDF \u00b6 You can build your own UDF in multiple languages. Check the links below to see the UDF examples for different languages. Python Golang Java After building a docker image for the written UDF, specify the image as below in the vertex spec. spec : vertices : - name : my-vertex udf : container : image : my-python-udf-example:latest Streaming Mode \u00b6 In cases the map function generates more than one output (e.g., flat map), the UDF can be configured to run in a streaming mode instead of batching, which is the default mode. In streaming mode, the messages will be pushed to the downstream vertices once generated instead of in a batch at the end. Note that to maintain data orderliness, we restrict the read batch size to be 1 . spec : vertices : - name : my-vertex limits : # mapstreaming won't work if readBatchSize is != 1 readBatchSize : 1 Check the links below to see the UDF examples in streaming mode for different languages. Python Golang Java Batch Map Mode \u00b6 BatchMap is an interface that allows developers to process multiple data items in a UDF single call, rather than each item in separate calls. The BatchMap interface can be helpful in scenarios where performing operations on a group of data can be more efficient. Important Considerations \u00b6 When using BatchMap, there are a few important considerations to keep in mind: Ensure that the BatchResponses object is tagged with the correct request ID. Each Datum has a unique ID tag, which will be used by Numaflow to ensure correctness. Ensure that the length of the BatchResponses list is equal to the number of requests received. This means that for every input data item, there should be a corresponding response in the BatchResponses list. The total batch size can be up to readBatchSize long. Check the links below to see the UDF examples in batch mode for different languages. Python Golang Java Rust Available Environment Variables \u00b6 Some environment variables are available in the user-defined function container, they might be useful in your own UDF implementation. NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex. Configuration \u00b6 Configuration data can be provided to the UDF container at runtime multiple ways. environment variables args command volumes init containers","title":"Overview"},{"location":"user-guide/user-defined-functions/map/map/#map-udf","text":"Map in a Map vertex takes an input and returns 0, 1, or more outputs (also known as flat-map operation). Map is an element wise operator.","title":"Map UDF"},{"location":"user-guide/user-defined-functions/map/map/#build-your-own-udf","text":"You can build your own UDF in multiple languages. Check the links below to see the UDF examples for different languages. Python Golang Java After building a docker image for the written UDF, specify the image as below in the vertex spec. spec : vertices : - name : my-vertex udf : container : image : my-python-udf-example:latest","title":"Build Your Own UDF"},{"location":"user-guide/user-defined-functions/map/map/#streaming-mode","text":"In cases the map function generates more than one output (e.g., flat map), the UDF can be configured to run in a streaming mode instead of batching, which is the default mode. In streaming mode, the messages will be pushed to the downstream vertices once generated instead of in a batch at the end. Note that to maintain data orderliness, we restrict the read batch size to be 1 . spec : vertices : - name : my-vertex limits : # mapstreaming won't work if readBatchSize is != 1 readBatchSize : 1 Check the links below to see the UDF examples in streaming mode for different languages. Python Golang Java","title":"Streaming Mode"},{"location":"user-guide/user-defined-functions/map/map/#batch-map-mode","text":"BatchMap is an interface that allows developers to process multiple data items in a UDF single call, rather than each item in separate calls. The BatchMap interface can be helpful in scenarios where performing operations on a group of data can be more efficient.","title":"Batch Map Mode"},{"location":"user-guide/user-defined-functions/map/map/#important-considerations","text":"When using BatchMap, there are a few important considerations to keep in mind: Ensure that the BatchResponses object is tagged with the correct request ID. Each Datum has a unique ID tag, which will be used by Numaflow to ensure correctness. Ensure that the length of the BatchResponses list is equal to the number of requests received. This means that for every input data item, there should be a corresponding response in the BatchResponses list. The total batch size can be up to readBatchSize long. Check the links below to see the UDF examples in batch mode for different languages. Python Golang Java Rust","title":"Important Considerations"},{"location":"user-guide/user-defined-functions/map/map/#available-environment-variables","text":"Some environment variables are available in the user-defined function container, they might be useful in your own UDF implementation. NUMAFLOW_NAMESPACE - Namespace. NUMAFLOW_POD - Pod name. NUMAFLOW_REPLICA - Replica index. NUMAFLOW_PIPELINE_NAME - Name of the pipeline. NUMAFLOW_VERTEX_NAME - Name of the vertex.","title":"Available Environment Variables"},{"location":"user-guide/user-defined-functions/map/map/#configuration","text":"Configuration data can be provided to the UDF container at runtime multiple ways. environment variables args command volumes init containers","title":"Configuration"},{"location":"user-guide/user-defined-functions/map/builtin-functions/","text":"Built-in Functions [Deprecated] \u00b6 \u26a0\ufe0f Note: Builtins for UDF are deprecated and will be removed with 1.6 release. Numaflow provides some built-in functions that can be used directly. Cat A cat builtin UDF does nothing but return the same messages it receives, it is very useful for debugging and testing. spec : vertices : - name : cat-vertex udf : builtin : name : cat Filter A filter built-in UDF does filter the message based on expression. payload keyword represents message object. see documentation for expression here spec : vertices : - name : filter-vertex udf : builtin : name : filter kwargs : expression : int(object(payload).id) > 100","title":"Overview"},{"location":"user-guide/user-defined-functions/map/builtin-functions/#built-in-functions-deprecated","text":"\u26a0\ufe0f Note: Builtins for UDF are deprecated and will be removed with 1.6 release. Numaflow provides some built-in functions that can be used directly. Cat A cat builtin UDF does nothing but return the same messages it receives, it is very useful for debugging and testing. spec : vertices : - name : cat-vertex udf : builtin : name : cat Filter A filter built-in UDF does filter the message based on expression. payload keyword represents message object. see documentation for expression here spec : vertices : - name : filter-vertex udf : builtin : name : filter kwargs : expression : int(object(payload).id) > 100","title":"Built-in Functions [Deprecated]"},{"location":"user-guide/user-defined-functions/map/builtin-functions/cat/","text":"Cat [Deprecated] \u00b6 \u26a0\ufe0f Note: Builtins for UDF are deprecated. An example in Go for cat can be found here. A cat builtin function does nothing but return the same messages it receives, it is very useful for debugging and testing. spec : vertices : - name : cat-vertex udf : builtin : name : cat","title":"Cat"},{"location":"user-guide/user-defined-functions/map/builtin-functions/cat/#cat-deprecated","text":"\u26a0\ufe0f Note: Builtins for UDF are deprecated. An example in Go for cat can be found here. A cat builtin function does nothing but return the same messages it receives, it is very useful for debugging and testing. spec : vertices : - name : cat-vertex udf : builtin : name : cat","title":"Cat [Deprecated]"},{"location":"user-guide/user-defined-functions/map/builtin-functions/filter/","text":"Filter [Deprecated] \u00b6 \u26a0\ufe0f Note: Builtins for UDF are deprecated. An example in Go for filter can be found here. A filter is a special-purpose built-in function. It is used to evaluate on each message in a pipeline and is often used to filter the number of messages that are passed to next vertices. Filter function supports comprehensive expression language which extend flexibility write complex expressions. payload will be root element to represent the message object in expression. Expression \u00b6 Filter expression implemented with expr and sprig libraries. Data conversion functions \u00b6 These function can be accessed directly in expression. json - Convert payload in JSON object. e.g: json(payload) int - Convert element/payload into int value. e.g: int(json(payload).id) string - Convert element/payload into string value. e.g: string(json(payload).amount) Sprig functions \u00b6 Sprig library has 70+ functions. sprig prefix need to be added to access the sprig functions. sprig functions E.g: sprig.contains('James', json(payload).name) # James is contained in the value of name . int(json(sprig.b64dec(payload)).id) < 100 Filter Spec \u00b6 - name : filter-vertex udf : builtin : name : filter kwargs : expression : int(json(payload).id) < 100","title":"Filter"},{"location":"user-guide/user-defined-functions/map/builtin-functions/filter/#filter-deprecated","text":"\u26a0\ufe0f Note: Builtins for UDF are deprecated. An example in Go for filter can be found here. A filter is a special-purpose built-in function. It is used to evaluate on each message in a pipeline and is often used to filter the number of messages that are passed to next vertices. Filter function supports comprehensive expression language which extend flexibility write complex expressions. payload will be root element to represent the message object in expression.","title":"Filter [Deprecated]"},{"location":"user-guide/user-defined-functions/map/builtin-functions/filter/#expression","text":"Filter expression implemented with expr and sprig libraries.","title":"Expression"},{"location":"user-guide/user-defined-functions/map/builtin-functions/filter/#data-conversion-functions","text":"These function can be accessed directly in expression. json - Convert payload in JSON object. e.g: json(payload) int - Convert element/payload into int value. e.g: int(json(payload).id) string - Convert element/payload into string value. e.g: string(json(payload).amount)","title":"Data conversion functions"},{"location":"user-guide/user-defined-functions/map/builtin-functions/filter/#sprig-functions","text":"Sprig library has 70+ functions. sprig prefix need to be added to access the sprig functions. sprig functions E.g: sprig.contains('James', json(payload).name) # James is contained in the value of name . int(json(sprig.b64dec(payload)).id) < 100","title":"Sprig functions"},{"location":"user-guide/user-defined-functions/map/builtin-functions/filter/#filter-spec","text":"- name : filter-vertex udf : builtin : name : filter kwargs : expression : int(json(payload).id) < 100","title":"Filter Spec"},{"location":"user-guide/user-defined-functions/reduce/examples/","text":"Reduce Examples \u00b6 Please read reduce to get the best out of these examples. Prerequisites \u00b6 Inter-Step Buffer Service (ISB Service) \u00b6 What is ISB Service? \u00b6 An Inter-Step Buffer Service is described by a Custom Resource , which is used to pass data between vertices of a numaflow pipeline. Please refer to the doc Inter-Step Buffer Service for more information on ISB. How to install the ISB Service \u00b6 kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/stable/examples/0-isbsvc-jetstream.yaml The expected output of the above command is shown below: $ kubectl get isbsvc NAME TYPE PHASE MESSAGE AGE default jetstream Running 3d19h # Wait for pods to be ready $ kubectl get pods NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3 /3 Running 0 19s isbsvc-default-js-1 3 /3 Running 0 19s isbsvc-default-js-2 3 /3 Running 0 19s NOTE The Source used in the examples is an HTTP source producing messages with values 5 and 10 with event time starting from 60000. Please refer to the doc http source on how to use an HTTP source. An example will be as follows, curl -kq -X POST -H \"x-numaflow-event-time: 60000\" -d \"5\" ${ http -source-url } curl -kq -X POST -H \"x-numaflow-event-time: 60000\" -d \"10\" ${ http -source-url } Sum Pipeline Using Fixed Window \u00b6 This is a simple reduce pipeline that just does summation (sum of numbers) but uses fixed window. The snippet for the reduce vertex is as follows. - name : compute-sum udf : container : # compute the sum image : quay.io/numaio/numaflow-go/reduce-sum:v0.5.0 groupBy : window : fixed : length : 60s keyed : true 6-reduce-fixed-window.yaml has the complete pipeline definition. In this example we use a partitions of 2 . We are setting a partitions > 1 because it is a keyed window. - name : compute-sum partitions : 2 kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/6-reduce-fixed-window.yaml Output : 2023/01/05 11:54:41 (sink) Payload - 300 Key - odd Start - 60000 End - 120000 2023/01/05 11:54:41 (sink) Payload - 600 Key - even Start - 60000 End - 120000 2023/01/05 11:54:41 (sink) Payload - 300 Key - odd Start - 120000 End - 180000 2023/01/05 11:54:41 (sink) Payload - 600 Key - even Start - 120000 End - 180000 2023/01/05 11:54:42 (sink) Payload - 600 Key - even Start - 180000 End - 240000 2023/01/05 11:54:42 (sink) Payload - 300 Key - odd Start - 180000 End - 240000 In our example, input is an HTTP source producing 2 messages each second with values 5 and 10, and the event time starts from 60000. Since we have considered a fixed window of length 60s, and also we are producing two messages with different keys \"even\" and \"odd\", Numaflow will create two different windows with a start time of 60000 and an end time of 120000. So the output will be 300(5 * 60) and 600(10 * 60). If we had used a non keyed window ( keyed: false ), we would have seen one single output with value of 900(300 of odd + 600 of even) for each window. Sum Pipeline Using Sliding Window \u00b6 This is a simple reduce pipeline that just does summation (sum of numbers) but uses sliding window. The snippet for the reduce vertex is as follows. - name : reduce-sliding udf : container : # compute the sum image : quay.io/numaio/numaflow-go/reduce-sum:v0.5.0 groupBy : window : sliding : length : 60s slide : 10s keyed : true 7-reduce-sliding-window.yaml has the complete pipeline definition kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/7-reduce-sliding-window.yaml Output: 2023/01/05 15:13:16 (sink) Payload - 300 Key - odd Start - 60000 End - 120000 2023/01/05 15:13:16 (sink) Payload - 600 Key - even Start - 60000 End - 120000 2023/01/05 15:13:16 (sink) Payload - 300 Key - odd Start - 70000 End - 130000 2023/01/05 15:13:16 (sink) Payload - 600 Key - even Start - 700000 End - 1300000 2023/01/05 15:13:16 (sink) Payload - 300 Key - odd Start - 80000 End - 140000 2023/01/05 15:13:16 (sink) Payload - 600 Key - even Start - 80000 End - 140000 In our example, input is an HTTP source producing 2 messages each second with values 5 and 10, and the event time starts from 60000. Since we have considered a sliding window of length 60s and slide 10s, and also we are producing two messages with different keys \"even\" and \"odd\". Numaflow will create two different windows with a start time of 60000 and an end time of 120000, and because the slide duration is 10s, a next set of windows will be created with start time of 70000 and an end time of 130000. Since it's a sum operation the output will be 300(5 * 60) and 600(10 * 60). Payload - 50 Key - odd Start - 10000 End - 70000 , we see 50 here for odd because the first window has only 10 elements Complex Reduce Pipeline \u00b6 In the complex reduce example, we will chain of reduce functions use both fixed and sliding windows use keyed and non-keyed windowing 8-reduce-complex-pipeline.yaml has the complete pipeline definition kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/8-reduce-complex-pipeline.yaml Output: 2023/01/05 15:33:55 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 80000 End - 140000 2023/01/05 15:33:55 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 90000 End - 150000 2023/01/05 15:33:55 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 100000 End - 160000 2023/01/05 15:33:56 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 110000 End - 170000 2023/01/05 15:33:56 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 120000 End - 180000 2023/01/05 15:33:56 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 130000 End - 190000 In our example, first we have the reduce vertex with a fixed window of duration 5s. Since the input is 5 and 10, the output from the first reduce vertex will be 25 (5 * 5) and 50 (5 * 10). This will be passed to the next non-keyed reduce vertex with the fixed window duration of 10s. This being a non-keyed, it will combine the inputs and produce the output of 150(25 * 2 + 50 * 2), which will be passed to the reduce vertex with a sliding window of duration 60s and with the slide duration of 10s. Hence the final output will be 900(150 * 6).","title":"Examples"},{"location":"user-guide/user-defined-functions/reduce/examples/#reduce-examples","text":"Please read reduce to get the best out of these examples.","title":"Reduce Examples"},{"location":"user-guide/user-defined-functions/reduce/examples/#prerequisites","text":"","title":"Prerequisites"},{"location":"user-guide/user-defined-functions/reduce/examples/#inter-step-buffer-service-isb-service","text":"","title":"Inter-Step Buffer Service (ISB Service)"},{"location":"user-guide/user-defined-functions/reduce/examples/#what-is-isb-service","text":"An Inter-Step Buffer Service is described by a Custom Resource , which is used to pass data between vertices of a numaflow pipeline. Please refer to the doc Inter-Step Buffer Service for more information on ISB.","title":"What is ISB Service?"},{"location":"user-guide/user-defined-functions/reduce/examples/#how-to-install-the-isb-service","text":"kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/stable/examples/0-isbsvc-jetstream.yaml The expected output of the above command is shown below: $ kubectl get isbsvc NAME TYPE PHASE MESSAGE AGE default jetstream Running 3d19h # Wait for pods to be ready $ kubectl get pods NAME READY STATUS RESTARTS AGE isbsvc-default-js-0 3 /3 Running 0 19s isbsvc-default-js-1 3 /3 Running 0 19s isbsvc-default-js-2 3 /3 Running 0 19s NOTE The Source used in the examples is an HTTP source producing messages with values 5 and 10 with event time starting from 60000. Please refer to the doc http source on how to use an HTTP source. An example will be as follows, curl -kq -X POST -H \"x-numaflow-event-time: 60000\" -d \"5\" ${ http -source-url } curl -kq -X POST -H \"x-numaflow-event-time: 60000\" -d \"10\" ${ http -source-url }","title":"How to install the ISB Service"},{"location":"user-guide/user-defined-functions/reduce/examples/#sum-pipeline-using-fixed-window","text":"This is a simple reduce pipeline that just does summation (sum of numbers) but uses fixed window. The snippet for the reduce vertex is as follows. - name : compute-sum udf : container : # compute the sum image : quay.io/numaio/numaflow-go/reduce-sum:v0.5.0 groupBy : window : fixed : length : 60s keyed : true 6-reduce-fixed-window.yaml has the complete pipeline definition. In this example we use a partitions of 2 . We are setting a partitions > 1 because it is a keyed window. - name : compute-sum partitions : 2 kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/6-reduce-fixed-window.yaml Output : 2023/01/05 11:54:41 (sink) Payload - 300 Key - odd Start - 60000 End - 120000 2023/01/05 11:54:41 (sink) Payload - 600 Key - even Start - 60000 End - 120000 2023/01/05 11:54:41 (sink) Payload - 300 Key - odd Start - 120000 End - 180000 2023/01/05 11:54:41 (sink) Payload - 600 Key - even Start - 120000 End - 180000 2023/01/05 11:54:42 (sink) Payload - 600 Key - even Start - 180000 End - 240000 2023/01/05 11:54:42 (sink) Payload - 300 Key - odd Start - 180000 End - 240000 In our example, input is an HTTP source producing 2 messages each second with values 5 and 10, and the event time starts from 60000. Since we have considered a fixed window of length 60s, and also we are producing two messages with different keys \"even\" and \"odd\", Numaflow will create two different windows with a start time of 60000 and an end time of 120000. So the output will be 300(5 * 60) and 600(10 * 60). If we had used a non keyed window ( keyed: false ), we would have seen one single output with value of 900(300 of odd + 600 of even) for each window.","title":"Sum Pipeline Using Fixed Window"},{"location":"user-guide/user-defined-functions/reduce/examples/#sum-pipeline-using-sliding-window","text":"This is a simple reduce pipeline that just does summation (sum of numbers) but uses sliding window. The snippet for the reduce vertex is as follows. - name : reduce-sliding udf : container : # compute the sum image : quay.io/numaio/numaflow-go/reduce-sum:v0.5.0 groupBy : window : sliding : length : 60s slide : 10s keyed : true 7-reduce-sliding-window.yaml has the complete pipeline definition kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/7-reduce-sliding-window.yaml Output: 2023/01/05 15:13:16 (sink) Payload - 300 Key - odd Start - 60000 End - 120000 2023/01/05 15:13:16 (sink) Payload - 600 Key - even Start - 60000 End - 120000 2023/01/05 15:13:16 (sink) Payload - 300 Key - odd Start - 70000 End - 130000 2023/01/05 15:13:16 (sink) Payload - 600 Key - even Start - 700000 End - 1300000 2023/01/05 15:13:16 (sink) Payload - 300 Key - odd Start - 80000 End - 140000 2023/01/05 15:13:16 (sink) Payload - 600 Key - even Start - 80000 End - 140000 In our example, input is an HTTP source producing 2 messages each second with values 5 and 10, and the event time starts from 60000. Since we have considered a sliding window of length 60s and slide 10s, and also we are producing two messages with different keys \"even\" and \"odd\". Numaflow will create two different windows with a start time of 60000 and an end time of 120000, and because the slide duration is 10s, a next set of windows will be created with start time of 70000 and an end time of 130000. Since it's a sum operation the output will be 300(5 * 60) and 600(10 * 60). Payload - 50 Key - odd Start - 10000 End - 70000 , we see 50 here for odd because the first window has only 10 elements","title":"Sum Pipeline Using Sliding Window"},{"location":"user-guide/user-defined-functions/reduce/examples/#complex-reduce-pipeline","text":"In the complex reduce example, we will chain of reduce functions use both fixed and sliding windows use keyed and non-keyed windowing 8-reduce-complex-pipeline.yaml has the complete pipeline definition kubectl apply -f https://raw.githubusercontent.com/numaproj/numaflow/main/examples/8-reduce-complex-pipeline.yaml Output: 2023/01/05 15:33:55 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 80000 End - 140000 2023/01/05 15:33:55 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 90000 End - 150000 2023/01/05 15:33:55 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 100000 End - 160000 2023/01/05 15:33:56 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 110000 End - 170000 2023/01/05 15:33:56 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 120000 End - 180000 2023/01/05 15:33:56 (sink) Payload - 900 Key - NON_KEYED_STREAM Start - 130000 End - 190000 In our example, first we have the reduce vertex with a fixed window of duration 5s. Since the input is 5 and 10, the output from the first reduce vertex will be 25 (5 * 5) and 50 (5 * 10). This will be passed to the next non-keyed reduce vertex with the fixed window duration of 10s. This being a non-keyed, it will combine the inputs and produce the output of 150(25 * 2 + 50 * 2), which will be passed to the reduce vertex with a sliding window of duration 60s and with the slide duration of 10s. Hence the final output will be 900(150 * 6).","title":"Complex Reduce Pipeline"},{"location":"user-guide/user-defined-functions/reduce/reduce/","text":"Reduce UDF \u00b6 Overview \u00b6 Reduce is one of the most commonly used abstractions in a stream processing pipeline to define aggregation functions on a stream of data. It is the reduce feature that helps us solve problems like \"performs a summary operation(such as counting the number of occurrences of a key, yielding user login frequencies), etc. \"Since the input is an unbounded stream (with infinite entries), we need an additional parameter to convert the unbounded problem to a bounded problem and provide results on that. That bounding condition is \"time\", eg, \"number of users logged in per minute\". So while processing an unbounded stream of data, we need a way to group elements into finite chunks using time. To build these chunks, the reduce function is applied to the set of records produced using the concept of windowing . Reduce Pseudo code \u00b6 Unlike in map vertex where only an element is given to user-defined function, in reduce since there is a group of elements, an iterator is passed to the reduce function. The following is a generic outlook of a reduce function. I have written the pseudo-code using the accumulator to show that very powerful functions can be applied using this reduce semantics. # reduceFn func is a generic reduce function that processes a set of elements def reduceFn ( keys : List [ str ], datums : Iterator [ Datum ], md : Metadata ) -> Messages : # initialize_accumalor could be any function that starts of with an empty # state. eg, accumulator = 0 accumulator = initialize_accumalor () # we are iterating on the input set of elements for d in datums : # accumulator.add_input() can be any function. # e.g., it could be as simple as accumulator += 1 accumulator . add_input ( d ) # once we are done with iterating on the elements, we return the result # acumulator.result() can be str.encode(accumulator) return Messages ( Message ( acumulator . result (), keys )) Specification \u00b6 The structure for defining a reduce vertex is as follows. - name : my-reduce-udf udf : container : image : my-reduce-udf:latest groupBy : window : ... keyed : ... storage : ... The reduce spec adds a new section called groupBy and this how we differentiate a map vertex from reduce vertex. There are two important fields, the window and keyed . These two fields play an important role in grouping the data together and pass it to the user-defined reduce code. The reduce supports parallelism processing by defining a partitions in the vertex. This is because auto-scaling is not supported in reduce vertex. If partitions is not defined default of one will be used. - name : my-reduce-udf partitions : integer It is wrong to give a partitions > 1 if it is a non-keyed vertex ( keyed: false ). There are a couple of examples that demonstrate Fixed windows, Sliding windows, chaining of windows, keyed streams, etc. Time Characteristics \u00b6 All windowing operations generate new records as an output of reduce operations. Event-time and Watermark are two main primitives that determine how the time propagates in a streaming application. so for all new records generated in a reduce operation, event time is set to the end time of the window. For example, for a reduce operation over a keyed/non-keyed window with a start and end defined by [2031-09-29T18:47:00Z, 2031-09-29T18:48:00Z) , event time for all the records generated will be set to 2031-09-29T18:47:59.999Z since millisecond is the smallest granularity (as of now) event time is set to the last timestamp that belongs to a window. Watermark is treated similarly, the watermark is set to the last timestamp for a given window. So for the example above, the value of the watermark will be set to the last timestamp, i.e., 2031-09-29T18:47:59.999Z . This applies to all the window types regardless of whether they are keyed or non-keyed windows. Allowed Lateness \u00b6 allowedLateness flag on the Reduce vertex will allow late data to be processed by slowing the down the close-of-book operation of the Reduce vertex. Late data will be included for the Reduce operation as long as the late data is not later than (CurrentWatermark - AllowedLateness) . Without allowedLateness , late data will be rejected and dropped. The key reason for introducing allowedLateness is to support further out-of-order data beyond the watermark withholding, esp. when there are multiple reducers in the pipeline. Each Reduce vertex can have its own allowedLateness , but the watermark delay is additive. vertices : - name : my-udf udf : groupBy : allowedLateness : 5s # Optional, allowedLateness is disabled by default Storage \u00b6 Reduce unlike map requires persistence. To support persistence user has to define the storage configuration. We replay the data stored in this storage on pod startup if there has been a restart of the reduce pod caused due to pod migrations, etc. vertices : - name : my-udf udf : groupBy : storage : .... Persistent Volume Claim (PVC) \u00b6 persistentVolumeClaim supports the following fields, volumeSize , storageClassName , and accessMode . As name suggests, volumeSize specifies the size of the volume. accessMode can be of many types, but for reduce use case we need only ReadWriteOncePod . storageClassName can also be provided, more info on storage class can be found here . The default value of storageClassName is default which is default StorageClass may be deployed to a Kubernetes cluster by addon manager during installation. Example \u00b6 vertices : - name : my-udf udf : groupBy : storage : persistentVolumeClaim : volumeSize : 10Gi accessMode : ReadWriteOncePod EmptyDir \u00b6 We also support emptyDir for quick experimentation. We do not recommend this in production setup. If we use emptyDir , we will end up in data loss if there are pod migrations. emptyDir also takes an optional sizeLimit . medium field controls where emptyDir volumes are stored. By default emptyDir volumes are stored on whatever medium that backs the node such as disk, SSD, or network storage, depending on your environment. If you set the medium field to \"Memory\" , Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead. Example \u00b6 vertices : - name : my-udf udf : groupBy : storage : emptyDir : {}","title":"Overview"},{"location":"user-guide/user-defined-functions/reduce/reduce/#reduce-udf","text":"","title":"Reduce UDF"},{"location":"user-guide/user-defined-functions/reduce/reduce/#overview","text":"Reduce is one of the most commonly used abstractions in a stream processing pipeline to define aggregation functions on a stream of data. It is the reduce feature that helps us solve problems like \"performs a summary operation(such as counting the number of occurrences of a key, yielding user login frequencies), etc. \"Since the input is an unbounded stream (with infinite entries), we need an additional parameter to convert the unbounded problem to a bounded problem and provide results on that. That bounding condition is \"time\", eg, \"number of users logged in per minute\". So while processing an unbounded stream of data, we need a way to group elements into finite chunks using time. To build these chunks, the reduce function is applied to the set of records produced using the concept of windowing .","title":"Overview"},{"location":"user-guide/user-defined-functions/reduce/reduce/#reduce-pseudo-code","text":"Unlike in map vertex where only an element is given to user-defined function, in reduce since there is a group of elements, an iterator is passed to the reduce function. The following is a generic outlook of a reduce function. I have written the pseudo-code using the accumulator to show that very powerful functions can be applied using this reduce semantics. # reduceFn func is a generic reduce function that processes a set of elements def reduceFn ( keys : List [ str ], datums : Iterator [ Datum ], md : Metadata ) -> Messages : # initialize_accumalor could be any function that starts of with an empty # state. eg, accumulator = 0 accumulator = initialize_accumalor () # we are iterating on the input set of elements for d in datums : # accumulator.add_input() can be any function. # e.g., it could be as simple as accumulator += 1 accumulator . add_input ( d ) # once we are done with iterating on the elements, we return the result # acumulator.result() can be str.encode(accumulator) return Messages ( Message ( acumulator . result (), keys ))","title":"Reduce Pseudo code"},{"location":"user-guide/user-defined-functions/reduce/reduce/#specification","text":"The structure for defining a reduce vertex is as follows. - name : my-reduce-udf udf : container : image : my-reduce-udf:latest groupBy : window : ... keyed : ... storage : ... The reduce spec adds a new section called groupBy and this how we differentiate a map vertex from reduce vertex. There are two important fields, the window and keyed . These two fields play an important role in grouping the data together and pass it to the user-defined reduce code. The reduce supports parallelism processing by defining a partitions in the vertex. This is because auto-scaling is not supported in reduce vertex. If partitions is not defined default of one will be used. - name : my-reduce-udf partitions : integer It is wrong to give a partitions > 1 if it is a non-keyed vertex ( keyed: false ). There are a couple of examples that demonstrate Fixed windows, Sliding windows, chaining of windows, keyed streams, etc.","title":"Specification"},{"location":"user-guide/user-defined-functions/reduce/reduce/#time-characteristics","text":"All windowing operations generate new records as an output of reduce operations. Event-time and Watermark are two main primitives that determine how the time propagates in a streaming application. so for all new records generated in a reduce operation, event time is set to the end time of the window. For example, for a reduce operation over a keyed/non-keyed window with a start and end defined by [2031-09-29T18:47:00Z, 2031-09-29T18:48:00Z) , event time for all the records generated will be set to 2031-09-29T18:47:59.999Z since millisecond is the smallest granularity (as of now) event time is set to the last timestamp that belongs to a window. Watermark is treated similarly, the watermark is set to the last timestamp for a given window. So for the example above, the value of the watermark will be set to the last timestamp, i.e., 2031-09-29T18:47:59.999Z . This applies to all the window types regardless of whether they are keyed or non-keyed windows.","title":"Time Characteristics"},{"location":"user-guide/user-defined-functions/reduce/reduce/#allowed-lateness","text":"allowedLateness flag on the Reduce vertex will allow late data to be processed by slowing the down the close-of-book operation of the Reduce vertex. Late data will be included for the Reduce operation as long as the late data is not later than (CurrentWatermark - AllowedLateness) . Without allowedLateness , late data will be rejected and dropped. The key reason for introducing allowedLateness is to support further out-of-order data beyond the watermark withholding, esp. when there are multiple reducers in the pipeline. Each Reduce vertex can have its own allowedLateness , but the watermark delay is additive. vertices : - name : my-udf udf : groupBy : allowedLateness : 5s # Optional, allowedLateness is disabled by default","title":"Allowed Lateness"},{"location":"user-guide/user-defined-functions/reduce/reduce/#storage","text":"Reduce unlike map requires persistence. To support persistence user has to define the storage configuration. We replay the data stored in this storage on pod startup if there has been a restart of the reduce pod caused due to pod migrations, etc. vertices : - name : my-udf udf : groupBy : storage : ....","title":"Storage"},{"location":"user-guide/user-defined-functions/reduce/reduce/#persistent-volume-claim-pvc","text":"persistentVolumeClaim supports the following fields, volumeSize , storageClassName , and accessMode . As name suggests, volumeSize specifies the size of the volume. accessMode can be of many types, but for reduce use case we need only ReadWriteOncePod . storageClassName can also be provided, more info on storage class can be found here . The default value of storageClassName is default which is default StorageClass may be deployed to a Kubernetes cluster by addon manager during installation.","title":"Persistent Volume Claim (PVC)"},{"location":"user-guide/user-defined-functions/reduce/reduce/#example","text":"vertices : - name : my-udf udf : groupBy : storage : persistentVolumeClaim : volumeSize : 10Gi accessMode : ReadWriteOncePod","title":"Example"},{"location":"user-guide/user-defined-functions/reduce/reduce/#emptydir","text":"We also support emptyDir for quick experimentation. We do not recommend this in production setup. If we use emptyDir , we will end up in data loss if there are pod migrations. emptyDir also takes an optional sizeLimit . medium field controls where emptyDir volumes are stored. By default emptyDir volumes are stored on whatever medium that backs the node such as disk, SSD, or network storage, depending on your environment. If you set the medium field to \"Memory\" , Kubernetes mounts a tmpfs (RAM-backed filesystem) for you instead.","title":"EmptyDir"},{"location":"user-guide/user-defined-functions/reduce/reduce/#example_1","text":"vertices : - name : my-udf udf : groupBy : storage : emptyDir : {}","title":"Example"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/","text":"Accumulator \u00b6 Accumulator is a special kind of window similar to a Session Window designed for complex operations like reordering, custom triggering, and joining multiple ordered streams. Like other windowing strategies (fixed, sliding, or session windows), the Accumulator window maintains state for each key, but unlike others, it allows for manipulation of the Datum and emitting them based on custom rules (e.g., sorting) . Accumulator solves is a different type of problem outside both map / flatmap (one to ~one) and reduce (many to ~one) and instead of Message , we have to emit back the \"manipulated\" Datum . Another difference between the Accumulator and the Session windows is that in Accumulator, there is no concept of window merge . Why Accumulator? \u00b6 Accumulator is a powerful concept that lets you tap into the raw Datum stream and manipulate not just the order but the Datum stream itself. It has a powerful semantics where the input and output is a stream of Datum creating a Global Window. It opens up the possibility of very advanced use cases like custom triggers (e.g., count based triggers combined with windowing strategies). def Accumulator ( <- stream in [ Datum ]) -> stream out [ Datum ] { let state = OrderedList () for i = range in { # The condition will return true if Watermark progresses if WatermarkProgressed ( i ) == true { # pop all sorted elements and Write to output stream Write ( out , state . popN ()) } state . insert ( i ) } } Considerations \u00b6 The Accumulator window is powerful but should be used carefully as it can cause pipeline stalling if not configured properly. Factors to consider \u00b6 Please consider the following factors when using the Accumulator window (not comprehensive): For high-throughput scenarios, ensure adequate storage is provisioned The timeout should be set based on the expected data arrival patterns and latency requirements Consider the trade-off between data completeness (longer timeout) and processing latency (shorter timeout) Please make sure Watermark is honored when publishing the data, else completeness and correctness is not guaranteed Data Retention \u00b6 To ensure there is no data loss during pod restarts, the Accumulator window replays data from persistent storage. The system stores data until Outbound(Watermark) - 1 , which means it keeps the minimum necessary data to ensure correctness while managing resource usage. Constraints \u00b6 For data older than Outbound(Watermark) - 1 , users need to bring in an external store and implement replay on restart Data deletion is based on the Outbound(Watermark) Few general use cases \u00b6 Stream Joining : Combining multiple ordered streams into a single ordered output Event Reordering : Handling out-of-order events and ensuring they're processed in the correct sequence Time-based Correlation : Correlating events from different sources based on their timestamps Custom Sorting : Implementing user-defined sorting logic for event streams Custom Triggering : Triggering actions based on specific conditions or events within the stream Configuration \u00b6 vertices : - name : my-udf udf : groupBy : window : accumulator : timeout : duration NOTE: A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". timeout \u00b6 The timeout is the duration of inactivity (no data flowing in for a particular key) after which the accumulator state is removed. This helps prevent memory leaks by cleaning up state for keys that are no longer active. How It Works \u00b6 The Accumulator window works by: Maintaining an ordered list of elements for each key When the watermark progresses, it pops all sorted elements and writes them to the output stream New elements are inserted into the ordered list based on their event time If no new data arrives for a key within the specified timeout period, the window is closed Unlike both map or reduce operations, where Datum is consumed and Message is returned, for reordering with the Accumulator, the Datum is kept intact. Example \u00b6 Here's an example of using an Accumulator window to join and sort two HTTP sources: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-accumulator spec : vertices : - name : http-one scale : min : 1 max : 1 source : http : {} - name : http-two scale : min : 1 max : 1 source : http : {} - name : accum udf : container : # stream sorter example image : quay.io/numaio/numaflow-go/stream-sorter:stable groupBy : window : accumulator : timeout : 10s keyed : true storage : persistentVolumeClaim : volumeSize : 1Gi - name : sink scale : min : 1 max : 1 sink : log : {} edges : - from : http-one to : accum - from : http-two to : accum - from : accum to : sink In this example: We have two HTTP sources ( http-one and http-two ) that produce ordered streams The accum vertex uses an Accumulator window with a timeout of 10 seconds The accumulator joins and sorts the events from both sources based on their event time The sorted output is sent to a log sink Note: Setting readBatchSize: 1 helps maintain the ordering of events in the input streams. Check the links below to see the UDF examples for different languages: Golang Python","title":"Accumulator"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#accumulator","text":"Accumulator is a special kind of window similar to a Session Window designed for complex operations like reordering, custom triggering, and joining multiple ordered streams. Like other windowing strategies (fixed, sliding, or session windows), the Accumulator window maintains state for each key, but unlike others, it allows for manipulation of the Datum and emitting them based on custom rules (e.g., sorting) . Accumulator solves is a different type of problem outside both map / flatmap (one to ~one) and reduce (many to ~one) and instead of Message , we have to emit back the \"manipulated\" Datum . Another difference between the Accumulator and the Session windows is that in Accumulator, there is no concept of window merge .","title":"Accumulator"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#why-accumulator","text":"Accumulator is a powerful concept that lets you tap into the raw Datum stream and manipulate not just the order but the Datum stream itself. It has a powerful semantics where the input and output is a stream of Datum creating a Global Window. It opens up the possibility of very advanced use cases like custom triggers (e.g., count based triggers combined with windowing strategies). def Accumulator ( <- stream in [ Datum ]) -> stream out [ Datum ] { let state = OrderedList () for i = range in { # The condition will return true if Watermark progresses if WatermarkProgressed ( i ) == true { # pop all sorted elements and Write to output stream Write ( out , state . popN ()) } state . insert ( i ) } }","title":"Why Accumulator?"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#considerations","text":"The Accumulator window is powerful but should be used carefully as it can cause pipeline stalling if not configured properly.","title":"Considerations"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#factors-to-consider","text":"Please consider the following factors when using the Accumulator window (not comprehensive): For high-throughput scenarios, ensure adequate storage is provisioned The timeout should be set based on the expected data arrival patterns and latency requirements Consider the trade-off between data completeness (longer timeout) and processing latency (shorter timeout) Please make sure Watermark is honored when publishing the data, else completeness and correctness is not guaranteed","title":"Factors to consider"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#data-retention","text":"To ensure there is no data loss during pod restarts, the Accumulator window replays data from persistent storage. The system stores data until Outbound(Watermark) - 1 , which means it keeps the minimum necessary data to ensure correctness while managing resource usage.","title":"Data Retention"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#constraints","text":"For data older than Outbound(Watermark) - 1 , users need to bring in an external store and implement replay on restart Data deletion is based on the Outbound(Watermark)","title":"Constraints"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#few-general-use-cases","text":"Stream Joining : Combining multiple ordered streams into a single ordered output Event Reordering : Handling out-of-order events and ensuring they're processed in the correct sequence Time-based Correlation : Correlating events from different sources based on their timestamps Custom Sorting : Implementing user-defined sorting logic for event streams Custom Triggering : Triggering actions based on specific conditions or events within the stream","title":"Few general use cases"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#configuration","text":"vertices : - name : my-udf udf : groupBy : window : accumulator : timeout : duration NOTE: A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".","title":"Configuration"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#timeout","text":"The timeout is the duration of inactivity (no data flowing in for a particular key) after which the accumulator state is removed. This helps prevent memory leaks by cleaning up state for keys that are no longer active.","title":"timeout"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#how-it-works","text":"The Accumulator window works by: Maintaining an ordered list of elements for each key When the watermark progresses, it pops all sorted elements and writes them to the output stream New elements are inserted into the ordered list based on their event time If no new data arrives for a key within the specified timeout period, the window is closed Unlike both map or reduce operations, where Datum is consumed and Message is returned, for reordering with the Accumulator, the Datum is kept intact.","title":"How It Works"},{"location":"user-guide/user-defined-functions/reduce/windowing/accumulator/#example","text":"Here's an example of using an Accumulator window to join and sort two HTTP sources: apiVersion : numaflow.numaproj.io/v1alpha1 kind : Pipeline metadata : name : simple-accumulator spec : vertices : - name : http-one scale : min : 1 max : 1 source : http : {} - name : http-two scale : min : 1 max : 1 source : http : {} - name : accum udf : container : # stream sorter example image : quay.io/numaio/numaflow-go/stream-sorter:stable groupBy : window : accumulator : timeout : 10s keyed : true storage : persistentVolumeClaim : volumeSize : 1Gi - name : sink scale : min : 1 max : 1 sink : log : {} edges : - from : http-one to : accum - from : http-two to : accum - from : accum to : sink In this example: We have two HTTP sources ( http-one and http-two ) that produce ordered streams The accum vertex uses an Accumulator window with a timeout of 10 seconds The accumulator joins and sorts the events from both sources based on their event time The sorted output is sent to a log sink Note: Setting readBatchSize: 1 helps maintain the ordering of events in the input streams. Check the links below to see the UDF examples for different languages: Golang Python","title":"Example"},{"location":"user-guide/user-defined-functions/reduce/windowing/fixed/","text":"Fixed \u00b6 Fixed windows (sometimes called tumbling windows) are defined by a static window size, e.g. 30 second windows, one minute windows, etc. They are generally aligned, i.e. every window applies across all the data for the corresponding period of time. It has a fixed size measured in time and does not overlap. The element which belongs to one window will not belong to any other tumbling window. For example, a window size of 20 seconds will include all entities of the stream which came in a certain 20-second interval. Configuration \u00b6 To enable Fixed window, we use fixed under window section. vertices : - name : my-udf udf : groupBy : window : fixed : length : duration NOTE: A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". Length \u00b6 The length is the window size of the fixed window. Example \u00b6 A 60-second window size can be defined as following. vertices : - name : my-udf udf : groupBy : window : fixed : length : 60s The yaml snippet above contains an example spec of a reduce vertex that uses fixed window aggregation. As we can see, the length of the window is 60s. This means only one window will be active at any point in time. It is also possible to have multiple inactive and non-empty windows (based on out-of-order arrival of elements). The window boundaries for the first window (post bootstrap) are determined by rounding down from time.now() to the nearest multiple of length of the window. So considering the above example, if the time.now() corresponds to 2031-09-29T18:46:30Z , then the start-time of the window will be adjusted to 2031-09-29T18:46:00Z and the end-time is set accordingly to 2031-09-29T18:47:00Z . Windows are left inclusive and right exclusive which means an element with event time (considering event time characteristic) of 2031-09-29T18:47:00Z will belong to the window with boundaries [2031-09-29T18:47:00Z, 2031-09-29T18:48:00Z) It is important to note that because of this property, for a constant throughput, the first window may contain fewer elements than other windows. Check the links below to see the UDF examples for different languages. Python Golang Java Streaming Mode \u00b6 Reduce can be enabled on streaming mode to stream messages or forward partial responses to the next vertex. This is useful for custom triggering, where we want to forward responses to the next vertex quickly, even before the fixed window closes. The close-of-book and a final triggering will still happen even if partial results have been emitted. To enable reduce streaming, set the streaming flag to true in the fixed window configuration. vertices : - name : my-udf udf : groupBy : window : fixed : length : duration streaming : true # set streaming to true to enable reduce streamer Note: UDFs should use the ReduceStreamer functionality in the SDKs to use this feature. Check the links below to see the UDF examples in streaming mode for different languages. Python Golang Java","title":"Fixed"},{"location":"user-guide/user-defined-functions/reduce/windowing/fixed/#fixed","text":"Fixed windows (sometimes called tumbling windows) are defined by a static window size, e.g. 30 second windows, one minute windows, etc. They are generally aligned, i.e. every window applies across all the data for the corresponding period of time. It has a fixed size measured in time and does not overlap. The element which belongs to one window will not belong to any other tumbling window. For example, a window size of 20 seconds will include all entities of the stream which came in a certain 20-second interval.","title":"Fixed"},{"location":"user-guide/user-defined-functions/reduce/windowing/fixed/#configuration","text":"To enable Fixed window, we use fixed under window section. vertices : - name : my-udf udf : groupBy : window : fixed : length : duration NOTE: A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".","title":"Configuration"},{"location":"user-guide/user-defined-functions/reduce/windowing/fixed/#length","text":"The length is the window size of the fixed window.","title":"Length"},{"location":"user-guide/user-defined-functions/reduce/windowing/fixed/#example","text":"A 60-second window size can be defined as following. vertices : - name : my-udf udf : groupBy : window : fixed : length : 60s The yaml snippet above contains an example spec of a reduce vertex that uses fixed window aggregation. As we can see, the length of the window is 60s. This means only one window will be active at any point in time. It is also possible to have multiple inactive and non-empty windows (based on out-of-order arrival of elements). The window boundaries for the first window (post bootstrap) are determined by rounding down from time.now() to the nearest multiple of length of the window. So considering the above example, if the time.now() corresponds to 2031-09-29T18:46:30Z , then the start-time of the window will be adjusted to 2031-09-29T18:46:00Z and the end-time is set accordingly to 2031-09-29T18:47:00Z . Windows are left inclusive and right exclusive which means an element with event time (considering event time characteristic) of 2031-09-29T18:47:00Z will belong to the window with boundaries [2031-09-29T18:47:00Z, 2031-09-29T18:48:00Z) It is important to note that because of this property, for a constant throughput, the first window may contain fewer elements than other windows. Check the links below to see the UDF examples for different languages. Python Golang Java","title":"Example"},{"location":"user-guide/user-defined-functions/reduce/windowing/fixed/#streaming-mode","text":"Reduce can be enabled on streaming mode to stream messages or forward partial responses to the next vertex. This is useful for custom triggering, where we want to forward responses to the next vertex quickly, even before the fixed window closes. The close-of-book and a final triggering will still happen even if partial results have been emitted. To enable reduce streaming, set the streaming flag to true in the fixed window configuration. vertices : - name : my-udf udf : groupBy : window : fixed : length : duration streaming : true # set streaming to true to enable reduce streamer Note: UDFs should use the ReduceStreamer functionality in the SDKs to use this feature. Check the links below to see the UDF examples in streaming mode for different languages. Python Golang Java","title":"Streaming Mode"},{"location":"user-guide/user-defined-functions/reduce/windowing/session/","text":"Session \u00b6 Session window is a type of Unaligned window where the window\u2019s end time keeps moving until there is no data for a given time duration. Unlike fixed and sliding windows, session windows do not overlap, nor do they have a set start and end time. They can be used to group data based on activity. Window Merge \u00b6 There are cases where two session windows can be merged into one. This happens when the end time of one window is greater than the start time of the other window. The reason for creating two sessions is that there was a gap (greater than the session timeout) between the arrival of the events due to out-of-orderliness at the source. The moment an event that arrives in between the two windows arrives, the two windows are merged into one. Configuration \u00b6 vertices : - name : my-udf udf : groupBy : window : session : timeout : duration NOTE: A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". timeout \u00b6 The timeout is the duration of inactivity (no data flowing in for the particular key) after which the session is considered to be closed. Example \u00b6 To create a session window of timeout 1 minute, we can use the following snippet. vertices : - name : my-udf udf : groupBy : window : session : timeout : 60s The yaml snippet above contains an example spec of a reduce vertex that uses session window aggregation. As we can see, the timeout of the window is 60s. This means we no data arrives for a particular key for 60 seconds, we will mark it as closed. Let's say, time.now() in the pipeline is 2031-09-29T18:46:30Z as the current time, and we have a session gap of 30s. If we receive events in this pattern: Event-1 at 2031-09-29T18:45:40Z Event-2 at 2031-09-29T18:45:55Z # Notice the 15 sec interval from Event-1, still within session gap Event-3 at 2031-09-29T18:46:20Z # Notice the 25 sec interval from Event-2, still within session gap Event-4 at 2031-09-29T18:46:55Z # Notice the 35 sec interval from Event-3, beyond the session gap Event-5 at 2031-09-29T18:47:10Z # Notice the 15 sec interval from Event-4, within the new session gap This would lead to two session windows as follows: [2031-09-29T18:45:40Z, 2031-09-29T18:46:20Z) # includes Event-1, Event-2 and Event-3 [2031-09-29T18:46:55Z, 2031-09-29T18:47:10Z) # includes Event-4 and Event-5 In this example, the start time is inclusive and the end time is exclusive. Event-1 , Event-2 , and Event-3 fall within the first window, and this window closes 30 seconds after Event-3 at 2031-09-29T18:46:50Z . Event-4 arrives 5 seconds later, meaning it's beyond the session gap of the previous window, initiating a new window. The second window includes Event-4 and Event-5 , and it closes 30 seconds after Event-5 at 2031-09-29T18:47:40Z , if no further events arrive for the key until the timeout. Note: Streaming mode is by default enabled for session windows. Check the links below to see the UDF examples for different languages. Currently, we have the SDK support for Golang and Java. Golang Java","title":"Session"},{"location":"user-guide/user-defined-functions/reduce/windowing/session/#session","text":"Session window is a type of Unaligned window where the window\u2019s end time keeps moving until there is no data for a given time duration. Unlike fixed and sliding windows, session windows do not overlap, nor do they have a set start and end time. They can be used to group data based on activity.","title":"Session"},{"location":"user-guide/user-defined-functions/reduce/windowing/session/#window-merge","text":"There are cases where two session windows can be merged into one. This happens when the end time of one window is greater than the start time of the other window. The reason for creating two sessions is that there was a gap (greater than the session timeout) between the arrival of the events due to out-of-orderliness at the source. The moment an event that arrives in between the two windows arrives, the two windows are merged into one.","title":"Window Merge"},{"location":"user-guide/user-defined-functions/reduce/windowing/session/#configuration","text":"vertices : - name : my-udf udf : groupBy : window : session : timeout : duration NOTE: A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".","title":"Configuration"},{"location":"user-guide/user-defined-functions/reduce/windowing/session/#timeout","text":"The timeout is the duration of inactivity (no data flowing in for the particular key) after which the session is considered to be closed.","title":"timeout"},{"location":"user-guide/user-defined-functions/reduce/windowing/session/#example","text":"To create a session window of timeout 1 minute, we can use the following snippet. vertices : - name : my-udf udf : groupBy : window : session : timeout : 60s The yaml snippet above contains an example spec of a reduce vertex that uses session window aggregation. As we can see, the timeout of the window is 60s. This means we no data arrives for a particular key for 60 seconds, we will mark it as closed. Let's say, time.now() in the pipeline is 2031-09-29T18:46:30Z as the current time, and we have a session gap of 30s. If we receive events in this pattern: Event-1 at 2031-09-29T18:45:40Z Event-2 at 2031-09-29T18:45:55Z # Notice the 15 sec interval from Event-1, still within session gap Event-3 at 2031-09-29T18:46:20Z # Notice the 25 sec interval from Event-2, still within session gap Event-4 at 2031-09-29T18:46:55Z # Notice the 35 sec interval from Event-3, beyond the session gap Event-5 at 2031-09-29T18:47:10Z # Notice the 15 sec interval from Event-4, within the new session gap This would lead to two session windows as follows: [2031-09-29T18:45:40Z, 2031-09-29T18:46:20Z) # includes Event-1, Event-2 and Event-3 [2031-09-29T18:46:55Z, 2031-09-29T18:47:10Z) # includes Event-4 and Event-5 In this example, the start time is inclusive and the end time is exclusive. Event-1 , Event-2 , and Event-3 fall within the first window, and this window closes 30 seconds after Event-3 at 2031-09-29T18:46:50Z . Event-4 arrives 5 seconds later, meaning it's beyond the session gap of the previous window, initiating a new window. The second window includes Event-4 and Event-5 , and it closes 30 seconds after Event-5 at 2031-09-29T18:47:40Z , if no further events arrive for the key until the timeout. Note: Streaming mode is by default enabled for session windows. Check the links below to see the UDF examples for different languages. Currently, we have the SDK support for Golang and Java. Golang Java","title":"Example"},{"location":"user-guide/user-defined-functions/reduce/windowing/sliding/","text":"Sliding \u00b6 Sliding windows are similar to Fixed windows, the size of the windows is measured in time and is fixed. The important difference from the Fixed window is the fact that it allows an element to be present in more than one window. The additional window slide parameter controls how frequently a sliding window is started. Hence, sliding windows will be overlapping and the slide should be smaller than the window length. Configuration \u00b6 vertices : - name : my-udf udf : groupBy : window : sliding : length : duration slide : duration NOTE: A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\". Length \u00b6 The length is the window size of the fixed window. Slide \u00b6 slide is the slide parameter that controls the frequency at which the sliding window is created. Example \u00b6 To create a sliding window of length 1 minute which slides every 10 seconds, we can use the following snippet. vertices : - name : my-udf udf : groupBy : window : sliding : length : 60s slide : 10s The yaml snippet above contains an example spec of a reduce vertex that uses sliding window aggregation. As we can see, the length of the window is 60s and sliding frequency is once every 10s. This means there will be multiple windows active at any point in time. Let's say, time.now() in the pipeline is 2031-09-29T18:46:30Z the active window boundaries will be as follows (there are total of 6 windows 60s/10s ) [2031-09-29T18:45:40Z, 2031-09-29T18:46:40Z) [2031-09-29T18:45:50Z, 2031-09-29T18:46:50Z) # notice the 10 sec shift from the above window [2031-09-29T18:46:00Z, 2031-09-29T18:47:00Z) [2031-09-29T18:46:10Z, 2031-09-29T18:47:10Z) [2031-09-29T18:46:20Z, 2031-09-29T18:47:20Z) [2031-09-29T18:46:30Z, 2031-09-29T18:47:30Z) The window start time is always be left inclusive and right exclusive. That is why [2031-09-29T18:45:30Z, 2031-09-29T18:46:30Z) window is not considered active (it fell on the previous window, right exclusive) but [2031-09-29T18:46:30Z, 2031-09-29T18:47:30Z) is an active (left inclusive). The first window always ends after the sliding seconds from the time.Now() , the start time of the window will be the nearest integer multiple of the slide which is less than the message's event time. So the first window starts in the past and ends _sliding_duration (based on time progression in the pipeline and not the wall time) from present. It is important to note that regardless of the window boundary (starting in the past or ending in the future) the target element set totally depends on the matching time (in case of event time, all the elements with the time that falls with in the boundaries of the window, and in case of system time, all the elements that arrive from the present until the end of window present + sliding ) From the point above, it follows then that immediately upon startup, for the first window, fewer elements may get aggregated depending on the current lateness of the data stream. Check the links below to see the UDF examples for different languages. Python Golang Java Streaming Mode \u00b6 Reduce can be enabled on streaming mode to stream messages or forward partial responses to the next vertex. This is useful for custom triggering, where we want to forward responses to the next vertex quickly, even before the fixed window closes. The close-of-book and a final triggering will still happen even if partial results have been emitted. To enable reduce streaming, set the streaming flag to true in the sliding window configuration. vertices : - name : my-udf udf : groupBy : window : sliding : length : duration slide : duration streaming : true # set streaming to true to enable reduce streamer Note: UDFs should use the ReduceStreamer functionality in the SDKs to use this feature. Check the links below to see the UDF examples in streaming mode for different languages. Python Golang Java","title":"Sliding"},{"location":"user-guide/user-defined-functions/reduce/windowing/sliding/#sliding","text":"Sliding windows are similar to Fixed windows, the size of the windows is measured in time and is fixed. The important difference from the Fixed window is the fact that it allows an element to be present in more than one window. The additional window slide parameter controls how frequently a sliding window is started. Hence, sliding windows will be overlapping and the slide should be smaller than the window length.","title":"Sliding"},{"location":"user-guide/user-defined-functions/reduce/windowing/sliding/#configuration","text":"vertices : - name : my-udf udf : groupBy : window : sliding : length : duration slide : duration NOTE: A duration string is a possibly signed sequence of decimal numbers, each with optional fraction and a unit suffix, such as \"300ms\", \"1.5h\" or \"2h45m\". Valid time units are \"ns\", \"us\" (or \"\u00b5s\"), \"ms\", \"s\", \"m\", \"h\".","title":"Configuration"},{"location":"user-guide/user-defined-functions/reduce/windowing/sliding/#length","text":"The length is the window size of the fixed window.","title":"Length"},{"location":"user-guide/user-defined-functions/reduce/windowing/sliding/#slide","text":"slide is the slide parameter that controls the frequency at which the sliding window is created.","title":"Slide"},{"location":"user-guide/user-defined-functions/reduce/windowing/sliding/#example","text":"To create a sliding window of length 1 minute which slides every 10 seconds, we can use the following snippet. vertices : - name : my-udf udf : groupBy : window : sliding : length : 60s slide : 10s The yaml snippet above contains an example spec of a reduce vertex that uses sliding window aggregation. As we can see, the length of the window is 60s and sliding frequency is once every 10s. This means there will be multiple windows active at any point in time. Let's say, time.now() in the pipeline is 2031-09-29T18:46:30Z the active window boundaries will be as follows (there are total of 6 windows 60s/10s ) [2031-09-29T18:45:40Z, 2031-09-29T18:46:40Z) [2031-09-29T18:45:50Z, 2031-09-29T18:46:50Z) # notice the 10 sec shift from the above window [2031-09-29T18:46:00Z, 2031-09-29T18:47:00Z) [2031-09-29T18:46:10Z, 2031-09-29T18:47:10Z) [2031-09-29T18:46:20Z, 2031-09-29T18:47:20Z) [2031-09-29T18:46:30Z, 2031-09-29T18:47:30Z) The window start time is always be left inclusive and right exclusive. That is why [2031-09-29T18:45:30Z, 2031-09-29T18:46:30Z) window is not considered active (it fell on the previous window, right exclusive) but [2031-09-29T18:46:30Z, 2031-09-29T18:47:30Z) is an active (left inclusive). The first window always ends after the sliding seconds from the time.Now() , the start time of the window will be the nearest integer multiple of the slide which is less than the message's event time. So the first window starts in the past and ends _sliding_duration (based on time progression in the pipeline and not the wall time) from present. It is important to note that regardless of the window boundary (starting in the past or ending in the future) the target element set totally depends on the matching time (in case of event time, all the elements with the time that falls with in the boundaries of the window, and in case of system time, all the elements that arrive from the present until the end of window present + sliding ) From the point above, it follows then that immediately upon startup, for the first window, fewer elements may get aggregated depending on the current lateness of the data stream. Check the links below to see the UDF examples for different languages. Python Golang Java","title":"Example"},{"location":"user-guide/user-defined-functions/reduce/windowing/sliding/#streaming-mode","text":"Reduce can be enabled on streaming mode to stream messages or forward partial responses to the next vertex. This is useful for custom triggering, where we want to forward responses to the next vertex quickly, even before the fixed window closes. The close-of-book and a final triggering will still happen even if partial results have been emitted. To enable reduce streaming, set the streaming flag to true in the sliding window configuration. vertices : - name : my-udf udf : groupBy : window : sliding : length : duration slide : duration streaming : true # set streaming to true to enable reduce streamer Note: UDFs should use the ReduceStreamer functionality in the SDKs to use this feature. Check the links below to see the UDF examples in streaming mode for different languages. Python Golang Java","title":"Streaming Mode"},{"location":"user-guide/user-defined-functions/reduce/windowing/windowing/","text":"Windowing \u00b6 In the world of data processing on an unbounded stream, Windowing is a concept of grouping data using temporal boundaries. We use event-time to discover temporal boundaries on an unbounded, infinite stream and Watermark to ensure the datasets within the boundaries are complete. The reduce is applied on these grouped datasets. For example, when we say, we want to find number of users online per minute, we use windowing to group the users into one minute buckets. Window Types \u00b6 Numaflow supports the following types of windows Fixed Sliding Session Accumulator Configuration \u00b6 The entirety of windowing is under the groupBy section. vertices : - name : my-udf udf : groupBy : window : ... keyed : ... Since a window can be Non-Keyed v/s Keyed , we have an explicit field called keyed to differentiate between both (see below). Under the window section we will define different types of windows. Non-Keyed v/s Keyed Windows \u00b6 Non-Keyed \u00b6 A non-keyed partition is a partition where the window is the boundary condition. Data processing on a non-keyed partition cannot be scaled horizontally because only one partition exists. A non-keyed partition is usually used after aggregation and is hardly seen at the head section of any data processing pipeline. (There is a concept called Global Window where there is no windowing, but let us table that for later). Keyed \u00b6 A keyed partition is a partition where the partition boundary is a composite key of both the window and the key from the payload (e.g., GROUP BY country, where country names are the keys). Each smaller partition now has a complete set of datasets for that key and boundary. The subdivision of dividing a huge window-based partition into smaller partitions by adding keys along with the window will help us horizontally scale the distribution. Keyed partitions are heavily used to aggregate data and are frequently seen throughout the processing pipeline. We could also convert a non-keyed problem to a set of keyed problems and apply a non-keyed function at the end. This will help solve the original problem in a scalable manner without affecting the result's completeness and/or accuracy. When a keyed window is used, an optional partitions can be specified in the vertex for parallel processing. Usage \u00b6 Numaflow supports both Keyed and Non-Keyed windows. We set keyed to either true (keyed) or false (non-keyed). Please note that the non-keyed windows are not horizontally scalable as mentioned above. vertices : - name : my-reduce partitions : 5 # Optional, defaults to 1 udf : groupBy : window : ... keyed : true # Optional, defaults to false","title":"Overview"},{"location":"user-guide/user-defined-functions/reduce/windowing/windowing/#windowing","text":"In the world of data processing on an unbounded stream, Windowing is a concept of grouping data using temporal boundaries. We use event-time to discover temporal boundaries on an unbounded, infinite stream and Watermark to ensure the datasets within the boundaries are complete. The reduce is applied on these grouped datasets. For example, when we say, we want to find number of users online per minute, we use windowing to group the users into one minute buckets.","title":"Windowing"},{"location":"user-guide/user-defined-functions/reduce/windowing/windowing/#window-types","text":"Numaflow supports the following types of windows Fixed Sliding Session Accumulator","title":"Window Types"},{"location":"user-guide/user-defined-functions/reduce/windowing/windowing/#configuration","text":"The entirety of windowing is under the groupBy section. vertices : - name : my-udf udf : groupBy : window : ... keyed : ... Since a window can be Non-Keyed v/s Keyed , we have an explicit field called keyed to differentiate between both (see below). Under the window section we will define different types of windows.","title":"Configuration"},{"location":"user-guide/user-defined-functions/reduce/windowing/windowing/#non-keyed-vs-keyed-windows","text":"","title":"Non-Keyed v/s Keyed Windows"},{"location":"user-guide/user-defined-functions/reduce/windowing/windowing/#non-keyed","text":"A non-keyed partition is a partition where the window is the boundary condition. Data processing on a non-keyed partition cannot be scaled horizontally because only one partition exists. A non-keyed partition is usually used after aggregation and is hardly seen at the head section of any data processing pipeline. (There is a concept called Global Window where there is no windowing, but let us table that for later).","title":"Non-Keyed"},{"location":"user-guide/user-defined-functions/reduce/windowing/windowing/#keyed","text":"A keyed partition is a partition where the partition boundary is a composite key of both the window and the key from the payload (e.g., GROUP BY country, where country names are the keys). Each smaller partition now has a complete set of datasets for that key and boundary. The subdivision of dividing a huge window-based partition into smaller partitions by adding keys along with the window will help us horizontally scale the distribution. Keyed partitions are heavily used to aggregate data and are frequently seen throughout the processing pipeline. We could also convert a non-keyed problem to a set of keyed problems and apply a non-keyed function at the end. This will help solve the original problem in a scalable manner without affecting the result's completeness and/or accuracy. When a keyed window is used, an optional partitions can be specified in the vertex for parallel processing.","title":"Keyed"},{"location":"user-guide/user-defined-functions/reduce/windowing/windowing/#usage","text":"Numaflow supports both Keyed and Non-Keyed windows. We set keyed to either true (keyed) or false (non-keyed). Please note that the non-keyed windows are not horizontally scalable as mentioned above. vertices : - name : my-reduce partitions : 5 # Optional, defaults to 1 udf : groupBy : window : ... keyed : true # Optional, defaults to false","title":"Usage"}]}