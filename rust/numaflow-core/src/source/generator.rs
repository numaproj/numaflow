use bytes::Bytes;
use futures::Stream;
use pin_project::pin_project;
use std::future::Future;
use std::pin::Pin;
use std::task::{Context, Poll};
use std::time::Duration;
use tokio::time::Instant;

pub(crate) struct Generator {}

#[pin_project]
struct StreamGenerator {
    /// the content generated by Generator.
    content: Bytes,
    /// requests per unit of time-period.
    rpu: usize,
    /// batch size per read
    batch: usize,
    /// unit of time-period over which the [rpu] is defined.
    unit: Duration,
    /// the amount of credits used for the current time-period.
    /// remaining = (rpu - used) for that time-period
    used: usize,
    /// the last time we generated data. now() - prev_time is compared against the duration.
    prev_time: Instant,
    #[pin]
    tick: tokio::time::Interval,
}

impl StreamGenerator {
    pub(crate) fn new(content: Bytes, rpu: usize, batch: usize, unit: Duration) -> Self {
        let tick = tokio::time::interval(unit);

        Self {
            content,
            rpu,
            // batch cannot > rpu
            batch: if batch > rpu { rpu } else { batch },
            unit,
            used: 0,
            prev_time: Instant::now().checked_sub(unit).unwrap(),
            tick,
        }
    }
}

impl Stream for StreamGenerator {
    type Item = Vec<Bytes>;

    fn poll_next(
        mut self: Pin<&mut StreamGenerator>,
        cx: &mut Context<'_>,
    ) -> Poll<Option<Self::Item>> {
        let this = self.as_mut().project();

        // Calculate the elapsed time since the last poll
        let elapsed = this.prev_time.elapsed();

        // we can return the complete batch if enough time has passed
        if elapsed >= *this.unit {
            // Reset the timer
            *this.prev_time = Instant::now();

            *this.used = 0;

            // Generate data that equals to batch data
            let data = vec![this.content.clone(); *this.batch];
            // Reset used credits
            *this.used = *this.batch;

            // Return the generated data
            Poll::Ready(Some(data))
        } else if this.used < this.rpu {
            // even if enough time hasn't passed, we can still send data if we have
            // quota (rpu - used) left

            // make sure we do not send more than desired
            let to_send = if *this.rpu - *this.used < *this.batch {
                *this.rpu - *this.used
            } else {
                *this.batch
            };

            // update the counters
            *this.used += to_send;

            Poll::Ready(Some(vec![this.content.clone(); to_send]))
        } else {
            // we have to wait for the next tick because we are out of quota
            let mut tick = this.tick;
            match tick.poll_tick(cx) {
                // we can recurse ourselves to return data since enough time has passed
                Poll::Ready(_) => {
                    // recursively call the poll_next since we are ready to serve
                    self.poll_next(cx)
                }
                Poll::Pending => Poll::Pending,
            }
        }
    }

    /// size is roughly what is remaining and upper bound is for sure RPU. This is a very
    /// rough approximation because Duration is not taken into account for the lower bound.
    fn size_hint(&self) -> (usize, Option<usize>) {
        (self.rpu - self.used, Some(self.rpu))
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use futures::stream::StreamExt;
    use tokio::time::Duration;

    #[tokio::test]
    async fn test_stream_generator() {
        // Define the content to be generated
        let content = Bytes::from("test_data");
        // Define requests per unit (rpu), batch size, and time unit
        let rpu = 10;
        let batch = 6;
        let unit = Duration::from_millis(100);

        // Create a new StreamGenerator
        let mut stream_generator = StreamGenerator::new(content.clone(), rpu, batch, unit);

        // Collect the first batch of data
        let first_batch = stream_generator.next().await.unwrap();
        assert_eq!(first_batch.len(), batch);
        for item in first_batch {
            assert_eq!(item, content);
        }

        // Collect the second batch of data
        let second_batch = stream_generator.next().await.unwrap();
        assert_eq!(second_batch.len(), rpu - batch);
        for item in second_batch {
            assert_eq!(item, content);
        }

        // no there is no more data left in the quota
        let size = stream_generator.size_hint();
        assert_eq!(size.0, 0);
        assert_eq!(size.1, Some(rpu));

        let third_batch = stream_generator.next().await.unwrap();
        assert_eq!(third_batch.len(), 6);
        for item in third_batch {
            assert_eq!(item, content);
        }

        // we should now have data
        let size = stream_generator.size_hint();
        assert_eq!(size.0, 4);
        assert_eq!(size.1, Some(rpu));
    }
}
