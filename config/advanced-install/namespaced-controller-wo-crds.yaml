apiVersion: v1
kind: ServiceAccount
metadata:
  name: numaflow-sa
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:
    app.kubernetes.io/component: controller-manager
    app.kubernetes.io/name: numaflow-controller-manager
    app.kubernetes.io/part-of: numaflow
  name: numaflow-role
rules:
- apiGroups:
  - numaflow.numaproj.io
  resources:
  - interstepbufferservices
  - interstepbufferservices/finalizers
  - interstepbufferservices/status
  - pipelines
  - pipelines/finalizers
  - pipelines/status
  - vertices
  - vertices/finalizers
  - vertices/status
  - vertices/scale
  verbs:
  - create
  - delete
  - deletecollection
  - get
  - list
  - patch
  - update
  - watch
- apiGroups:
  - coordination.k8s.io
  resources:
  - leases
  verbs:
  - get
  - list
  - watch
  - create
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - events
  verbs:
  - create
  - patch
- apiGroups:
  - ""
  resources:
  - pods
  - pods/exec
  - configmaps
  - services
  - persistentvolumeclaims
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - ""
  resources:
  - secrets
  verbs:
  - create
  - get
  - list
  - update
  - patch
  - delete
- apiGroups:
  - apps
  resources:
  - deployments
  - statefulsets
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
- apiGroups:
  - batch
  resources:
  - jobs
  verbs:
  - create
  - get
  - list
  - watch
  - update
  - patch
  - delete
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:
    app.kubernetes.io/component: controller-manager
    app.kubernetes.io/name: numaflow-controller-manager
    app.kubernetes.io/part-of: numaflow
  name: numaflow-role-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: numaflow-role
subjects:
- kind: ServiceAccount
  name: numaflow-sa
---
apiVersion: v1
data:
  namespaced: "true"
  server.disable.auth: "true"
kind: ConfigMap
metadata:
  name: numaflow-cmd-params-config
---
apiVersion: v1
data:
  controller-config.yaml: |
    defaults:
      containerResources: |
        requests:
          memory: "128Mi"
          cpu: "100m"
    isbsvc:
      redis:
        # Default Redis settings, could be overridden by InterStepBufferService specs
        settings:
          # Redis config shared by both master and replicas
          redis: |
            min-replicas-to-write 1
            # Disable RDB persistence, AOF persistence already enabled.
            save ""
            # Enable AOF https://redis.io/topics/persistence#append-only-file
            appendonly yes
            auto-aof-rewrite-percentage 100
            auto-aof-rewrite-min-size 64mb
            maxmemory 512mb
            maxmemory-policy allkeys-lru
          # Special config only used by master
          master: ""
          # Special config only used by replicas
          replica: ""
          # Sentinel config
          sentinel: |
            sentinel down-after-milliseconds mymaster 10000
            sentinel failover-timeout mymaster 2000
            sentinel parallel-syncs mymaster 1
        versions:
        - version: 7.0.11
          redisImage: bitnami/redis:7.0.11-debian-11-r3
          sentinelImage: bitnami/redis-sentinel:7.0.11-debian-11-r3
          redisExporterImage: bitnami/redis-exporter:1.50.0-debian-11-r4
          initContainerImage: debian:latest
        - version: 7.0.15
          redisImage: bitnami/redis:7.0.15-debian-11-r2
          sentinelImage: bitnami/redis-sentinel:7.0.15-debian-11-r2
          redisExporterImage: bitnami/redis-exporter:1.56.0-debian-11-r2
          initContainerImage: debian:latest
      jetstream:
        # Default JetStream settings, could be overridden by InterStepBufferService specs
        settings: |
          # https://docs.nats.io/running-a-nats-service/configuration#limits
          # Only support to configure "max_payload".
          # Max payload size in bytes, defaults to 1 MB. It is not recommended to use values over 8MB but max_payload can be set up to 64MB.
          max_payload: 1048576
          # https://docs.nats.io/running-a-nats-service/configuration#jetstream
          # Only configure "max_memory_store" or "max_file_store", do not set "store_dir" as it has been hardcoded.
          # e.g. 1G. -1 means no limit, up to 75% of available memory. This only take effect for streams created using memory storage.
          max_memory_store: -1
          # e.g. 20G. -1 means no limit, Up to 1TB if available
          max_file_store: 1TB
        bufferConfig: |
          # The default properties of the buffers (streams) to be created in this JetStream service
          stream:
            # 0: Limits, 1: Interest, 2: WorkQueue
            retention: 0
            maxMsgs: 100000
            maxAge: 72h
            maxBytes: -1
            # 0: File, 1: Memory
            storage: 0
            replicas: 3
            duplicates: 60s
          # The default consumer properties for the created streams
          consumer:
            ackWait: 60s
            maxAckPending: 25000
          otBucket:
            maxValueSize: 0
            history: 1
            ttl: 3h
            maxBytes: 0
            # 0: File, 1: Memory
            storage: 0
            replicas: 3
          procBucket:
            maxValueSize: 0
            history: 1
            ttl: 72h
            maxBytes: 0
            # 0: File, 1: Memory
            storage: 0
            replicas: 3
        versions:
        - version: latest
          natsImage: nats:2.10.3
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
        - version: 2.8.1
          natsImage: nats:2.8.1
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
        - version: 2.8.1-alpine
          natsImage: nats:2.8.1-alpine
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: nats-server
        - version: 2.8.3
          natsImage: nats:2.8.3
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
        - version: 2.8.3-alpine
          natsImage: nats:2.8.3-alpine
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: nats-server
        - version: 2.9.0
          natsImage: nats:2.9.0
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
        - version: 2.9.0-alpine
          natsImage: nats:2.9.0-alpine
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: nats-server
        - version: 2.9.6
          natsImage: nats:2.9.6
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
        - version: 2.9.8
          natsImage: nats:2.9.8
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
        - version: 2.9.15
          natsImage: nats:2.9.15
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
        - version: 2.10.3
          natsImage: nats:2.10.3
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
        - version: 2.10.11
          natsImage: nats:2.10.11
          metricsExporterImage: natsio/prometheus-nats-exporter:0.9.1
          configReloaderImage: natsio/nats-server-config-reloader:0.7.0
          startCommand: /nats-server
kind: ConfigMap
metadata:
  name: numaflow-controller-config
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: numaflow-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/component: controller-manager
      app.kubernetes.io/name: controller-manager
      app.kubernetes.io/part-of: numaflow
  template:
    metadata:
      labels:
        app.kubernetes.io/component: controller-manager
        app.kubernetes.io/name: controller-manager
        app.kubernetes.io/part-of: numaflow
    spec:
      containers:
      - args:
        - controller
        env:
        - name: NUMAFLOW_IMAGE
          value: quay.io/numaproj/numaflow:latest
        - name: NAMESPACE
          valueFrom:
            fieldRef:
              fieldPath: metadata.namespace
        - name: NUMAFLOW_CONTROLLER_NAMESPACED
          valueFrom:
            configMapKeyRef:
              key: namespaced
              name: numaflow-cmd-params-config
              optional: true
        - name: NUMAFLOW_CONTROLLER_MANAGED_NAMESPACE
          valueFrom:
            configMapKeyRef:
              key: managed.namespace
              name: numaflow-cmd-params-config
              optional: true
        - name: NUMAFLOW_LEADER_ELECTION_DISABLED
          valueFrom:
            configMapKeyRef:
              key: controller.leader.election.disabled
              name: numaflow-cmd-params-config
              optional: true
        - name: NUMAFLOW_LEADER_ELECTION_LEASE_DURATION
          valueFrom:
            configMapKeyRef:
              key: controller.leader.election.lease.duration
              name: numaflow-cmd-params-config
              optional: true
        - name: NUMAFLOW_LEADER_ELECTION_LEASE_RENEW_DEADLINE
          valueFrom:
            configMapKeyRef:
              key: controller.leader.election.lease.renew.deadline
              name: numaflow-cmd-params-config
              optional: true
        - name: NUMAFLOW_LEADER_ELECTION_LEASE_RENEW_PERIOD
          valueFrom:
            configMapKeyRef:
              key: controller.leader.election.lease.renew.period
              name: numaflow-cmd-params-config
              optional: true
        image: quay.io/numaproj/numaflow:latest
        imagePullPolicy: Always
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 3
          periodSeconds: 3
        name: controller-manager
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 3
          periodSeconds: 3
        resources:
          limits:
            cpu: 500m
            memory: 1024Mi
          requests:
            cpu: 100m
            memory: 200Mi
        volumeMounts:
        - mountPath: /etc/numaflow
          name: controller-config-volume
      securityContext:
        runAsNonRoot: true
        runAsUser: 9737
      serviceAccountName: numaflow-sa
      volumes:
      - configMap:
          name: numaflow-controller-config
        name: controller-config-volume
